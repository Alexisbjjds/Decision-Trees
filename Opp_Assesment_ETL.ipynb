{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexisalduncin/Decision-Trees/blob/main/Opp_Assesment_ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uDj4GWF4zkl",
        "outputId": "373c2e68-696b-45a1-f849-2cb0328a5a85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.39.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (16.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<6,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.39.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.39.0 watchdog-5.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXPvkS6EM_I8"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from openpyxl import load_workbook\n",
        "from datetime import datetime\n",
        "\n",
        "from collections import defaultdict\n",
        "pd.set_option('display.max_columns', None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cOWgemvNIaP",
        "outputId": "bfc7183b-6d90-48cc-c8a2-64427b3591fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "honhQ_4ENfo-"
      },
      "source": [
        "JSON CATALOG for excel column mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQLp_9JANk4B",
        "outputId": "d5e7b682-661e-405d-cd73-919a46423606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame columns:\n",
            "['Vendor ID', 'Product Name', 'Purchase Date 2023', 'Unit Cost', '#SKU', 'Amount', 'Itm grp']\n",
            "Initial column mapping:\n",
            "Supplier ID: Vendor ID\n",
            "Supplier Name: Vendor ID\n",
            "Item Description: Product Name\n",
            "SKU: #SKU\n",
            "Price: Unit Cost\n",
            "Date: Purchase Date 2023\n",
            "\n",
            "Unmatched columns:\n",
            "No match found for 'Item Group'\n",
            "No match found for 'Product Group'\n",
            "No match found for 'UOM'\n",
            "No match found for 'Quantity'\n",
            "No match found for 'Quantity in UOM'\n",
            "\n",
            "Enter the correct column name for 'Item Group' (or press Enter to skip): \n",
            "\n",
            "Enter the correct column name for 'Product Group' (or press Enter to skip): \n",
            "\n",
            "Enter the correct column name for 'UOM' (or press Enter to skip): \n",
            "\n",
            "Enter the correct column name for 'Quantity' (or press Enter to skip): \n",
            "\n",
            "Enter the correct column name for 'Quantity in UOM' (or press Enter to skip): \n",
            "\n",
            "Processed DataFrame:\n",
            "  Supplier Name Item Description       Date   Price     SKU\n",
            "0          V001         Widget A 2023-01-15  100.50  SKU001\n",
            "1          V002         Gadget B 2023-02-20  200.75  SKU002\n",
            "2          V003           Tool C 2023-03-25  150.25  SKU003\n",
            "3          V004         Device D 2023-04-30  300.00  SKU004\n",
            "4          V005           Item E 2023-05-05  175.50  SKU005\n",
            "\n",
            "Processed DataFrame columns:\n",
            "['Supplier Name', 'Item Description', 'Date', 'Price', 'SKU']\n",
            "\n",
            "ETL process completed. DataFrame shape: (5, 5)\n",
            "\n",
            "Updated Catalog:\n",
            "{\n",
            "  \"Supplier ID\": [\n",
            "    \"Supplier ID\",\n",
            "    \"Supplier Number\",\n",
            "    \"Vendor ID\",\n",
            "    \"Vendor Number\",\n",
            "    \"Supplier\",\n",
            "    \"Vendor\"\n",
            "  ],\n",
            "  \"Supplier Name\": [\n",
            "    \"Supplier Name\",\n",
            "    \"Vendor Name\",\n",
            "    \"Supplier\",\n",
            "    \"Vendor\"\n",
            "  ],\n",
            "  \"Item Description\": [\n",
            "    \"Item Description\",\n",
            "    \"Product Description\",\n",
            "    \"Description\",\n",
            "    \"Item Name\",\n",
            "    \"Product Name\"\n",
            "  ],\n",
            "  \"SKU\": [\n",
            "    \"SKU\",\n",
            "    \"Item Number\",\n",
            "    \"Product ID\",\n",
            "    \"Product Code\",\n",
            "    \"Item Code\",\n",
            "    \"Part Number\"\n",
            "  ],\n",
            "  \"Item Group\": [\n",
            "    \"Item Group\",\n",
            "    \"Item Category\",\n",
            "    \"Product Group\",\n",
            "    \"Product Category\"\n",
            "  ],\n",
            "  \"Product Group\": [\n",
            "    \"Product Group\",\n",
            "    \"Product Category\",\n",
            "    \"Item Group\",\n",
            "    \"Item Category\"\n",
            "  ],\n",
            "  \"UOM\": [\n",
            "    \"UOM\",\n",
            "    \"Unit of Measure\",\n",
            "    \"Purchase UOM\",\n",
            "    \"Order UOM\"\n",
            "  ],\n",
            "  \"Quantity\": [\n",
            "    \"Quantity\",\n",
            "    \"Order Qty\",\n",
            "    \"Ordered Quantity\",\n",
            "    \"Qty\"\n",
            "  ],\n",
            "  \"Quantity in UOM\": [\n",
            "    \"Qty Received In Purch UOM\",\n",
            "    \"Order Qty UOM\",\n",
            "    \"Quantity in Purchase UOM\"\n",
            "  ],\n",
            "  \"Price\": [\n",
            "    \"Price\",\n",
            "    \"Purchase Price\",\n",
            "    \"Unit Price\",\n",
            "    \"Cost\"\n",
            "  ],\n",
            "  \"Date\": [\n",
            "    \"Received Date\",\n",
            "    \"Purchase Date\",\n",
            "    \"Order Date\",\n",
            "    \"Transaction Date\",\n",
            "    \"Invoice Date\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def load_or_create_catalog(catalog_file='column_name_catalog.json'):\n",
        "    if os.path.exists(catalog_file):\n",
        "        with open(catalog_file, 'r') as f:\n",
        "            return json.load(f)\n",
        "    else:\n",
        "        return {\n",
        "            'Supplier ID': ['Supplier ID', 'Supplier Number', 'Vendor ID', 'Vendor Number', 'Supplier', 'Vendor'],\n",
        "            'Supplier Name': ['Supplier Name', 'Vendor Name', 'Supplier', 'Vendor'],\n",
        "            'Item Description': ['Item Description', 'Product Description', 'Description', 'Item Name', 'Product Name'],\n",
        "            'SKU': ['SKU', 'Item Number', 'Product ID', 'Product Code', 'Item Code', 'Part Number'],\n",
        "            'Item Group': ['Item Group', 'Item Category', 'Product Group', 'Product Category'],\n",
        "            'Product Group': ['Product Group', 'Product Category', 'Item Group', 'Item Category'],\n",
        "            'UOM': ['UOM', 'Unit of Measure', 'Purchase UOM', 'Order UOM'],\n",
        "            'Quantity': ['Quantity', 'Order Qty', 'Ordered Quantity', 'Qty'],\n",
        "            'Quantity in UOM': ['Qty Received In Purch UOM', 'Order Qty UOM', 'Quantity in Purchase UOM'],\n",
        "            'Price': ['Price', 'Purchase Price', 'Unit Price', 'Cost'],\n",
        "            'Date': ['Received Date', 'Purchase Date', 'Order Date', 'Transaction Date', 'Invoice Date']\n",
        "        }\n",
        "def save_catalog(catalog, catalog_file='column_name_catalog.json'):\n",
        "    \"\"\"\n",
        "    Save the catalog to a JSON file.\n",
        "\n",
        "    Args:\n",
        "    catalog (dict): The catalog dictionary to be saved.\n",
        "    catalog_file (str): The filename where the catalog will be saved.\n",
        "    \"\"\"\n",
        "    with open(catalog_file, 'w') as f:\n",
        "        json.dump(catalog, f, indent=4)\n",
        "\n",
        "def find_matching_column(df_columns, catalog):\n",
        "    column_mapping = {}\n",
        "    unmatched_columns = []\n",
        "    df_columns = pd.Index(df_columns)\n",
        "\n",
        "    for standard_name, variations in catalog.items():\n",
        "        match_found = False\n",
        "        for possible_name in variations:\n",
        "            exact_match = df_columns[df_columns.str.lower() == possible_name.lower()]\n",
        "            if len(exact_match) > 0:\n",
        "                column_mapping[standard_name] = exact_match[0]\n",
        "                match_found = True\n",
        "                break\n",
        "\n",
        "            partial_matches = [col for col in df_columns if possible_name.lower() in col.lower()]\n",
        "            if partial_matches:\n",
        "                column_mapping[standard_name] = partial_matches[0]\n",
        "                match_found = True\n",
        "                break\n",
        "\n",
        "        if not match_found:\n",
        "            unmatched_columns.append(standard_name)\n",
        "\n",
        "    return column_mapping, unmatched_columns\n",
        "def update_catalog(catalog, new_mappings):\n",
        "    \"\"\"\n",
        "    Update the existing catalog with new column name mappings.\n",
        "\n",
        "    Args:\n",
        "    catalog (dict): The existing catalog dictionary.\n",
        "    new_mappings (dict): A dictionary of new mappings to add or update.\n",
        "\n",
        "    Returns:\n",
        "    dict: The updated catalog dictionary.\n",
        "    \"\"\"\n",
        "    for standard_name, variations in new_mappings.items():\n",
        "        if standard_name in catalog:\n",
        "            catalog[standard_name].extend([v for v in variations if v not in catalog[standard_name]])\n",
        "        else:\n",
        "            catalog[standard_name] = variations\n",
        "    return catalog\n",
        "\n",
        "def interactive_column_mapping(df_columns, catalog):\n",
        "    column_mapping, unmatched_columns = find_matching_column(df_columns, catalog)\n",
        "\n",
        "    print(\"Initial column mapping:\")\n",
        "    for standard_name, actual_name in column_mapping.items():\n",
        "        print(f\"{standard_name}: {actual_name}\")\n",
        "\n",
        "    print(\"\\nUnmatched columns:\")\n",
        "    for col in unmatched_columns:\n",
        "        print(f\"No match found for '{col}'\")\n",
        "\n",
        "    new_mappings = {}\n",
        "    for col in unmatched_columns:\n",
        "        user_input = input(f\"\\nEnter the correct column name for '{col}' (or press Enter to skip): \").strip()\n",
        "        if user_input:\n",
        "            if user_input in column_mapping.values():\n",
        "                print(f\"Error: '{user_input}' is already mapped to another column. Please choose a different name.\")\n",
        "            else:\n",
        "                column_mapping[col] = user_input\n",
        "                new_mappings[col] = [user_input]\n",
        "\n",
        "    if new_mappings:\n",
        "        catalog = update_catalog(catalog, new_mappings)\n",
        "        save_catalog(catalog)\n",
        "        print(\"\\nCatalog updated with new mappings.\")\n",
        "\n",
        "    return column_mapping, catalog\n",
        "\n",
        "def process_file(df, column_mapping):\n",
        "    # Create a reverse mapping (from original column names to standard names)\n",
        "    reverse_mapping = {v: k for k, v in column_mapping.items()}\n",
        "\n",
        "    # Rename columns based on the mapping\n",
        "    df_renamed = df.rename(columns=reverse_mapping)\n",
        "\n",
        "    # Keep only the columns that were successfully mapped\n",
        "    mapped_columns = [col for col in df_renamed.columns if col in column_mapping.keys()]\n",
        "    df_renamed = df_renamed[mapped_columns]\n",
        "\n",
        "    # Perform some basic processing\n",
        "    if 'Date' in df_renamed.columns:\n",
        "        df_renamed['Date'] = pd.to_datetime(df_renamed['Date'], errors='coerce')\n",
        "    if 'Price' in df_renamed.columns:\n",
        "        df_renamed['Price'] = pd.to_numeric(df_renamed['Price'], errors='coerce')\n",
        "    if 'Quantity' in df_renamed.columns:\n",
        "        df_renamed['Quantity'] = pd.to_numeric(df_renamed['Quantity'], errors='coerce')\n",
        "\n",
        "    return df_renamed\n",
        "\n",
        "def etl_process(df):\n",
        "    catalog = load_or_create_catalog()\n",
        "\n",
        "    print(\"Original DataFrame columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    column_mapping, updated_catalog = interactive_column_mapping(df.columns, catalog)\n",
        "\n",
        "    processed_df = process_file(df, column_mapping)\n",
        "\n",
        "    print(\"\\nProcessed DataFrame:\")\n",
        "    print(processed_df.head())\n",
        "    print(\"\\nProcessed DataFrame columns:\")\n",
        "    print(processed_df.columns.tolist())\n",
        "\n",
        "    return processed_df, updated_catalog\n",
        "\n",
        "# Example usage\n",
        "def create_sample_dataframe():\n",
        "    data = {\n",
        "        'Vendor ID': ['V001', 'V002', 'V003', 'V004', 'V005'],\n",
        "        'Product Name': ['Widget A', 'Gadget B', 'Tool C', 'Device D', 'Item E'],\n",
        "        'Purchase Date 2023': ['2023-01-15', '2023-02-20', '2023-03-25', '2023-04-30', '2023-05-05'],\n",
        "        'Unit Cost': [100.50, 200.75, 150.25, 300.00, 175.50],\n",
        "        '#SKU': ['SKU001', 'SKU002', 'SKU003', 'SKU004', 'SKU005'],\n",
        "        'Amount': ['1', '2', '3', '4', '5'],\n",
        "        'Itm grp': ['Electronics', 'Tools', 'Hardware', 'Accessories', 'Parts']\n",
        "    }\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def run_example():\n",
        "    df = create_sample_dataframe()\n",
        "    processed_df, updated_catalog = etl_process(df)\n",
        "    print(\"\\nETL process completed. DataFrame shape:\", processed_df.shape)\n",
        "    print(\"\\nUpdated Catalog:\")\n",
        "    print(json.dumps(updated_catalog, indent=2))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beV8YmoKAxyb"
      },
      "source": [
        "### Client uploads file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGflLxNE8GtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dfed4d1-01ef-4709-c209-26b055cbb3e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame columns:\n",
            "['Supplier Name', 'Supplier Number', 'Supplier Group Name', 'Supplier Group Number', 'Product', 'Product Description', 'Gauge', 'Item Group', 'Product Group', 'Color', 'Finish', 'Material Code', 'Material Description', 'Width', 'Sheet Length', 'Division', 'Received Year', 'Purchase Order Date', 'Facility Code', 'Warehouse', 'Pounds Received', 'Purchase Price', 'Order Qty', 'Purchase UOM', 'Qty Received In Purch UOM', 'Cost', 'Purchase Order Number', 'Purchase Order Line Number', 'Purchase Order Type', 'Purchase Order Our Reference Number', 'PO Line Order Type', 'PO Line Our Reference Number', 'Received Date', 'System', 'Supplier Item Number', 'BFMPC', 'Requested Date', 'Confirmed Date', 'Final Destination', \"Price * Qty Rec'd\", 'Include?', 'Include for Savings?', 'Parent Vendor', 'ST Category', 'ST Sub-Category', 'Opportunity Term', 'Est. Savings % (Low)', 'Est. Savings % (High)', 'Est Savings $ (Low)', 'Est. Savings $ (High)', 'Market Place Designation']\n",
            "Initial column mapping:\n",
            "Supplier ID: Supplier Number\n",
            "Supplier Name: Supplier Name\n",
            "Item Description: Product Description\n",
            "SKU: Supplier Item Number\n",
            "Item Group: Item Group\n",
            "Product Group: Product Group\n",
            "UOM: Purchase UOM\n",
            "Quantity: Order Qty\n",
            "Quantity in UOM: Qty Received In Purch UOM\n",
            "Price: Purchase Price\n",
            "Date: Received Date\n",
            "\n",
            "Unmatched columns:\n",
            "\n",
            "Processed DataFrame:\n",
            "   Supplier ID                         Supplier Name  \\\n",
            "0        55311  Mid-Ohio Wood Products                 \n",
            "1         9141  Bunzl Processor Division               \n",
            "2        89437  Hel, Inc.                              \n",
            "3        54448  Mactac, Inc.                           \n",
            "4        89959  Rocheux International, Inc.            \n",
            "\n",
            "                 Item Description                             SKU  \\\n",
            "0  PALLET 42X42\"                                                    \n",
            "1  OPERATION SUPPLIES                                               \n",
            "2  SERVICES-NON MATERIAL                                            \n",
            "3  .00400 RM FPVC.TC2N.PS CL 0     4-1550                           \n",
            "4  .01500 RM PVC WHTOPQ MM         BPGBMXX                          \n",
            "\n",
            "                  Item Group       Product Group  UOM  Quantity  \\\n",
            "0                     PALLET  PACKAGING MATERIAL  EA      200.0   \n",
            "1  SUBCONTRACTED ITEMS GROUP       MISCELLANEOUS  EA       12.0   \n",
            "2  SUBCONTRACTED ITEMS GROUP       MISCELLANEOUS  EA        1.0   \n",
            "3              TRANSCLING II  Vinyl Print family  FT    10000.0   \n",
            "4                VINYL GROUP  Vinyl Print family  LBS   15000.0   \n",
            "\n",
            "   Quantity in UOM      Price       Date     Total                   Stage  \\\n",
            "0            100.0    18.7500 2024-06-26   1875.00  Opportunity Assessment   \n",
            "1             12.0     4.9900 2024-07-03     59.88  Opportunity Assessment   \n",
            "2              1.0  1637.0000 2024-03-17   1637.00  Opportunity Assessment   \n",
            "3          10000.0     0.8165 2024-07-30   8165.00  Opportunity Assessment   \n",
            "4           6489.0     1.7600 2024-01-05  11420.64  Opportunity Assessment   \n",
            "\n",
            "   Client Name  \n",
            "0  Transcendia  \n",
            "1  Transcendia  \n",
            "2  Transcendia  \n",
            "3  Transcendia  \n",
            "4  Transcendia  \n",
            "\n",
            "Processed DataFrame columns:\n",
            "['Supplier ID', 'Supplier Name', 'Item Description', 'SKU', 'Item Group', 'Product Group', 'UOM', 'Quantity', 'Quantity in UOM', 'Price', 'Date', 'Total', 'Stage', 'Client Name']\n",
            "\n",
            "Processing completed. DataFrame shape: (33027, 14)\n",
            "\n",
            "First few rows of the processed DataFrame:\n",
            "   Supplier ID                         Supplier Name  \\\n",
            "0        55311  Mid-Ohio Wood Products                 \n",
            "1         9141  Bunzl Processor Division               \n",
            "2        89437  Hel, Inc.                              \n",
            "3        54448  Mactac, Inc.                           \n",
            "4        89959  Rocheux International, Inc.            \n",
            "\n",
            "                 Item Description                             SKU  \\\n",
            "0  PALLET 42X42\"                                                    \n",
            "1  OPERATION SUPPLIES                                               \n",
            "2  SERVICES-NON MATERIAL                                            \n",
            "3  .00400 RM FPVC.TC2N.PS CL 0     4-1550                           \n",
            "4  .01500 RM PVC WHTOPQ MM         BPGBMXX                          \n",
            "\n",
            "                  Item Group       Product Group  UOM  Quantity  \\\n",
            "0                     PALLET  PACKAGING MATERIAL  EA      200.0   \n",
            "1  SUBCONTRACTED ITEMS GROUP       MISCELLANEOUS  EA       12.0   \n",
            "2  SUBCONTRACTED ITEMS GROUP       MISCELLANEOUS  EA        1.0   \n",
            "3              TRANSCLING II  Vinyl Print family  FT    10000.0   \n",
            "4                VINYL GROUP  Vinyl Print family  LBS   15000.0   \n",
            "\n",
            "   Quantity in UOM      Price       Date     Total                   Stage  \\\n",
            "0            100.0    18.7500 2024-06-26   1875.00  Opportunity Assessment   \n",
            "1             12.0     4.9900 2024-07-03     59.88  Opportunity Assessment   \n",
            "2              1.0  1637.0000 2024-03-17   1637.00  Opportunity Assessment   \n",
            "3          10000.0     0.8165 2024-07-30   8165.00  Opportunity Assessment   \n",
            "4           6489.0     1.7600 2024-01-05  11420.64  Opportunity Assessment   \n",
            "\n",
            "   Client Name  \n",
            "0  Transcendia  \n",
            "1  Transcendia  \n",
            "2  Transcendia  \n",
            "3  Transcendia  \n",
            "4  Transcendia  \n",
            "\n",
            "Updated Catalog:\n",
            "{\n",
            "  \"Supplier ID\": [\n",
            "    \"Supplier ID\",\n",
            "    \"Supplier Number\",\n",
            "    \"Vendor ID\",\n",
            "    \"Vendor Number\",\n",
            "    \"Supplier\",\n",
            "    \"Vendor\"\n",
            "  ],\n",
            "  \"Supplier Name\": [\n",
            "    \"Supplier Name\",\n",
            "    \"Vendor Name\",\n",
            "    \"Supplier\",\n",
            "    \"Vendor\"\n",
            "  ],\n",
            "  \"Item Description\": [\n",
            "    \"Item Description\",\n",
            "    \"Product Description\",\n",
            "    \"Description\",\n",
            "    \"Item Name\",\n",
            "    \"Product Name\"\n",
            "  ],\n",
            "  \"SKU\": [\n",
            "    \"SKU\",\n",
            "    \"Item Number\",\n",
            "    \"Product ID\",\n",
            "    \"Product Code\",\n",
            "    \"Item Code\",\n",
            "    \"Part Number\"\n",
            "  ],\n",
            "  \"Item Group\": [\n",
            "    \"Item Group\",\n",
            "    \"Item Category\",\n",
            "    \"Product Group\",\n",
            "    \"Product Category\"\n",
            "  ],\n",
            "  \"Product Group\": [\n",
            "    \"Product Group\",\n",
            "    \"Product Category\",\n",
            "    \"Item Group\",\n",
            "    \"Item Category\"\n",
            "  ],\n",
            "  \"UOM\": [\n",
            "    \"UOM\",\n",
            "    \"Unit of Measure\",\n",
            "    \"Purchase UOM\",\n",
            "    \"Order UOM\"\n",
            "  ],\n",
            "  \"Quantity\": [\n",
            "    \"Quantity\",\n",
            "    \"Order Qty\",\n",
            "    \"Ordered Quantity\",\n",
            "    \"Qty\"\n",
            "  ],\n",
            "  \"Quantity in UOM\": [\n",
            "    \"Qty Received In Purch UOM\",\n",
            "    \"Order Qty UOM\",\n",
            "    \"Quantity in Purchase UOM\"\n",
            "  ],\n",
            "  \"Price\": [\n",
            "    \"Price\",\n",
            "    \"Purchase Price\",\n",
            "    \"Unit Price\",\n",
            "    \"Cost\"\n",
            "  ],\n",
            "  \"Date\": [\n",
            "    \"Received Date\",\n",
            "    \"Purchase Date\",\n",
            "    \"Order Date\",\n",
            "    \"Transaction Date\",\n",
            "    \"Invoice Date\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def process_client_input(file_path):\n",
        "    \"\"\"\n",
        "    Process the client's input Excel file.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): The path to the client's Excel file.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - pd.DataFrame: The processed DataFrame with mapped columns and additional required columns.\n",
        "        - dict: The updated catalog.\n",
        "    \"\"\"\n",
        "    # Load the catalog with the function\n",
        "    catalog = load_or_create_catalog()\n",
        "\n",
        "    # Extract client name from the file name to put on table\n",
        "    client_name = os.path.basename(file_path).split(' - ')[0]\n",
        "\n",
        "    # Load the workbook\n",
        "    wb = load_workbook(file_path, read_only=True, data_only=True)\n",
        "\n",
        "    # Find the sheet with 'Data' in its name\n",
        "    data_sheet = next((sheet for sheet in wb.sheetnames if 'data' in sheet.lower()), None)\n",
        "    if not data_sheet:\n",
        "        raise ValueError(\"No sheet with 'Data' in its name found in the workbook.\")\n",
        "\n",
        "    # Read the Excel file\n",
        "    df = pd.read_excel(file_path, sheet_name=data_sheet)\n",
        "\n",
        "    print(\"Original DataFrame columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    # Perform interactive column mapping\n",
        "    column_mapping, updated_catalog = interactive_column_mapping(df.columns, catalog)\n",
        "\n",
        "    # Process the file with the new mapping\n",
        "    df = process_file(df, column_mapping)\n",
        "\n",
        "    # Filter the DataFrame to include only the columns from our catalog\n",
        "    catalog_columns = list(column_mapping.keys())\n",
        "    df = df[catalog_columns]\n",
        "\n",
        "    # Add required columns\n",
        "    if 'Quantity in UOM' in df.columns and 'Price' in df.columns:\n",
        "        df['Total'] = df['Quantity in UOM'] * df['Price']\n",
        "    else:\n",
        "        print(\"Warning: 'Quantity in UOM' or 'Price' column not found. 'Total' column not added.\")\n",
        "\n",
        "    df['Stage'] = 'Opportunity Assessment'\n",
        "    df['Client Name'] = client_name\n",
        "\n",
        "    print(\"\\nProcessed DataFrame:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nProcessed DataFrame columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    return df, updated_catalog\n",
        "\n",
        "def run_example():\n",
        "    \"\"\"\n",
        "    Run an example of the client input processing.\n",
        "\n",
        "    This function processes a sample Excel file and displays the results.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: The processed DataFrame.\n",
        "    \"\"\"\n",
        "    # File path (replace with Azure Blob Storage path when moving to Azure)\n",
        "    file_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/Transcendia - Opportunity Assessment.xlsx\"\n",
        "\n",
        "    # Process the client input\n",
        "    processed_df, updated_catalog = process_client_input(file_path)\n",
        "\n",
        "    # Here you would typically save the processed DataFrame or perform further analysis\n",
        "    print(\"\\nProcessing completed. DataFrame shape:\", processed_df.shape)\n",
        "    print(\"\\nFirst few rows of the processed DataFrame:\")\n",
        "    print(processed_df.head())\n",
        "\n",
        "    # Display the updated catalog\n",
        "    print(\"\\nUpdated Catalog:\")\n",
        "    print(json.dumps(updated_catalog, indent=2))\n",
        "\n",
        "    return processed_df\n",
        "\n",
        "# Run the example and store the result in a global variable\n",
        "df = run_example()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxj9HddbsBUN"
      },
      "source": [
        "Azure environement example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raiExsrRBAgB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "ac928b4e-a2e7-4931-a6ad-f854d8e776a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# In Azure environment:\\nfrom azure.storage.blob import BlobServiceClient\\n\\ndef read_excel_from_blob(container_name, blob_name):\\n    connection_string = \"your_connection_string\"\\n    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\\n    container_client = blob_service_client.get_container_client(container_name)\\n    blob_client = container_client.get_blob_client(blob_name)\\n\\n    with BytesIO(blob_client.download_blob().readall()) as input_file:\\n        return pd.read_excel(input_file)\\n\\n# Then use this function instead of pd.read_excel in process_client_input\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\"\"\"\n",
        "# In Azure environment:\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "\n",
        "def read_excel_from_blob(container_name, blob_name):\n",
        "    connection_string = \"your_connection_string\"\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "    container_client = blob_service_client.get_container_client(container_name)\n",
        "    blob_client = container_client.get_blob_client(blob_name)\n",
        "\n",
        "    with BytesIO(blob_client.download_blob().readall()) as input_file:\n",
        "        return pd.read_excel(input_file)\n",
        "\n",
        "# Then use this function instead of pd.read_excel in process_client_input\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN5NI_Ywafx8"
      },
      "source": [
        "Show initial DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PztcQaCgB7qK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "ca930e38-1716-42ff-e0e5-816e4f880473"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Supplier ID                         Supplier Name  \\\n",
              "0        55311  Mid-Ohio Wood Products                 \n",
              "1         9141  Bunzl Processor Division               \n",
              "2        89437  Hel, Inc.                              \n",
              "3        54448  Mactac, Inc.                           \n",
              "4        89959  Rocheux International, Inc.            \n",
              "\n",
              "                 Item Description                             SKU  \\\n",
              "0  PALLET 42X42\"                                                    \n",
              "1  OPERATION SUPPLIES                                               \n",
              "2  SERVICES-NON MATERIAL                                            \n",
              "3  .00400 RM FPVC.TC2N.PS CL 0     4-1550                           \n",
              "4  .01500 RM PVC WHTOPQ MM         BPGBMXX                          \n",
              "\n",
              "                  Item Group       Product Group  UOM  Quantity  \\\n",
              "0                     PALLET  PACKAGING MATERIAL  EA      200.0   \n",
              "1  SUBCONTRACTED ITEMS GROUP       MISCELLANEOUS  EA       12.0   \n",
              "2  SUBCONTRACTED ITEMS GROUP       MISCELLANEOUS  EA        1.0   \n",
              "3              TRANSCLING II  Vinyl Print family  FT    10000.0   \n",
              "4                VINYL GROUP  Vinyl Print family  LBS   15000.0   \n",
              "\n",
              "   Quantity in UOM      Price       Date     Total                   Stage  \\\n",
              "0            100.0    18.7500 2024-06-26   1875.00  Opportunity Assessment   \n",
              "1             12.0     4.9900 2024-07-03     59.88  Opportunity Assessment   \n",
              "2              1.0  1637.0000 2024-03-17   1637.00  Opportunity Assessment   \n",
              "3          10000.0     0.8165 2024-07-30   8165.00  Opportunity Assessment   \n",
              "4           6489.0     1.7600 2024-01-05  11420.64  Opportunity Assessment   \n",
              "\n",
              "   Client Name  \n",
              "0  Transcendia  \n",
              "1  Transcendia  \n",
              "2  Transcendia  \n",
              "3  Transcendia  \n",
              "4  Transcendia  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d4203493-86e5-4e0b-a3c3-f23840e8004a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Supplier ID</th>\n",
              "      <th>Supplier Name</th>\n",
              "      <th>Item Description</th>\n",
              "      <th>SKU</th>\n",
              "      <th>Item Group</th>\n",
              "      <th>Product Group</th>\n",
              "      <th>UOM</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>Quantity in UOM</th>\n",
              "      <th>Price</th>\n",
              "      <th>Date</th>\n",
              "      <th>Total</th>\n",
              "      <th>Stage</th>\n",
              "      <th>Client Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>55311</td>\n",
              "      <td>Mid-Ohio Wood Products</td>\n",
              "      <td>PALLET 42X42\"</td>\n",
              "      <td></td>\n",
              "      <td>PALLET</td>\n",
              "      <td>PACKAGING MATERIAL</td>\n",
              "      <td>EA</td>\n",
              "      <td>200.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>18.7500</td>\n",
              "      <td>2024-06-26</td>\n",
              "      <td>1875.00</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9141</td>\n",
              "      <td>Bunzl Processor Division</td>\n",
              "      <td>OPERATION SUPPLIES</td>\n",
              "      <td></td>\n",
              "      <td>SUBCONTRACTED ITEMS GROUP</td>\n",
              "      <td>MISCELLANEOUS</td>\n",
              "      <td>EA</td>\n",
              "      <td>12.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>4.9900</td>\n",
              "      <td>2024-07-03</td>\n",
              "      <td>59.88</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>89437</td>\n",
              "      <td>Hel, Inc.</td>\n",
              "      <td>SERVICES-NON MATERIAL</td>\n",
              "      <td></td>\n",
              "      <td>SUBCONTRACTED ITEMS GROUP</td>\n",
              "      <td>MISCELLANEOUS</td>\n",
              "      <td>EA</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1637.0000</td>\n",
              "      <td>2024-03-17</td>\n",
              "      <td>1637.00</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54448</td>\n",
              "      <td>Mactac, Inc.</td>\n",
              "      <td>.00400 RM FPVC.TC2N.PS CL 0</td>\n",
              "      <td>4-1550</td>\n",
              "      <td>TRANSCLING II</td>\n",
              "      <td>Vinyl Print family</td>\n",
              "      <td>FT</td>\n",
              "      <td>10000.0</td>\n",
              "      <td>10000.0</td>\n",
              "      <td>0.8165</td>\n",
              "      <td>2024-07-30</td>\n",
              "      <td>8165.00</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>89959</td>\n",
              "      <td>Rocheux International, Inc.</td>\n",
              "      <td>.01500 RM PVC WHTOPQ MM</td>\n",
              "      <td>BPGBMXX</td>\n",
              "      <td>VINYL GROUP</td>\n",
              "      <td>Vinyl Print family</td>\n",
              "      <td>LBS</td>\n",
              "      <td>15000.0</td>\n",
              "      <td>6489.0</td>\n",
              "      <td>1.7600</td>\n",
              "      <td>2024-01-05</td>\n",
              "      <td>11420.64</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4203493-86e5-4e0b-a3c3-f23840e8004a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d4203493-86e5-4e0b-a3c3-f23840e8004a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d4203493-86e5-4e0b-a3c3-f23840e8004a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-771a3d62-c8ae-4643-898d-bf1bac28bb74\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-771a3d62-c8ae-4643-898d-bf1bac28bb74')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-771a3d62-c8ae-4643-898d-bf1bac28bb74 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 33027,\n  \"fields\": [\n    {\n      \"column\": \"Supplier ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31681,\n        \"min\": 154,\n        \"max\": 96980,\n        \"num_unique_values\": 1045,\n        \"samples\": [\n          337,\n          74209,\n          71531\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Supplier Name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1024,\n        \"samples\": [\n          \"DE Lage Landen Financial Services   \",\n          \"ZORO Tools Inc.                     \",\n          \"Advanced Roofing Systems Inc.       \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Item Description\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1780,\n        \"samples\": [\n          \"R&D RAW ACTEGA AQ-414         \",\n          \"R&D RAW STANDRIDGE 2SAM0394 >>\",\n          \"9X14\\\" CLEAR IND POLY SLEEVES  \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SKU\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2014,\n        \"samples\": [\n          \"ColeSlaw                      \",\n          \"30018                         \",\n          \"KC10                          \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Item Group\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 73,\n        \"samples\": [\n          \"RESIN GROUP\",\n          \"TRANS-BANNER\",\n          \"DIVIDERS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Product Group\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 93,\n        \"samples\": [\n          \"VCI Polypropylene\",\n          \"WHITE\",\n          \"DIGI KOTE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UOM\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"M  \",\n          \"FT \",\n          \"BOX\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Quantity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 50871.92454483623,\n        \"min\": 0.25,\n        \"max\": 2375974.0,\n        \"num_unique_values\": 3265,\n        \"samples\": [\n          2267.99,\n          2181.64,\n          10346.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Quantity in UOM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 44002.35827477427,\n        \"min\": 0.17,\n        \"max\": 2376198.0,\n        \"num_unique_values\": 7045,\n        \"samples\": [\n          22527.0,\n          4479.0,\n          20903.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19478.914559185825,\n        \"min\": 0.0,\n        \"max\": 2760000.0,\n        \"num_unique_values\": 9204,\n        \"samples\": [\n          13.75,\n          29.904,\n          12.85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2023-08-01 00:00:00\",\n        \"max\": \"2024-07-31 00:00:00\",\n        \"num_unique_values\": 288,\n        \"samples\": [\n          \"2023-08-17 00:00:00\",\n          \"2023-09-07 00:00:00\",\n          \"2024-04-19 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 42197.82202688142,\n        \"min\": 0.0,\n        \"max\": 5296500.0,\n        \"num_unique_values\": 19275,\n        \"samples\": [\n          72.84,\n          8269.61,\n          2761.92\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Stage\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Opportunity Assessment\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Client Name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Transcendia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcDsI6Ul03pM"
      },
      "source": [
        "generate_parent_ID(name, counter):\n",
        "\n",
        "*   Creates a unique Parent Vendor ID from a supplier's name\n",
        "*   Uses the first letters of up to three words in the name\n",
        "*   Adds a number to ensure uniqueness\n",
        "*   For Supplier ID it assigns the IDs using a counter for each Parent Vendor\n",
        "\n",
        "\n",
        "load_unified_vendor_lookup(path):\n",
        "\n",
        "*   Loads an existing vendor lookup table from an Excel file\n",
        "\n",
        "\n",
        "update_vendor_lookup(unified_lookup, new_suppliers):\n",
        "\n",
        "*   Updates the vendor lookup table with new suppliers\n",
        "*   Initializes counters for Parent Vendor IDs and Vendor IDs based on existing entries\n",
        "*   For each new supplier:\n",
        "\n",
        "*   Generates a new Parent Vendor ID\n",
        "*   Creates a new Vendor ID\n",
        "*   Adds a new entry with default values (empty strings and 0s)for categories and savings estimates\n",
        "\n",
        "\n",
        "*   Combines new entries with the existing lookup table\n",
        "*   Maintains existing IDs and generates new ones for new suppliers\n",
        "*   Ensures each supplier has a unique identifier\n",
        "*   Allows for tracking of parent-child relationships between vendors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWpHJpLJxFM2"
      },
      "outputs": [],
      "source": [
        "def generate_parent_ID(name, counter):\n",
        "    \"\"\"Generate a Parent Vendor ID from a name.\"\"\"\n",
        "    base = ''.join(word[0] for word in str(name).upper().split()[:3])\n",
        "    counter[base] += 1\n",
        "    return f\"{base}{counter[base]:03d}\"\n",
        "\n",
        "def load_unified_vendor_lookup(path):\n",
        "    \"\"\"Load the unified Vendor Look Up table from the given path.\"\"\"\n",
        "    return pd.read_excel(path)\n",
        "\n",
        "def update_vendor_lookup(unified_lookup, new_suppliers):\n",
        "    \"\"\"Update the unified Vendor Look Up table with new suppliers.\"\"\"\n",
        "    parent_ID_counter = defaultdict(int)\n",
        "    vendor_ID_counter = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # Initialize counters based on existing IDs\n",
        "    for _, row in unified_lookup.iterrows():\n",
        "        parent_ID = row['Parent Vendor ID']\n",
        "        if pd.notna(parent_ID):\n",
        "            base = parent_ID[:3]\n",
        "            num = int(parent_ID[3:])\n",
        "            parent_ID_counter[base] = max(parent_ID_counter[base], num)\n",
        "\n",
        "            vendor_ID = row['Vendor ID']\n",
        "            if pd.notna(vendor_ID):\n",
        "                vendor_num = int(vendor_ID.split('-')[1])\n",
        "                vendor_ID_counter[parent_ID][row['Parent Vendor']] = max(\n",
        "                    vendor_ID_counter[parent_ID][row['Parent Vendor']], vendor_num)\n",
        "\n",
        "    # Process new suppliers\n",
        "    new_entries = []\n",
        "    for supplier in new_suppliers:\n",
        "        if supplier not in unified_lookup['Supplier Name'].values:\n",
        "            parent_ID = generate_parent_ID(supplier, parent_ID_counter)\n",
        "            vendor_ID_counter[parent_ID][supplier] += 1\n",
        "            vendor_ID = f\"{parent_ID}-{vendor_ID_counter[parent_ID][supplier]}\"\n",
        "\n",
        "            new_entry = {\n",
        "                'Supplier Name': supplier,\n",
        "                'Parent Vendor': supplier,\n",
        "                'Parent Vendor ID': parent_ID,\n",
        "                'Vendor ID': vendor_ID,\n",
        "                'ST Category': '',\n",
        "                'ST Sub-Category': '',\n",
        "                'Opportunity Term': '',\n",
        "                'Est. Savings % (Low)': 0,\n",
        "                'Est. Savings % (High)': 0,\n",
        "                'Marketplace Designation': '',\n",
        "                'Notes': ''\n",
        "            }\n",
        "            new_entries.append(new_entry)\n",
        "\n",
        "    # Concatenate new entries with existing table\n",
        "    if new_entries:\n",
        "        new_entries_df = pd.DataFrame(new_entries)\n",
        "        updated_lookup = pd.concat([unified_lookup, new_entries_df], ignore_index=True)\n",
        "    else:\n",
        "        updated_lookup = unified_lookup\n",
        "\n",
        "    return updated_lookup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykLH4bjbB1Dt"
      },
      "source": [
        "Merging Data:\n",
        "\n",
        "*  Combines the original DataFrame (df) with the unified vendor lookup table\n",
        "*  Uses a left join on the 'Supplier Name' column\n",
        "\n",
        "\n",
        "Calculating Total Cost:\n",
        "\n",
        "*  Checks if 'Total' column already exists\n",
        "*  If not, attempts to calculate it by multiplying 'Quantity in UOM' and 'Price'\n",
        "*  If unable to calculate (due to missing columns), sets 'Total' to NaN and issues a warning\n",
        "\n",
        "\n",
        "Estimating Savings:\n",
        "\n",
        "*  Calculates low-end estimated savings: 'Est. Savings (Low)'\n",
        "\n",
        "*  Multiplies 'Total' by 'Est. Savings % (Low)' (converted to decimal)\n",
        "\n",
        "\n",
        "*  Calculates high-end estimated savings: 'Est. Savings (High)'\n",
        "\n",
        "*  Multiplies 'Total' by 'Est. Savings % (High)' (converted to decimal)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Handling Missing Data:\n",
        "\n",
        "*  Replaces any NaN values in the new savings columns with 0\n",
        "*  Ensures no null values in the estimated savings columns\n",
        "\n",
        "\n",
        "Returns the final DataFrame with all original data, vendor lookup information, and calculated fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po9_7FKsxKSx"
      },
      "outputs": [],
      "source": [
        "def merge_vendor_data(df, unified_lookup):\n",
        "    \"\"\"Merge vendor data with the original DataFrame and calculate estimated savings.\"\"\"\n",
        "    # Merge with unified lookup\n",
        "    final_df = pd.merge(df, unified_lookup, on='Supplier Name', how='left')\n",
        "\n",
        "    # Calculate Total if it doesn't exist\n",
        "    if 'Total' not in final_df.columns:\n",
        "        if 'Quantity in UOM' in final_df.columns and 'Price' in final_df.columns:\n",
        "            final_df['Total'] = final_df['Quantity in UOM'] * final_df['Price']\n",
        "        else:\n",
        "            print(\"Warning: Unable to calculate Total. 'Quantity in UOM' or 'Price' column missing.\")\n",
        "            final_df['Total'] = np.nan\n",
        "\n",
        "    # Calculate estimated savings\n",
        "    final_df['Est. Savings (Low)'] = final_df['Total'] * (final_df['Est. Savings % (Low)'])\n",
        "    final_df['Est. Savings (High)'] = final_df['Total'] * (final_df['Est. Savings % (High)'])\n",
        "\n",
        "    # Handle potential NaN values in the new columns\n",
        "    final_df['Est. Savings (Low)'] = final_df['Est. Savings (Low)'].fillna(0)\n",
        "    final_df['Est. Savings (High)'] = final_df['Est. Savings (High)'].fillna(0)\n",
        "\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2zftRbdCcWW"
      },
      "source": [
        "create_missing_raw_data_report(df, output_path):\n",
        "\n",
        "*  Identifies rows with missing data in specific columns\n",
        "*  Columns checked: Supplier ID, Supplier Name, Item Description, Item Group, Product Group, UOM, Quantity, Quantity in UOM, Price\n",
        "*  Creates a new DataFrame with only the rows containing missing data\n",
        "*  Saves this DataFrame to an Excel file at the specified output path\n",
        "*  Returns the DataFrame of missing data\n",
        "\n",
        "\n",
        "create_missing_vendor_info_report(final_df, unified_lookup, output_path):\n",
        "\n",
        "*  Identifies vendors in the client's data\n",
        "*  Checks for missing information in the unified vendor lookup for these vendors\n",
        "*  Columns checked: ST Category, ST Sub-Category, Opportunity Term, Est. Savings % (Low), Est. Savings % (High), Marketplace Designation\n",
        "*  Creates a DataFrame of vendors with missing information\n",
        "*  Saves this DataFrame to an Excel file at the specified output path\n",
        "*  Returns the DataFrame of vendors with missing information\n",
        "\n",
        "\n",
        "calculate_missing_data_percentage(df):\n",
        "\n",
        "*  Calculates the percentage of missing data for each column in the DataFrame\n",
        "*  Returns a sorted Series with columns and their missing data percentages\n",
        "\n",
        "\n",
        "update_df_with_missing_data(original_df, missing_data):\n",
        "\n",
        "*  Attempts to update the original DataFrame with data from a missing data file\n",
        "*  Uses 'Supplier Name', 'SKU', and 'Item Description' as key columns for matching\n",
        "*  Merges the original DataFrame with the missing data\n",
        "*  Updates existing rows where new data is available\n",
        "*  Adds completely new rows from the missing data file\n",
        "*  Handles cases where key columns might be missing\n",
        "*  Returns the updated DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2EyKOb5xP7m"
      },
      "outputs": [],
      "source": [
        "def create_missing_raw_data_report(df, output_path):\n",
        "    \"\"\"Create a report of missing raw data from the client's upload.\"\"\"\n",
        "    columns_to_check = ['Supplier ID', 'Supplier Name', 'Item Description', 'Item Group',\n",
        "                        'Product Group', 'UOM', 'Quantity', 'Quantity in UOM', 'Price']\n",
        "\n",
        "    missing_mask = df[columns_to_check].isnull().any(axis=1)\n",
        "    missing_data = df[missing_mask]\n",
        "    missing_data.to_excel(output_path, index=False)\n",
        "    print(f\"Missing raw data report saved to {output_path}\")\n",
        "    return missing_data\n",
        "\n",
        "def create_missing_vendor_info_report(final_df, unified_lookup, output_path):\n",
        "    \"\"\"Create a report of missing vendor information for vendors in the client's data.\"\"\"\n",
        "    columns_to_check = ['ST Category', 'ST Sub-Category', 'Opportunity Term',\n",
        "                        'Est. Savings % (Low)', 'Est. Savings % (High)', 'Marketplace Designation']\n",
        "\n",
        "    client_vendors = final_df['Vendor ID'].unique()\n",
        "    vendor_info = unified_lookup[unified_lookup['Vendor ID'].isin(client_vendors)]\n",
        "    missing_mask = vendor_info[columns_to_check].isnull().any(axis=1)\n",
        "    missing_vendor_info = vendor_info[missing_mask]\n",
        "    missing_vendor_info.to_excel(output_path, index=False)\n",
        "    print(f\"Missing vendor information report saved to {output_path}\")\n",
        "    return missing_vendor_info\n",
        "\n",
        "def calculate_missing_data_percentage(df):\n",
        "    \"\"\"Calculate the percentage of missing data for each column.\"\"\"\n",
        "    total_rows = len(df)\n",
        "    missing_percentages = (df.isnull().sum() / total_rows * 100).round(2)\n",
        "    return missing_percentages.sort_values(ascending=False)\n",
        "\n",
        "def update_df_with_missing_data(original_df, missing_data):\n",
        "    \"\"\"Update the original DataFrame with data from the missing data file.\"\"\"\n",
        "    key_columns = ['Supplier Name', 'SKU', 'Item Description']\n",
        "\n",
        "    for col in key_columns.copy():\n",
        "        if col not in original_df.columns or col not in missing_data.columns:\n",
        "            print(f\"Warning: Key column '{col}' not found in one of the DataFrames. Skipping this column for matching.\")\n",
        "            key_columns.remove(col)\n",
        "\n",
        "    if not key_columns:\n",
        "        raise ValueError(\"No valid key columns for matching. Cannot update DataFrame.\")\n",
        "\n",
        "    updated_df = pd.merge(\n",
        "        original_df,\n",
        "        missing_data,\n",
        "        on=key_columns,\n",
        "        how='left',\n",
        "        suffixes=('', '_new')\n",
        "    )\n",
        "\n",
        "    for col in missing_data.columns:\n",
        "        if col in original_df.columns and col not in key_columns:\n",
        "            mask = updated_df[f'{col}_new'].notna()\n",
        "            updated_df.loc[mask, col] = updated_df.loc[mask, f'{col}_new']\n",
        "\n",
        "    updated_df = updated_df.drop(columns=[col for col in updated_df.columns if col.endswith('_new')])\n",
        "\n",
        "    new_rows = missing_data[~missing_data[key_columns].isin(original_df[key_columns].to_dict('list')).all(axis=1)]\n",
        "    if not new_rows.empty:\n",
        "        updated_df = pd.concat([updated_df, new_rows], ignore_index=True)\n",
        "        print(f\"Added {len(new_rows)} completely new rows from missing data.\")\n",
        "\n",
        "    return updated_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRNk7_eJ1aow"
      },
      "source": [
        "process_file(df, column_mapping):\n",
        "\n",
        "This function is a lower-level utility that performs specific data cleaning and transformation tasks on a DataFrame. It:\n",
        "\n",
        "*  Renames columns based on a provided mapping\n",
        "*  Handles duplicate columns\n",
        "*  Converts specific columns to appropriate data types (datetime, numeric)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCduPBb5ZpYN"
      },
      "outputs": [],
      "source": [
        "def process_file(df, column_mapping):\n",
        "    # Rename columns based on the mapping\n",
        "    df_renamed = df.rename(columns={v: k for k, v in column_mapping.items()})\n",
        "\n",
        "    # Check for duplicate columns\n",
        "    duplicate_columns = df_renamed.columns[df_renamed.columns.duplicated()].tolist()\n",
        "    if duplicate_columns:\n",
        "        print(f\"Warning: Duplicate columns found after renaming: {duplicate_columns}\")\n",
        "        # Append a suffix to duplicate columns\n",
        "        for col in duplicate_columns:\n",
        "            mask = df_renamed.columns == col\n",
        "            df_renamed.columns = [f'{col}_{i}' if x else col for i, x in enumerate(mask)]\n",
        "\n",
        "    # Perform some basic processing\n",
        "    if 'Date' in df_renamed.columns:\n",
        "        df_renamed['Date'] = pd.to_datetime(df_renamed['Date'], errors='coerce')\n",
        "\n",
        "    if 'Price' in df_renamed.columns:\n",
        "        df_renamed['Price'] = pd.to_numeric(df_renamed['Price'], errors='coerce')\n",
        "    if 'Quantity' in df_renamed.columns:\n",
        "        df_renamed['Quantity'] = pd.to_numeric(df_renamed['Quantity'], errors='coerce')\n",
        "    if 'Quantity in UOM' in df_renamed.columns:\n",
        "        df_renamed['Quantity in UOM'] = pd.to_numeric(df_renamed['Quantity in UOM'], errors='coerce')\n",
        "\n",
        "    return df_renamed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xjvQGn81ciQ"
      },
      "source": [
        "process_client_input(file_path):\n",
        "\n",
        "This is a higher-level function that orchestrates the entire process of loading and initial processing of the client's input file. It:\n",
        "\n",
        "*  Loads the catalog\n",
        "*  Extracts the client name from the file name\n",
        "*  Loads the Excel workbook and finds the correct sheet\n",
        "*  Calls interactive_column_mapping to get the column mapping\n",
        "*  Calls process_file to apply the mapping and perform basic data cleaning\n",
        "*  Adds required columns like 'Total', 'Stage', and 'Client Name'\n",
        "*  Handles the case where the 'Date' column is missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kha7JhrBaKdx"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_client_input(file_path):\n",
        "    \"\"\"\n",
        "    Process the client's input Excel file.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): The path to the client's Excel file.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - pd.DataFrame: The processed DataFrame with mapped columns and additional required columns.\n",
        "        - dict: The updated catalog.\n",
        "    \"\"\"\n",
        "    # Load the catalog\n",
        "    catalog = load_or_create_catalog()\n",
        "\n",
        "    # Extract client name from the file name\n",
        "    client_name = os.path.basename(file_path).split(' - ')[0]\n",
        "\n",
        "    # Load the workbook\n",
        "    wb = load_workbook(file_path, read_only=True, data_only=True)\n",
        "\n",
        "    # Find the sheet with 'Data' in its name\n",
        "    data_sheet = next((sheet for sheet in wb.sheetnames if 'data' in sheet.lower()), None)\n",
        "    if not data_sheet:\n",
        "        raise ValueError(\"No sheet with 'Data' in its name found in the workbook.\")\n",
        "\n",
        "    # Read the Excel file\n",
        "    df = pd.read_excel(file_path, sheet_name=data_sheet)\n",
        "\n",
        "    print(\"Original DataFrame columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    # Perform interactive column mapping\n",
        "    column_mapping, updated_catalog = interactive_column_mapping(df.columns, catalog)\n",
        "\n",
        "    # Process the file with the new mapping\n",
        "    df = process_file(df, column_mapping)\n",
        "\n",
        "    # Filter the DataFrame to include only the columns from our catalog\n",
        "    catalog_columns = list(column_mapping.keys())\n",
        "    df = df[catalog_columns]\n",
        "\n",
        "    # Add required columns\n",
        "    if 'Quantity in UOM' in df.columns and 'Price' in df.columns:\n",
        "        df['Total'] = df['Quantity in UOM'] * df['Price']\n",
        "    else:\n",
        "        print(\"Warning: 'Quantity in UOM' or 'Price' column not found. 'Total' column not added.\")\n",
        "\n",
        "    df['Stage'] = 'Opportunity Assessment'\n",
        "    df['Client Name'] = client_name\n",
        "\n",
        "    # If 'Date' column is missing, add it with a default value\n",
        "    if 'Date' not in df.columns:\n",
        "        print(\"Warning: 'Date' column is missing. Adding it with a default value.\")\n",
        "        df['Date'] = pd.Timestamp.now()\n",
        "\n",
        "    print(\"\\nProcessed DataFrame:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nProcessed DataFrame columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    return df, updated_catalog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGsr33voDnlu"
      },
      "source": [
        "Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4xJ_dmueBux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94722d9-c2f1-438c-d67d-b747b8fae1c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting run_example function...\n",
            "Loading unified vendor lookup table...\n",
            "\n",
            "Unified lookup table columns:\n",
            "['Supplier Name', 'Parent Vendor', 'Parent Vendor ID', 'Vendor ID', 'ST Category', 'ST Sub-Category', 'Opportunity Term', 'Est. Savings % (Low)', 'Est. Savings % (High)', 'Marketplace Designation', 'Notes']\n",
            "\n",
            "First few rows of unified lookup table:\n",
            "                          Supplier Name                   Parent Vendor  \\\n",
            "0  419 Neon LLC                                            419 Neon LLC   \n",
            "1  48Forty Solutions, LLC                        48Forty Solutions, LLC   \n",
            "2  4C2 Electrical Associates, LLC        4C2 Electrical Associates, LLC   \n",
            "3  A & A Machine Works, Inc.                  A & A Machine Works, Inc.   \n",
            "4  A & D Flexographic Repair LLC          A & D Flexographic Repair LLC   \n",
            "\n",
            "  Parent Vendor ID Vendor ID                      ST Category  \\\n",
            "0           4NL001  4NL001-1                              NaN   \n",
            "1           4SL001  4SL001-1               Packaging Supplies   \n",
            "2           4EA001  4EA001-1                              NaN   \n",
            "3           A&A001  A&A001-1                              NaN   \n",
            "4           A&D001  A&D001-1  Facility & Maintenance Services   \n",
            "\n",
            "       ST Sub-Category Opportunity Term  Est. Savings % (Low)  \\\n",
            "0                  NaN              NaN                   NaN   \n",
            "1              Pallets              NaN                   NaN   \n",
            "2                  NaN              NaN                   NaN   \n",
            "3                  NaN              NaN                   NaN   \n",
            "4  General Maintenance              NaN                   NaN   \n",
            "\n",
            "   Est. Savings % (High) Marketplace Designation Notes  \n",
            "0                    NaN                     NaN   NaN  \n",
            "1                    NaN                     TBD   NaN  \n",
            "2                    NaN                       N   NaN  \n",
            "3                    NaN                       N   NaN  \n",
            "4                    NaN                     TBD   NaN  \n",
            "\n",
            "Processing client input...\n",
            "Original DataFrame columns:\n",
            "['Supplier Name', 'Supplier Number', 'Supplier Group Name', 'Supplier Group Number', 'Product', 'Product Description', 'Gauge', 'Item Group', 'Product Group', 'Color', 'Finish', 'Material Code', 'Material Description', 'Width', 'Sheet Length', 'Division', 'Received Year', 'Purchase Order Date', 'Facility Code', 'Warehouse', 'Pounds Received', 'Purchase Price', 'Order Qty', 'Purchase UOM', 'Qty Received In Purch UOM', 'Cost', 'Purchase Order Number', 'Purchase Order Line Number', 'Purchase Order Type', 'Purchase Order Our Reference Number', 'PO Line Order Type', 'PO Line Our Reference Number', 'Received Date', 'System', 'Supplier Item Number', 'BFMPC', 'Requested Date', 'Confirmed Date', 'Final Destination', \"Price * Qty Rec'd\", 'Include?', 'Include for Savings?', 'Parent Vendor', 'ST Category', 'ST Sub-Category', 'Opportunity Term', 'Est. Savings % (Low)', 'Est. Savings % (High)', 'Est Savings $ (Low)', 'Est. Savings $ (High)', 'Market Place Designation']\n",
            "Initial column mapping:\n",
            "Supplier ID: Supplier Number\n",
            "Supplier Name: Supplier Name\n",
            "Item Description: Product Description\n",
            "SKU: Supplier Item Number\n",
            "Item Group: Item Group\n",
            "Product Group: Product Group\n",
            "UOM: Purchase UOM\n",
            "Quantity: Order Qty\n",
            "Quantity in UOM: Qty Received In Purch UOM\n",
            "Price: Purchase Price\n",
            "Date: Received Date\n",
            "\n",
            "Unmatched columns:\n",
            "\n",
            "Processed DataFrame:\n",
            "   Supplier ID                         Supplier Name  \\\n",
            "0        55311  Mid-Ohio Wood Products                 \n",
            "1         9141  Bunzl Processor Division               \n",
            "2        89437  Hel, Inc.                              \n",
            "3        54448  Mactac, Inc.                           \n",
            "4        89959  Rocheux International, Inc.            \n",
            "\n",
            "                 Item Description                             SKU  \\\n",
            "0  PALLET 42X42\"                                                    \n",
            "1  OPERATION SUPPLIES                                               \n",
            "2  SERVICES-NON MATERIAL                                            \n",
            "3  .00400 RM FPVC.TC2N.PS CL 0     4-1550                           \n",
            "4  .01500 RM PVC WHTOPQ MM         BPGBMXX                          \n",
            "\n",
            "                  Item Group       Product Group  UOM  Quantity  \\\n",
            "0                     PALLET  PACKAGING MATERIAL  EA      200.0   \n",
            "1  SUBCONTRACTED ITEMS GROUP       MISCELLANEOUS  EA       12.0   \n",
            "2  SUBCONTRACTED ITEMS GROUP       MISCELLANEOUS  EA        1.0   \n",
            "3              TRANSCLING II  Vinyl Print family  FT    10000.0   \n",
            "4                VINYL GROUP  Vinyl Print family  LBS   15000.0   \n",
            "\n",
            "   Quantity in UOM      Price       Date     Total                   Stage  \\\n",
            "0            100.0    18.7500 2024-06-26   1875.00  Opportunity Assessment   \n",
            "1             12.0     4.9900 2024-07-03     59.88  Opportunity Assessment   \n",
            "2              1.0  1637.0000 2024-03-17   1637.00  Opportunity Assessment   \n",
            "3          10000.0     0.8165 2024-07-30   8165.00  Opportunity Assessment   \n",
            "4           6489.0     1.7600 2024-01-05  11420.64  Opportunity Assessment   \n",
            "\n",
            "   Client Name  \n",
            "0  Transcendia  \n",
            "1  Transcendia  \n",
            "2  Transcendia  \n",
            "3  Transcendia  \n",
            "4  Transcendia  \n",
            "\n",
            "Processed DataFrame columns:\n",
            "['Supplier ID', 'Supplier Name', 'Item Description', 'SKU', 'Item Group', 'Product Group', 'UOM', 'Quantity', 'Quantity in UOM', 'Price', 'Date', 'Total', 'Stage', 'Client Name']\n",
            "\n",
            "Client input DataFrame columns:\n",
            "['Supplier ID', 'Supplier Name', 'Item Description', 'SKU', 'Item Group', 'Product Group', 'UOM', 'Quantity', 'Quantity in UOM', 'Price', 'Date', 'Total', 'Stage', 'Client Name']\n",
            "\n",
            "First few rows of client input DataFrame:\n",
            "   Supplier ID                         Supplier Name  \\\n",
            "0        55311  Mid-Ohio Wood Products                 \n",
            "1         9141  Bunzl Processor Division               \n",
            "2        89437  Hel, Inc.                              \n",
            "3        54448  Mactac, Inc.                           \n",
            "4        89959  Rocheux International, Inc.            \n",
            "\n",
            "                 Item Description                             SKU  \\\n",
            "0  PALLET 42X42\"                                                    \n",
            "1  OPERATION SUPPLIES                                               \n",
            "2  SERVICES-NON MATERIAL                                            \n",
            "3  .00400 RM FPVC.TC2N.PS CL 0     4-1550                           \n",
            "4  .01500 RM PVC WHTOPQ MM         BPGBMXX                          \n",
            "\n",
            "                  Item Group       Product Group  UOM  Quantity  \\\n",
            "0                     PALLET  PACKAGING MATERIAL  EA      200.0   \n",
            "1  SUBCONTRACTED ITEMS GROUP       MISCELLANEOUS  EA       12.0   \n",
            "2  SUBCONTRACTED ITEMS GROUP       MISCELLANEOUS  EA        1.0   \n",
            "3              TRANSCLING II  Vinyl Print family  FT    10000.0   \n",
            "4                VINYL GROUP  Vinyl Print family  LBS   15000.0   \n",
            "\n",
            "   Quantity in UOM      Price       Date     Total                   Stage  \\\n",
            "0            100.0    18.7500 2024-06-26   1875.00  Opportunity Assessment   \n",
            "1             12.0     4.9900 2024-07-03     59.88  Opportunity Assessment   \n",
            "2              1.0  1637.0000 2024-03-17   1637.00  Opportunity Assessment   \n",
            "3          10000.0     0.8165 2024-07-30   8165.00  Opportunity Assessment   \n",
            "4           6489.0     1.7600 2024-01-05  11420.64  Opportunity Assessment   \n",
            "\n",
            "   Client Name  \n",
            "0  Transcendia  \n",
            "1  Transcendia  \n",
            "2  Transcendia  \n",
            "3  Transcendia  \n",
            "4  Transcendia  \n",
            "\n",
            "Creating missing raw data report...\n",
            "Missing raw data report saved to /content/drive/MyDrive/Seismic/Seismic Python Automation/missing_raw_data.xlsx\n",
            "Missing raw data report saved to /content/drive/MyDrive/Seismic/Seismic Python Automation/missing_raw_data.xlsx\n",
            "\n",
            "Percentage of missing data in raw data:\n",
            "Item Group          0.08\n",
            "Supplier ID         0.00\n",
            "Supplier Name       0.00\n",
            "Item Description    0.00\n",
            "SKU                 0.00\n",
            "Product Group       0.00\n",
            "UOM                 0.00\n",
            "Quantity            0.00\n",
            "Quantity in UOM     0.00\n",
            "Price               0.00\n",
            "Date                0.00\n",
            "Total               0.00\n",
            "Stage               0.00\n",
            "Client Name         0.00\n",
            "dtype: float64\n",
            "\n",
            "Updating unified vendor lookup with new suppliers...\n",
            "\n",
            "Merging vendor data...\n",
            "Estimated savings calculated.\n",
            "\n",
            "Final DataFrame columns after merging:\n",
            "['Supplier ID', 'Supplier Name', 'Item Description', 'SKU', 'Item Group', 'Product Group', 'UOM', 'Quantity', 'Quantity in UOM', 'Price', 'Date', 'Total', 'Stage', 'Client Name', 'Parent Vendor', 'Parent Vendor ID', 'Vendor ID', 'ST Category', 'ST Sub-Category', 'Opportunity Term', 'Est. Savings % (Low)', 'Est. Savings % (High)', 'Marketplace Designation', 'Notes', 'Est. Savings (Low)', 'Est. Savings (High)']\n",
            "\n",
            "First few rows of final DataFrame after merging:\n",
            "   Supplier ID                         Supplier Name  \\\n",
            "0        55311  Mid-Ohio Wood Products                 \n",
            "1         9141  Bunzl Processor Division               \n",
            "2        89437  Hel, Inc.                              \n",
            "3        54448  Mactac, Inc.                           \n",
            "4        89959  Rocheux International, Inc.            \n",
            "\n",
            "                 Item Description                             SKU  \\\n",
            "0  PALLET 42X42\"                                                    \n",
            "1  OPERATION SUPPLIES                                               \n",
            "2  SERVICES-NON MATERIAL                                            \n",
            "3  .00400 RM FPVC.TC2N.PS CL 0     4-1550                           \n",
            "4  .01500 RM PVC WHTOPQ MM         BPGBMXX                          \n",
            "\n",
            "                  Item Group       Product Group  UOM  Quantity  \\\n",
            "0                     PALLET  PACKAGING MATERIAL  EA      200.0   \n",
            "1  SUBCONTRACTED ITEMS GROUP       MISCELLANEOUS  EA       12.0   \n",
            "2  SUBCONTRACTED ITEMS GROUP       MISCELLANEOUS  EA        1.0   \n",
            "3              TRANSCLING II  Vinyl Print family  FT    10000.0   \n",
            "4                VINYL GROUP  Vinyl Print family  LBS   15000.0   \n",
            "\n",
            "   Quantity in UOM      Price       Date     Total                   Stage  \\\n",
            "0            100.0    18.7500 2024-06-26   1875.00  Opportunity Assessment   \n",
            "1             12.0     4.9900 2024-07-03     59.88  Opportunity Assessment   \n",
            "2              1.0  1637.0000 2024-03-17   1637.00  Opportunity Assessment   \n",
            "3          10000.0     0.8165 2024-07-30   8165.00  Opportunity Assessment   \n",
            "4           6489.0     1.7600 2024-01-05  11420.64  Opportunity Assessment   \n",
            "\n",
            "   Client Name                         Parent Vendor Parent Vendor ID  \\\n",
            "0  Transcendia  Mid-Ohio Wood Products                         MWP002   \n",
            "1  Transcendia              Bunzl Processor Division           BPD001   \n",
            "2  Transcendia                             Hel, Inc.            HI002   \n",
            "3  Transcendia                          Mactac, Inc.            MI002   \n",
            "4  Transcendia                 Rocheux International            RI002   \n",
            "\n",
            "  Vendor ID       ST Category              ST Sub-Category Opportunity Term  \\\n",
            "0  MWP002-1               NaN                          NaN              NaN   \n",
            "1  BPD001-1               MRO  General Industrial Supplies             Near   \n",
            "2   HI002-1               NaN                          NaN              NaN   \n",
            "3   MI002-1  Direct Materials                         Film              NaN   \n",
            "4   RI002-2  Direct Materials                         Film              NaN   \n",
            "\n",
            "   Est. Savings % (Low)  Est. Savings % (High) Marketplace Designation  \\\n",
            "0                   0.0                   0.00                     NaN   \n",
            "1                   0.1                   0.15                       G   \n",
            "2                   NaN                    NaN                     TBD   \n",
            "3                   NaN                    NaN                     TBD   \n",
            "4                   NaN                    NaN                     TBD   \n",
            "\n",
            "                          Notes  Est. Savings (Low)  Est. Savings (High)  \n",
            "0                           NaN               0.000                0.000  \n",
            "1                           NaN               5.988                8.982  \n",
            "2                           NaN                 NaN                  NaN  \n",
            "3  film and specialty adhesives                 NaN                  NaN  \n",
            "4                          film                 NaN                  NaN  \n",
            "\n",
            "Creating missing vendor info report...\n",
            "Missing vendor information report saved to /content/drive/MyDrive/Seismic/Seismic Python Automation/missing_vendor_info.xlsx\n",
            "Missing vendor information report saved to /content/drive/MyDrive/Seismic/Seismic Python Automation/missing_vendor_info.xlsx\n",
            "\n",
            "Percentage of missing data in vendor information:\n",
            "Opportunity Term           100.00\n",
            "Est. Savings % (Low)        99.90\n",
            "Est. Savings % (High)       99.90\n",
            "Notes                       93.56\n",
            "ST Sub-Category             62.98\n",
            "ST Category                 60.66\n",
            "Marketplace Designation      0.20\n",
            "Parent Vendor ID             0.00\n",
            "Parent Vendor                0.00\n",
            "Supplier Name                0.00\n",
            "Vendor ID                    0.00\n",
            "dtype: float64\n",
            "\n",
            "Calculating missing data percentages for final DataFrame...\n",
            "\n",
            "Percentage of missing data in final DataFrame:\n",
            "Notes                      77.80\n",
            "Opportunity Term           72.93\n",
            "Est. Savings % (High)      71.79\n",
            "Est. Savings % (Low)       71.79\n",
            "Est. Savings (High)        71.79\n",
            "Est. Savings (Low)         71.79\n",
            "ST Sub-Category            20.60\n",
            "ST Category                16.17\n",
            "Marketplace Designation     1.16\n",
            "Item Group                  0.08\n",
            "Price                       0.00\n",
            "Quantity in UOM             0.00\n",
            "Quantity                    0.00\n",
            "UOM                         0.00\n",
            "Product Group               0.00\n",
            "SKU                         0.00\n",
            "Item Description            0.00\n",
            "Supplier Name               0.00\n",
            "Supplier ID                 0.00\n",
            "Vendor ID                   0.00\n",
            "Parent Vendor               0.00\n",
            "Parent Vendor ID            0.00\n",
            "Total                       0.00\n",
            "Stage                       0.00\n",
            "Date                        0.00\n",
            "Client Name                 0.00\n",
            "dtype: float64\n",
            "\n",
            "Identifying missing suppliers...\n",
            "Number of missing suppliers: 0\n",
            "\n",
            "Final DataFrame shape: (33027, 26)\n",
            "\n",
            "Final DataFrame columns:\n",
            "['Supplier ID', 'Supplier Name', 'Item Description', 'SKU', 'Item Group', 'Product Group', 'UOM', 'Quantity', 'Quantity in UOM', 'Price', 'Date', 'Total', 'Stage', 'Client Name', 'Parent Vendor', 'Parent Vendor ID', 'Vendor ID', 'ST Category', 'ST Sub-Category', 'Opportunity Term', 'Est. Savings % (Low)', 'Est. Savings % (High)', 'Marketplace Designation', 'Notes', 'Est. Savings (Low)', 'Est. Savings (High)']\n",
            "\n",
            "First few rows of the final DataFrame:\n",
            "   Supplier ID                         Supplier Name  \\\n",
            "0        55311  Mid-Ohio Wood Products                 \n",
            "1         9141  Bunzl Processor Division               \n",
            "2        89437  Hel, Inc.                              \n",
            "3        54448  Mactac, Inc.                           \n",
            "4        89959  Rocheux International, Inc.            \n",
            "\n",
            "                 Item Description                             SKU  \\\n",
            "0  PALLET 42X42\"                                                    \n",
            "1  OPERATION SUPPLIES                                               \n",
            "2  SERVICES-NON MATERIAL                                            \n",
            "3  .00400 RM FPVC.TC2N.PS CL 0     4-1550                           \n",
            "4  .01500 RM PVC WHTOPQ MM         BPGBMXX                          \n",
            "\n",
            "                  Item Group       Product Group  UOM  Quantity  \\\n",
            "0                     PALLET  PACKAGING MATERIAL  EA      200.0   \n",
            "1  SUBCONTRACTED ITEMS GROUP       MISCELLANEOUS  EA       12.0   \n",
            "2  SUBCONTRACTED ITEMS GROUP       MISCELLANEOUS  EA        1.0   \n",
            "3              TRANSCLING II  Vinyl Print family  FT    10000.0   \n",
            "4                VINYL GROUP  Vinyl Print family  LBS   15000.0   \n",
            "\n",
            "   Quantity in UOM      Price       Date     Total                   Stage  \\\n",
            "0            100.0    18.7500 2024-06-26   1875.00  Opportunity Assessment   \n",
            "1             12.0     4.9900 2024-07-03     59.88  Opportunity Assessment   \n",
            "2              1.0  1637.0000 2024-03-17   1637.00  Opportunity Assessment   \n",
            "3          10000.0     0.8165 2024-07-30   8165.00  Opportunity Assessment   \n",
            "4           6489.0     1.7600 2024-01-05  11420.64  Opportunity Assessment   \n",
            "\n",
            "   Client Name                         Parent Vendor Parent Vendor ID  \\\n",
            "0  Transcendia  Mid-Ohio Wood Products                         MWP002   \n",
            "1  Transcendia              Bunzl Processor Division           BPD001   \n",
            "2  Transcendia                             Hel, Inc.            HI002   \n",
            "3  Transcendia                          Mactac, Inc.            MI002   \n",
            "4  Transcendia                 Rocheux International            RI002   \n",
            "\n",
            "  Vendor ID       ST Category              ST Sub-Category Opportunity Term  \\\n",
            "0  MWP002-1               NaN                          NaN              NaN   \n",
            "1  BPD001-1               MRO  General Industrial Supplies             Near   \n",
            "2   HI002-1               NaN                          NaN              NaN   \n",
            "3   MI002-1  Direct Materials                         Film              NaN   \n",
            "4   RI002-2  Direct Materials                         Film              NaN   \n",
            "\n",
            "   Est. Savings % (Low)  Est. Savings % (High) Marketplace Designation  \\\n",
            "0                   0.0                   0.00                     NaN   \n",
            "1                   0.1                   0.15                       G   \n",
            "2                   NaN                    NaN                     TBD   \n",
            "3                   NaN                    NaN                     TBD   \n",
            "4                   NaN                    NaN                     TBD   \n",
            "\n",
            "                          Notes  Est. Savings (Low)  Est. Savings (High)  \n",
            "0                           NaN               0.000                0.000  \n",
            "1                           NaN               5.988                8.982  \n",
            "2                           NaN                 NaN                  NaN  \n",
            "3  film and specialty adhesives                 NaN                  NaN  \n",
            "4                          film                 NaN                  NaN  \n",
            "\n",
            "Total Spend: $233,021,958.29\n",
            "Estimated Low Savings: $150,156.35 (0.06%)\n",
            "Estimated High Savings: $233,587.32 (0.10%)\n",
            "\n",
            "Saving updated unified lookup...\n",
            "Updated unified Vendor Look Up table saved.\n",
            "\n",
            "Saving updated catalog...\n",
            "Updated catalog saved to: /content/drive/MyDrive/Seismic/Seismic Python Automation/updated_catalog.json\n",
            "\n",
            "Missing Data Summary:\n",
            "Raw Data Missing Percentages:\n",
            "Item Group          0.08\n",
            "Supplier ID         0.00\n",
            "Supplier Name       0.00\n",
            "Item Description    0.00\n",
            "SKU                 0.00\n",
            "Product Group       0.00\n",
            "UOM                 0.00\n",
            "Quantity            0.00\n",
            "Quantity in UOM     0.00\n",
            "Price               0.00\n",
            "Date                0.00\n",
            "Total               0.00\n",
            "Stage               0.00\n",
            "Client Name         0.00\n",
            "dtype: float64\n",
            "\n",
            "Vendor Info Missing Percentages:\n",
            "Opportunity Term           100.00\n",
            "Est. Savings % (Low)        99.90\n",
            "Est. Savings % (High)       99.90\n",
            "Notes                       93.56\n",
            "ST Sub-Category             62.98\n",
            "ST Category                 60.66\n",
            "Marketplace Designation      0.20\n",
            "Parent Vendor ID             0.00\n",
            "Parent Vendor                0.00\n",
            "Supplier Name                0.00\n",
            "Vendor ID                    0.00\n",
            "dtype: float64\n",
            "\n",
            "Final DataFrame Missing Percentages:\n",
            "Notes                      77.80\n",
            "Opportunity Term           72.93\n",
            "Est. Savings % (High)      71.79\n",
            "Est. Savings % (Low)       71.79\n",
            "Est. Savings (High)        71.79\n",
            "Est. Savings (Low)         71.79\n",
            "ST Sub-Category            20.60\n",
            "ST Category                16.17\n",
            "Marketplace Designation     1.16\n",
            "Item Group                  0.08\n",
            "Price                       0.00\n",
            "Quantity in UOM             0.00\n",
            "Quantity                    0.00\n",
            "UOM                         0.00\n",
            "Product Group               0.00\n",
            "SKU                         0.00\n",
            "Item Description            0.00\n",
            "Supplier Name               0.00\n",
            "Supplier ID                 0.00\n",
            "Vendor ID                   0.00\n",
            "Parent Vendor               0.00\n",
            "Parent Vendor ID            0.00\n",
            "Total                       0.00\n",
            "Stage                       0.00\n",
            "Date                        0.00\n",
            "Client Name                 0.00\n",
            "dtype: float64\n",
            "Run completed successfully\n"
          ]
        }
      ],
      "source": [
        "def run_example():\n",
        "    unified_lookup_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/unified_vendor_lookup.xlsx\"\n",
        "    client_file_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/Transcendia - Opportunity Assessment.xlsx\"\n",
        "    missing_raw_data_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/missing_raw_data.xlsx\"\n",
        "    missing_vendor_info_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/missing_vendor_info.xlsx\"\n",
        "\n",
        "    print(\"Starting run_example function...\")\n",
        "\n",
        "    try:\n",
        "        # Load unified Vendor Look Up table\n",
        "        print(\"Loading unified vendor lookup table...\")\n",
        "        unified_lookup = load_unified_vendor_lookup(unified_lookup_path)\n",
        "\n",
        "        if unified_lookup is None:\n",
        "            raise ValueError(\"load_unified_vendor_lookup returned None\")\n",
        "\n",
        "        print(\"\\nUnified lookup table columns:\")\n",
        "        print(unified_lookup.columns.tolist())\n",
        "\n",
        "        print(\"\\nFirst few rows of unified lookup table:\")\n",
        "        print(unified_lookup.head())\n",
        "\n",
        "        # Process client input\n",
        "        print(\"\\nProcessing client input...\")\n",
        "        result = process_client_input(client_file_path)\n",
        "\n",
        "        if result is None:\n",
        "            raise ValueError(\"process_client_input returned None\")\n",
        "\n",
        "        # Handle the case where process_client_input returns a tuple\n",
        "        if isinstance(result, tuple) and len(result) == 2:\n",
        "            df, updated_catalog = result\n",
        "        elif isinstance(result, pd.DataFrame):\n",
        "            df = result\n",
        "            updated_catalog = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected return type from process_client_input: {type(result)}\")\n",
        "\n",
        "        if df is None or not isinstance(df, pd.DataFrame):\n",
        "            raise ValueError(f\"Invalid DataFrame returned from process_client_input: {type(df)}\")\n",
        "\n",
        "        print(\"\\nClient input DataFrame columns:\")\n",
        "        print(df.columns.tolist())\n",
        "\n",
        "        print(\"\\nFirst few rows of client input DataFrame:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Create missing raw data report\n",
        "        print(\"\\nCreating missing raw data report...\")\n",
        "        missing_raw_data = create_missing_raw_data_report(df, missing_raw_data_path)\n",
        "        print(f\"Missing raw data report saved to {missing_raw_data_path}\")\n",
        "\n",
        "        # Calculate percentage of missing raw data\n",
        "        raw_data_missing_percentages = calculate_missing_data_percentage(df)\n",
        "        print(\"\\nPercentage of missing data in raw data:\")\n",
        "        print(raw_data_missing_percentages)\n",
        "\n",
        "        # Get unique suppliers from raw data\n",
        "        new_suppliers = df['Supplier Name'].unique().tolist()\n",
        "\n",
        "        # Update unified vendor lookup with new suppliers\n",
        "        print(\"\\nUpdating unified vendor lookup with new suppliers...\")\n",
        "        unified_lookup = update_vendor_lookup(unified_lookup, new_suppliers)\n",
        "\n",
        "        # Merge vendor data\n",
        "        print(\"\\nMerging vendor data...\")\n",
        "        final_df = merge_vendor_data(df, unified_lookup)\n",
        "\n",
        "        if final_df is None:\n",
        "            raise ValueError(\"merge_vendor_data returned None\")\n",
        "\n",
        "        # Calculate estimated savings\n",
        "        if 'Total' in final_df.columns and 'Est. Savings % (Low)' in final_df.columns and 'Est. Savings % (High)' in final_df.columns:\n",
        "            final_df['Est. Savings (Low)'] = final_df['Total'] * (final_df['Est. Savings % (Low)'])\n",
        "            final_df['Est. Savings (High)'] = final_df['Total'] * (final_df['Est. Savings % (High)'])\n",
        "            print(\"Estimated savings calculated.\")\n",
        "        else:\n",
        "            print(\"Warning: Unable to calculate estimated savings. Required columns missing.\")\n",
        "\n",
        "        print(\"\\nFinal DataFrame columns after merging:\")\n",
        "        print(final_df.columns.tolist())\n",
        "\n",
        "        print(\"\\nFirst few rows of final DataFrame after merging:\")\n",
        "        print(final_df.head())\n",
        "\n",
        "        # Create missing vendor info report\n",
        "        print(\"\\nCreating missing vendor info report...\")\n",
        "        missing_vendor_info = create_missing_vendor_info_report(final_df, unified_lookup, missing_vendor_info_path)\n",
        "        print(f\"Missing vendor information report saved to {missing_vendor_info_path}\")\n",
        "\n",
        "        # Calculate percentage of missing vendor info\n",
        "        vendor_info_missing_percentages = calculate_missing_data_percentage(missing_vendor_info)\n",
        "        print(\"\\nPercentage of missing data in vendor information:\")\n",
        "        print(vendor_info_missing_percentages)\n",
        "\n",
        "        # Calculate missing data percentages\n",
        "        print(\"\\nCalculating missing data percentages for final DataFrame...\")\n",
        "        final_df_missing_percentages = calculate_missing_data_percentage(final_df)\n",
        "        print(\"\\nPercentage of missing data in final DataFrame:\")\n",
        "        print(final_df_missing_percentages)\n",
        "\n",
        "        # Identify missing suppliers\n",
        "        print(\"\\nIdentifying missing suppliers...\")\n",
        "        if 'Vendor ID' not in final_df.columns:\n",
        "            raise KeyError(\"'Vendor ID' column not found in final DataFrame\")\n",
        "\n",
        "        missing_suppliers = final_df[final_df['Vendor ID'].isnull()]['Supplier Name'].unique()\n",
        "        print(f\"Number of missing suppliers: {len(missing_suppliers)}\")\n",
        "\n",
        "        # Add new entries for missing suppliers\n",
        "        if len(missing_suppliers) > 0:\n",
        "            print(\"\\nAdding new entries for missing suppliers...\")\n",
        "            unified_lookup = update_vendor_lookup(unified_lookup, missing_suppliers)\n",
        "\n",
        "            # Merge again with updated lookup\n",
        "            print(\"\\nMerging again with updated lookup...\")\n",
        "            final_df = merge_vendor_data(df, unified_lookup)\n",
        "\n",
        "        print(\"\\nFinal DataFrame shape:\", final_df.shape)\n",
        "        print(\"\\nFinal DataFrame columns:\")\n",
        "        print(final_df.columns.tolist())\n",
        "        print(\"\\nFirst few rows of the final DataFrame:\")\n",
        "        print(final_df.head())\n",
        "\n",
        "        # Calculate and display total spend and estimated savings\n",
        "        if 'Total' in final_df.columns and 'Est. Savings (Low)' in final_df.columns and 'Est. Savings (High)' in final_df.columns:\n",
        "            total_spend = final_df['Total'].sum()\n",
        "            total_low_savings = final_df['Est. Savings (Low)'].sum()\n",
        "            total_high_savings = final_df['Est. Savings (High)'].sum()\n",
        "\n",
        "            print(f\"\\nTotal Spend: ${total_spend:,.2f}\")\n",
        "            print(f\"Estimated Low Savings: ${total_low_savings:,.2f} ({total_low_savings/total_spend*100:.2f}%)\")\n",
        "            print(f\"Estimated High Savings: ${total_high_savings:,.2f} ({total_high_savings/total_spend*100:.2f}%)\")\n",
        "\n",
        "        # Save updated unified lookup\n",
        "        print(\"\\nSaving updated unified lookup...\")\n",
        "        unified_lookup.to_excel(unified_lookup_path, index=False)\n",
        "        print(\"Updated unified Vendor Look Up table saved.\")\n",
        "\n",
        "        if updated_catalog is not None:\n",
        "            # Save updated catalog if it exists\n",
        "            print(\"\\nSaving updated catalog...\")\n",
        "            updated_catalog_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/updated_catalog.json\"\n",
        "            with open(updated_catalog_path, 'w') as f:\n",
        "                json.dump(updated_catalog, f, indent=4)\n",
        "            print(f\"Updated catalog saved to: {updated_catalog_path}\")\n",
        "\n",
        "        print(\"\\nMissing Data Summary:\")\n",
        "        print(\"Raw Data Missing Percentages:\")\n",
        "        print(raw_data_missing_percentages)\n",
        "        print(\"\\nVendor Info Missing Percentages:\")\n",
        "        print(vendor_info_missing_percentages)\n",
        "        print(\"\\nFinal DataFrame Missing Percentages:\")\n",
        "        print(final_df_missing_percentages)\n",
        "\n",
        "        return final_df, unified_lookup, missing_raw_data, missing_vendor_info, final_df_missing_percentages\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None, None, None, None\n",
        "\n",
        "# Run the example\n",
        "if __name__ == \"__main__\":\n",
        "    final_df, unified_lookup, missing_raw_data, missing_vendor_info, missing_percentages = run_example()\n",
        "\n",
        "    if all(v is not None for v in [final_df, unified_lookup, missing_raw_data, missing_vendor_info, missing_percentages]):\n",
        "        print(\"Run completed successfully\")\n",
        "    else:\n",
        "        print(\"Run failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEL82cgzHyi4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "outputId": "64af3f08-11d7-4caf-b095-d170b48089d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Supplier ID                         Supplier Name  \\\n",
              "16522        56600  Motion Industries, Inc.                \n",
              "29971        56600  Motion Industries, Inc.                \n",
              "29901        56600  Motion Industries, Inc.                \n",
              "17361        54650  McMaster-Carr                          \n",
              "26937        56600  Motion Industries, Inc.                \n",
              "...            ...                                   ...   \n",
              "33019        88726  Jack Daggy Flowers                     \n",
              "33023        70375  Richmond Hydraulic Service Inc.        \n",
              "33024        63615  PC Connections, Inc.                   \n",
              "33025        14991  Dayton Barns and All Steel Buildings   \n",
              "33026        90065  Transcendia Hebron (INTER)             \n",
              "\n",
              "                     Item Description                             SKU  \\\n",
              "16522  OPERATION SUPPLIES                                               \n",
              "29971  OPERATION SUPPLIES                                               \n",
              "29901  OPERATION SUPPLIES                                               \n",
              "17361  OPERATION SUPPLIES                                               \n",
              "26937  OPERATION SUPPLIES                                               \n",
              "...                               ...                             ...   \n",
              "33019  OPERATION SUPPLIES                                               \n",
              "33023  OPERATION SUPPLIES                                               \n",
              "33024  OPERATION SUPPLIES                                               \n",
              "33025  SERVICES-NON MATERIAL                                            \n",
              "33026  SNX650 75 550 CL E 3X1680 TX 3                                   \n",
              "\n",
              "                      Item Group                  Product Group  UOM  \\\n",
              "16522  SUBCONTRACTED ITEMS GROUP                  MISCELLANEOUS  EA    \n",
              "29971  SUBCONTRACTED ITEMS GROUP                  MISCELLANEOUS  EA    \n",
              "29901  SUBCONTRACTED ITEMS GROUP                  MISCELLANEOUS  EA    \n",
              "17361  SUBCONTRACTED ITEMS GROUP                  MISCELLANEOUS  EA    \n",
              "26937  SUBCONTRACTED ITEMS GROUP                  MISCELLANEOUS  EA    \n",
              "...                          ...                            ...  ...   \n",
              "33019  SUBCONTRACTED ITEMS GROUP                  MISCELLANEOUS  EA    \n",
              "33023  SUBCONTRACTED ITEMS GROUP                  MISCELLANEOUS  EA    \n",
              "33024  SUBCONTRACTED ITEMS GROUP                  MISCELLANEOUS  EA    \n",
              "33025  SUBCONTRACTED ITEMS GROUP                  MISCELLANEOUS  EA    \n",
              "33026             GROUP 1 (MISC)  Saran Films Barrier Films AFM  KG    \n",
              "\n",
              "       Quantity  Quantity in UOM       Price       Date        Total  \\\n",
              "16522      1.00             1.00  18309.4300 2024-03-18  18309.43000   \n",
              "29971      1.00             1.00   9936.0000 2024-05-08   9936.00000   \n",
              "29901      1.00             1.00   7890.4300 2024-02-02   7890.43000   \n",
              "17361      1.00             1.00   5512.3100 2024-06-27   5512.31000   \n",
              "26937      1.00             1.00   6297.8500 2023-11-16   6297.85000   \n",
              "...         ...              ...         ...        ...          ...   \n",
              "33019      1.00             1.00     93.0900 2023-09-06     93.09000   \n",
              "33023      5.67             5.67      6.5500 2024-07-09     37.13850   \n",
              "33024      1.00             1.00    329.0000 2024-05-31    329.00000   \n",
              "33025      1.00             1.00    205.0100 2024-05-21    205.01000   \n",
              "33026    699.00           675.90      7.8934 2024-07-23   5335.14906   \n",
              "\n",
              "                        Stage  Client Name  \\\n",
              "16522  Opportunity Assessment  Transcendia   \n",
              "29971  Opportunity Assessment  Transcendia   \n",
              "29901  Opportunity Assessment  Transcendia   \n",
              "17361  Opportunity Assessment  Transcendia   \n",
              "26937  Opportunity Assessment  Transcendia   \n",
              "...                       ...          ...   \n",
              "33019  Opportunity Assessment  Transcendia   \n",
              "33023  Opportunity Assessment  Transcendia   \n",
              "33024  Opportunity Assessment  Transcendia   \n",
              "33025  Opportunity Assessment  Transcendia   \n",
              "33026  Opportunity Assessment  Transcendia   \n",
              "\n",
              "                              Parent Vendor Parent Vendor ID Vendor ID  \\\n",
              "16522               Motion Industries, Inc.           MII002  MII002-1   \n",
              "29971               Motion Industries, Inc.           MII002  MII002-1   \n",
              "29901               Motion Industries, Inc.           MII002  MII002-1   \n",
              "17361                         McMaster-Carr             M003    M003-1   \n",
              "26937               Motion Industries, Inc.           MII002  MII002-1   \n",
              "...                                     ...              ...       ...   \n",
              "33019                    Jack Daggy Flowers           JDF001  JDF001-1   \n",
              "33023       Richmond Hydraulic Service Inc.           RHS001  RHS001-1   \n",
              "33024                  PC Connections, Inc.           PCI001  PCI001-1   \n",
              "33025  Dayton Barns and All Steel Buildings           DBA001  DBA001-1   \n",
              "33026                           Transcendia             T002   T002-16   \n",
              "\n",
              "            ST Category              ST Sub-Category Opportunity Term  \\\n",
              "16522               MRO           Power Transmission             Near   \n",
              "29971               MRO           Power Transmission             Near   \n",
              "29901               MRO           Power Transmission             Near   \n",
              "17361               MRO  General Industrial Supplies             Near   \n",
              "26937               MRO           Power Transmission             Near   \n",
              "...                 ...                          ...              ...   \n",
              "33019               NaN                          NaN              NaN   \n",
              "33023               NaN                          NaN              NaN   \n",
              "33024               NaN                          NaN              NaN   \n",
              "33025               NaN                          NaN              NaN   \n",
              "33026  Direct Materials                         Film              NaN   \n",
              "\n",
              "       Est. Savings % (Low)  Est. Savings % (High) Marketplace Designation  \\\n",
              "16522                  0.10                   0.15                       C   \n",
              "29971                  0.10                   0.15                       C   \n",
              "29901                  0.10                   0.15                       C   \n",
              "17361                  0.12                   0.17                       C   \n",
              "26937                  0.10                   0.15                       C   \n",
              "...                     ...                    ...                     ...   \n",
              "33019                   NaN                    NaN                       N   \n",
              "33023                   NaN                    NaN                       N   \n",
              "33024                   NaN                    NaN                     TBD   \n",
              "33025                   NaN                    NaN                     TBD   \n",
              "33026                   NaN                    NaN                     TBD   \n",
              "\n",
              "                       Notes  Est. Savings (Low)  Est. Savings (High)  \n",
              "16522                    NaN           1830.9430            2746.4145  \n",
              "29971                    NaN            993.6000            1490.4000  \n",
              "29901                    NaN            789.0430            1183.5645  \n",
              "17361                    NaN            661.4772             937.0927  \n",
              "26937                    NaN            629.7850             944.6775  \n",
              "...                      ...                 ...                  ...  \n",
              "33019                    NaN                 NaN                  NaN  \n",
              "33023                    NaN                 NaN                  NaN  \n",
              "33024                    NaN                 NaN                  NaN  \n",
              "33025                    NaN                 NaN                  NaN  \n",
              "33026  MYLAR, ClingZ, Kapton                 NaN                  NaN  \n",
              "\n",
              "[33027 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d9b0f18-9713-4522-bfc6-ff30062fdc52\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Supplier ID</th>\n",
              "      <th>Supplier Name</th>\n",
              "      <th>Item Description</th>\n",
              "      <th>SKU</th>\n",
              "      <th>Item Group</th>\n",
              "      <th>Product Group</th>\n",
              "      <th>UOM</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>Quantity in UOM</th>\n",
              "      <th>Price</th>\n",
              "      <th>Date</th>\n",
              "      <th>Total</th>\n",
              "      <th>Stage</th>\n",
              "      <th>Client Name</th>\n",
              "      <th>Parent Vendor</th>\n",
              "      <th>Parent Vendor ID</th>\n",
              "      <th>Vendor ID</th>\n",
              "      <th>ST Category</th>\n",
              "      <th>ST Sub-Category</th>\n",
              "      <th>Opportunity Term</th>\n",
              "      <th>Est. Savings % (Low)</th>\n",
              "      <th>Est. Savings % (High)</th>\n",
              "      <th>Marketplace Designation</th>\n",
              "      <th>Notes</th>\n",
              "      <th>Est. Savings (Low)</th>\n",
              "      <th>Est. Savings (High)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16522</th>\n",
              "      <td>56600</td>\n",
              "      <td>Motion Industries, Inc.</td>\n",
              "      <td>OPERATION SUPPLIES</td>\n",
              "      <td></td>\n",
              "      <td>SUBCONTRACTED ITEMS GROUP</td>\n",
              "      <td>MISCELLANEOUS</td>\n",
              "      <td>EA</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>18309.4300</td>\n",
              "      <td>2024-03-18</td>\n",
              "      <td>18309.43000</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "      <td>Motion Industries, Inc.</td>\n",
              "      <td>MII002</td>\n",
              "      <td>MII002-1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>Power Transmission</td>\n",
              "      <td>Near</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.15</td>\n",
              "      <td>C</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1830.9430</td>\n",
              "      <td>2746.4145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29971</th>\n",
              "      <td>56600</td>\n",
              "      <td>Motion Industries, Inc.</td>\n",
              "      <td>OPERATION SUPPLIES</td>\n",
              "      <td></td>\n",
              "      <td>SUBCONTRACTED ITEMS GROUP</td>\n",
              "      <td>MISCELLANEOUS</td>\n",
              "      <td>EA</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>9936.0000</td>\n",
              "      <td>2024-05-08</td>\n",
              "      <td>9936.00000</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "      <td>Motion Industries, Inc.</td>\n",
              "      <td>MII002</td>\n",
              "      <td>MII002-1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>Power Transmission</td>\n",
              "      <td>Near</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.15</td>\n",
              "      <td>C</td>\n",
              "      <td>NaN</td>\n",
              "      <td>993.6000</td>\n",
              "      <td>1490.4000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29901</th>\n",
              "      <td>56600</td>\n",
              "      <td>Motion Industries, Inc.</td>\n",
              "      <td>OPERATION SUPPLIES</td>\n",
              "      <td></td>\n",
              "      <td>SUBCONTRACTED ITEMS GROUP</td>\n",
              "      <td>MISCELLANEOUS</td>\n",
              "      <td>EA</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>7890.4300</td>\n",
              "      <td>2024-02-02</td>\n",
              "      <td>7890.43000</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "      <td>Motion Industries, Inc.</td>\n",
              "      <td>MII002</td>\n",
              "      <td>MII002-1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>Power Transmission</td>\n",
              "      <td>Near</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.15</td>\n",
              "      <td>C</td>\n",
              "      <td>NaN</td>\n",
              "      <td>789.0430</td>\n",
              "      <td>1183.5645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17361</th>\n",
              "      <td>54650</td>\n",
              "      <td>McMaster-Carr</td>\n",
              "      <td>OPERATION SUPPLIES</td>\n",
              "      <td></td>\n",
              "      <td>SUBCONTRACTED ITEMS GROUP</td>\n",
              "      <td>MISCELLANEOUS</td>\n",
              "      <td>EA</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>5512.3100</td>\n",
              "      <td>2024-06-27</td>\n",
              "      <td>5512.31000</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "      <td>McMaster-Carr</td>\n",
              "      <td>M003</td>\n",
              "      <td>M003-1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>General Industrial Supplies</td>\n",
              "      <td>Near</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.17</td>\n",
              "      <td>C</td>\n",
              "      <td>NaN</td>\n",
              "      <td>661.4772</td>\n",
              "      <td>937.0927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26937</th>\n",
              "      <td>56600</td>\n",
              "      <td>Motion Industries, Inc.</td>\n",
              "      <td>OPERATION SUPPLIES</td>\n",
              "      <td></td>\n",
              "      <td>SUBCONTRACTED ITEMS GROUP</td>\n",
              "      <td>MISCELLANEOUS</td>\n",
              "      <td>EA</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>6297.8500</td>\n",
              "      <td>2023-11-16</td>\n",
              "      <td>6297.85000</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "      <td>Motion Industries, Inc.</td>\n",
              "      <td>MII002</td>\n",
              "      <td>MII002-1</td>\n",
              "      <td>MRO</td>\n",
              "      <td>Power Transmission</td>\n",
              "      <td>Near</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.15</td>\n",
              "      <td>C</td>\n",
              "      <td>NaN</td>\n",
              "      <td>629.7850</td>\n",
              "      <td>944.6775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33019</th>\n",
              "      <td>88726</td>\n",
              "      <td>Jack Daggy Flowers</td>\n",
              "      <td>OPERATION SUPPLIES</td>\n",
              "      <td></td>\n",
              "      <td>SUBCONTRACTED ITEMS GROUP</td>\n",
              "      <td>MISCELLANEOUS</td>\n",
              "      <td>EA</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>93.0900</td>\n",
              "      <td>2023-09-06</td>\n",
              "      <td>93.09000</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "      <td>Jack Daggy Flowers</td>\n",
              "      <td>JDF001</td>\n",
              "      <td>JDF001-1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33023</th>\n",
              "      <td>70375</td>\n",
              "      <td>Richmond Hydraulic Service Inc.</td>\n",
              "      <td>OPERATION SUPPLIES</td>\n",
              "      <td></td>\n",
              "      <td>SUBCONTRACTED ITEMS GROUP</td>\n",
              "      <td>MISCELLANEOUS</td>\n",
              "      <td>EA</td>\n",
              "      <td>5.67</td>\n",
              "      <td>5.67</td>\n",
              "      <td>6.5500</td>\n",
              "      <td>2024-07-09</td>\n",
              "      <td>37.13850</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "      <td>Richmond Hydraulic Service Inc.</td>\n",
              "      <td>RHS001</td>\n",
              "      <td>RHS001-1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33024</th>\n",
              "      <td>63615</td>\n",
              "      <td>PC Connections, Inc.</td>\n",
              "      <td>OPERATION SUPPLIES</td>\n",
              "      <td></td>\n",
              "      <td>SUBCONTRACTED ITEMS GROUP</td>\n",
              "      <td>MISCELLANEOUS</td>\n",
              "      <td>EA</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>329.0000</td>\n",
              "      <td>2024-05-31</td>\n",
              "      <td>329.00000</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "      <td>PC Connections, Inc.</td>\n",
              "      <td>PCI001</td>\n",
              "      <td>PCI001-1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TBD</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33025</th>\n",
              "      <td>14991</td>\n",
              "      <td>Dayton Barns and All Steel Buildings</td>\n",
              "      <td>SERVICES-NON MATERIAL</td>\n",
              "      <td></td>\n",
              "      <td>SUBCONTRACTED ITEMS GROUP</td>\n",
              "      <td>MISCELLANEOUS</td>\n",
              "      <td>EA</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>205.0100</td>\n",
              "      <td>2024-05-21</td>\n",
              "      <td>205.01000</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "      <td>Dayton Barns and All Steel Buildings</td>\n",
              "      <td>DBA001</td>\n",
              "      <td>DBA001-1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TBD</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33026</th>\n",
              "      <td>90065</td>\n",
              "      <td>Transcendia Hebron (INTER)</td>\n",
              "      <td>SNX650 75 550 CL E 3X1680 TX 3</td>\n",
              "      <td></td>\n",
              "      <td>GROUP 1 (MISC)</td>\n",
              "      <td>Saran Films Barrier Films AFM</td>\n",
              "      <td>KG</td>\n",
              "      <td>699.00</td>\n",
              "      <td>675.90</td>\n",
              "      <td>7.8934</td>\n",
              "      <td>2024-07-23</td>\n",
              "      <td>5335.14906</td>\n",
              "      <td>Opportunity Assessment</td>\n",
              "      <td>Transcendia</td>\n",
              "      <td>Transcendia</td>\n",
              "      <td>T002</td>\n",
              "      <td>T002-16</td>\n",
              "      <td>Direct Materials</td>\n",
              "      <td>Film</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TBD</td>\n",
              "      <td>MYLAR, ClingZ, Kapton</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>33027 rows × 26 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d9b0f18-9713-4522-bfc6-ff30062fdc52')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9d9b0f18-9713-4522-bfc6-ff30062fdc52 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9d9b0f18-9713-4522-bfc6-ff30062fdc52');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8a108353-0b59-4c23-aa20-564544797b71\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8a108353-0b59-4c23-aa20-564544797b71')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8a108353-0b59-4c23-aa20-564544797b71 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "final_df.sort_values(by='Est. Savings (Low)',ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxBIXskX0Bfy"
      },
      "source": [
        "App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl4by8rx_zEu"
      },
      "outputs": [],
      "source": [
        "final_df.to_csv('final_df.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtyJbD1a4ag1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf81f3ae-db8e-4dc7-c0b5-84c16bf4461e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.39.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.24.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (16.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<6,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.39.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (16.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<6,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.0.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit plotly\n",
        "!pip install streamlit pyngrok\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhtSe8jG8ngl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "857f0b2e-81b4-4218-f882-3416ab2a8358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Set page configuration\n",
        "st.set_page_config(page_title=\"Opportunity Savings Dashboard\", layout=\"wide\")\n",
        "\n",
        "# Custom CSS to inject\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .big-font {\n",
        "        font-size:30px !important;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "    .medium-font {\n",
        "        font-size:20px !important;\n",
        "    }\n",
        "    .small-font {\n",
        "        font-size:14px !important;\n",
        "    }\n",
        "    .metric-card {\n",
        "        background-color: #f0f2f6;\n",
        "        border-radius: 10px;\n",
        "        padding: 15px;\n",
        "        text-align: center;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    df = pd.read_csv('final_df.csv')\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "        df = df.dropna(subset=['Date'])  # Remove rows with invalid dates\n",
        "\n",
        "    # Convert 'ST Category' to string and replace NaN with 'Uncategorized'\n",
        "    df['ST Category'] = df['ST Category'].fillna('Uncategorized').astype(str)\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_metrics(df):\n",
        "    total_spend = df['Total'].sum()\n",
        "    total_low_savings = df['Est. Savings (Low)'].sum()\n",
        "    total_high_savings = df['Est. Savings (High)'].sum()\n",
        "    avg_low_savings_pct = (total_low_savings / total_spend) * 100\n",
        "    avg_high_savings_pct = (total_high_savings / total_spend) * 100\n",
        "    return total_spend, total_low_savings, total_high_savings, avg_low_savings_pct, avg_high_savings_pct\n",
        "\n",
        "def display_savings_table(df, title, is_overall=True):\n",
        "    st.subheader(title)\n",
        "    st.table(df)\n",
        "\n",
        "    if is_overall:\n",
        "        # Pie chart code remains the same\n",
        "        df_pie = df[df['ST Category'] != 'Grand Total']\n",
        "        fig = px.pie(df_pie,\n",
        "                     values='Est. Savings $ (High)',\n",
        "                     names='ST Category',\n",
        "                     title=f\"{title} - High Savings Distribution\",\n",
        "                     hover_data=['Est Savings % (High)'],\n",
        "                     labels={'Est. Savings $ (High)': 'High Savings $'})\n",
        "        fig.update_traces(textposition='inside', textinfo='percent+label')\n",
        "        fig.update_layout(height=500)\n",
        "    else:\n",
        "        # Horizontal stacked bar chart for detailed high savings by sub-category\n",
        "        df_plot = df[(df['ST Category'] != 'Grand Total') &\n",
        "                     (df['ST Sub-Category / Parent Vendor'] != 'Grand Total')].copy()\n",
        "\n",
        "        # Remove rows where Sub-Category ends with ' Total'\n",
        "        df_plot = df_plot[~df_plot['ST Sub-Category / Parent Vendor'].astype(str).str.endswith(' Total')]\n",
        "\n",
        "        # Sort categories by total savings\n",
        "        category_order = df_plot.groupby('ST Category')['Est. Savings $ (High)'].sum().sort_values(ascending=True).index\n",
        "\n",
        "        fig = px.bar(df_plot,\n",
        "                     x='Est. Savings $ (High)',\n",
        "                     y='ST Category',\n",
        "                     color='ST Sub-Category / Parent Vendor',\n",
        "                     orientation='h',\n",
        "                     title=f\"{title} - High Savings by Category and Sub-Category\",\n",
        "                     labels={'Est. Savings $ (High)': 'High Savings ($)', 'ST Category': 'Category', 'ST Sub-Category / Parent Vendor': 'Sub-Category'},\n",
        "                     category_orders={'ST Category': category_order})\n",
        "\n",
        "        fig.update_layout(\n",
        "            barmode='stack',\n",
        "            height=800,\n",
        "            yaxis={'categoryorder':'array', 'categoryarray':category_order},\n",
        "            legend_title_text='Sub-Category'\n",
        "        )\n",
        "\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def display_monthly_savings(df):\n",
        "    st.subheader(\"Monthly Savings Trend\")\n",
        "\n",
        "    # Ensure 'Date' column is datetime\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    # Group by month and calculate total savings\n",
        "    monthly_savings = df.groupby(df['Date'].dt.to_period('M')).agg({\n",
        "        'Est. Savings (Low)': 'sum',\n",
        "        'Est. Savings (High)': 'sum'\n",
        "    }).reset_index()\n",
        "    monthly_savings['Date'] = monthly_savings['Date'].dt.to_timestamp()\n",
        "\n",
        "    # Create line chart\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=monthly_savings['Date'], y=monthly_savings['Est. Savings (Low)'],\n",
        "                             mode='lines+markers', name='Low Savings'))\n",
        "    fig.add_trace(go.Scatter(x=monthly_savings['Date'], y=monthly_savings['Est. Savings (High)'],\n",
        "                             mode='lines+markers', name='High Savings'))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"Monthly Savings Trend\",\n",
        "        xaxis_title=\"Month\",\n",
        "        yaxis_title=\"Savings ($)\",\n",
        "        height=500\n",
        "    )\n",
        "\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def create_savings_tables(df):\n",
        "    # Table 1: Overall Savings\n",
        "    overall_savings = df.groupby('ST Category').agg({\n",
        "        'Total': 'sum',\n",
        "        'Est. Savings % (Low)': 'mean',\n",
        "        'Est. Savings % (High)': 'mean',\n",
        "        'Est. Savings (Low)': 'sum',\n",
        "        'Est. Savings (High)': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    overall_savings = overall_savings.rename(columns={\n",
        "        'Total': 'Near Term Spend',\n",
        "        'Est. Savings % (Low)': 'Est Savings % (Low)',\n",
        "        'Est. Savings % (High)': 'Est Savings % (High)',\n",
        "        'Est. Savings (Low)': 'Est Savings $ (Low)',\n",
        "        'Est. Savings (High)': 'Est. Savings $ (High)'\n",
        "    })\n",
        "\n",
        "    # Add Grand Total row (calculated from the original dataframe)\n",
        "    grand_total = df.agg({\n",
        "        'Total': 'sum',\n",
        "        'Est. Savings % (Low)': 'mean',\n",
        "        'Est. Savings % (High)': 'mean',\n",
        "        'Est. Savings (Low)': 'sum',\n",
        "        'Est. Savings (High)': 'sum'\n",
        "    }).to_frame().T\n",
        "    grand_total['ST Category'] = 'Grand Total'\n",
        "    grand_total = grand_total.rename(columns={\n",
        "        'Total': 'Near Term Spend',\n",
        "        'Est. Savings % (Low)': 'Est Savings % (Low)',\n",
        "        'Est. Savings % (High)': 'Est Savings % (High)',\n",
        "        'Est. Savings (Low)': 'Est Savings $ (Low)',\n",
        "        'Est. Savings (High)': 'Est. Savings $ (High)'\n",
        "    })\n",
        "    overall_savings = pd.concat([overall_savings, grand_total], ignore_index=True)\n",
        "\n",
        "    # Table 2: Detailed Savings\n",
        "    detailed_savings = df.groupby(['ST Category', 'ST Sub-Category']).agg({\n",
        "        'Total': 'sum',\n",
        "        'Est. Savings % (Low)': 'mean',\n",
        "        'Est. Savings % (High)': 'mean',\n",
        "        'Est. Savings (Low)': 'sum',\n",
        "        'Est. Savings (High)': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    detailed_savings = detailed_savings.rename(columns={\n",
        "        'ST Sub-Category': 'ST Sub-Category / Parent Vendor',\n",
        "        'Total': 'Near Term Spend',\n",
        "        'Est. Savings % (Low)': 'Est Savings % (Low)',\n",
        "        'Est. Savings % (High)': 'Est Savings % (High)',\n",
        "        'Est. Savings (Low)': 'Est Savings $ (Low)',\n",
        "        'Est. Savings (High)': 'Est. Savings $ (High)'\n",
        "    })\n",
        "\n",
        "    # Add Total rows for each category\n",
        "    category_totals = df.groupby('ST Category').agg({\n",
        "        'Total': 'sum',\n",
        "        'Est. Savings % (Low)': 'mean',\n",
        "        'Est. Savings % (High)': 'mean',\n",
        "        'Est. Savings (Low)': 'sum',\n",
        "        'Est. Savings (High)': 'sum'\n",
        "    }).reset_index()\n",
        "    category_totals['ST Sub-Category / Parent Vendor'] = 'Total'\n",
        "    category_totals = category_totals.rename(columns={\n",
        "        'Total': 'Near Term Spend',\n",
        "        'Est. Savings % (Low)': 'Est Savings % (Low)',\n",
        "        'Est. Savings % (High)': 'Est Savings % (High)',\n",
        "        'Est. Savings (Low)': 'Est Savings $ (Low)',\n",
        "        'Est. Savings (High)': 'Est. Savings $ (High)'\n",
        "    })\n",
        "    detailed_savings = pd.concat([detailed_savings, category_totals], ignore_index=True)\n",
        "\n",
        "    # Add Grand Total row for detailed savings (same as overall grand total)\n",
        "    detailed_savings = pd.concat([detailed_savings, grand_total], ignore_index=True)\n",
        "\n",
        "    return overall_savings, detailed_savings\n",
        "\n",
        "def display_savings_vs_spend(df):\n",
        "    st.subheader(\"Spend vs Savings Opportunity\")\n",
        "\n",
        "    # Group by ST Category to aggregate the spend and savings data\n",
        "    category_data = df.groupby('ST Category').agg({\n",
        "        'Total': 'sum',\n",
        "        'Est. Savings (High)': 'sum',\n",
        "        'Est. Savings % (High)': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Calculate the relative size for bubbles\n",
        "    category_data['Savings Ratio'] = category_data['Est. Savings (High)'] / category_data['Total'] * 100\n",
        "\n",
        "    # Plot the bubble chart\n",
        "    fig = px.scatter(category_data,\n",
        "                     x='Total',\n",
        "                     y='Est. Savings (High)',\n",
        "                     size='Savings Ratio',\n",
        "                     color='ST Category',\n",
        "                     hover_data={'Total': ':.2f', 'Est. Savings (High)': ':.2f', 'Savings Ratio': ':.2f'},\n",
        "                     labels={'Total': 'Total Spend ($)', 'Est. Savings (High)': 'Potential Savings ($)', 'Savings Ratio': 'Savings Ratio (%)'},\n",
        "                     title=\"Total Spend vs. Potential Savings by Category\",\n",
        "                     size_max=60)\n",
        "\n",
        "    fig.update_layout(height=600, xaxis_title=\"Total Spend ($)\", yaxis_title=\"Potential High Savings ($)\")\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    st.markdown(\"<h1 style='text-align: center;'>Opportunity Savings Dashboard</h1>\", unsafe_allow_html=True)\n",
        "\n",
        "    # Load data\n",
        "    df = load_data()\n",
        "\n",
        "    # Sidebar filters\n",
        "    st.sidebar.title(\"Filters\")\n",
        "\n",
        "    # Sort categories, handling potential mixed types\n",
        "    categories = ['All'] + sorted(df['ST Category'].unique().tolist(), key=str)\n",
        "    selected_category = st.sidebar.selectbox(\"Select Category\", categories)\n",
        "\n",
        "    if 'Date' in df.columns and not df['Date'].empty:\n",
        "        min_date = df['Date'].min().date()\n",
        "        max_date = df['Date'].max().date()\n",
        "        date_range = st.sidebar.date_input(\"Select Date Range\",\n",
        "                                           value=[min_date, max_date],\n",
        "                                           min_value=min_date,\n",
        "                                           max_value=max_date)\n",
        "\n",
        "        # Apply filters\n",
        "        if selected_category != 'All':\n",
        "            df = df[df['ST Category'] == selected_category]\n",
        "        if len(date_range) == 2:\n",
        "            df = df[(df['Date'].dt.date >= date_range[0]) & (df['Date'].dt.date <= date_range[1])]\n",
        "    else:\n",
        "        st.warning(\"Date column is missing or empty. Date filtering is disabled.\")\n",
        "        if selected_category != 'All':\n",
        "            df = df[df['ST Category'] == selected_category]\n",
        "\n",
        "    # Calculate metrics\n",
        "    total_spend, total_low_savings, total_high_savings, avg_low_savings_pct, avg_high_savings_pct = calculate_metrics(df)\n",
        "\n",
        "    # Display high-level metrics\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        st.markdown('<div class=\"metric-card\">', unsafe_allow_html=True)\n",
        "        st.markdown('<p class=\"medium-font\">Total Spend</p>', unsafe_allow_html=True)\n",
        "        st.markdown(f'<p class=\"big-font\">${total_spend:,.0f}</p>', unsafe_allow_html=True)\n",
        "        st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n",
        "    with col2:\n",
        "        st.markdown('<div class=\"metric-card\">', unsafe_allow_html=True)\n",
        "        st.markdown('<p class=\"medium-font\">Est. Low Savings</p>', unsafe_allow_html=True)\n",
        "        st.markdown(f'<p class=\"big-font\">${total_low_savings:,.0f}</p>', unsafe_allow_html=True)\n",
        "        st.markdown(f'<p class=\"small-font\">({avg_low_savings_pct:.2f}%)</p>', unsafe_allow_html=True)\n",
        "        st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n",
        "    with col3:\n",
        "        st.markdown('<div class=\"metric-card\">', unsafe_allow_html=True)\n",
        "        st.markdown('<p class=\"medium-font\">Est. High Savings</p>', unsafe_allow_html=True)\n",
        "        st.markdown(f'<p class=\"big-font\">${total_high_savings:,.0f}</p>', unsafe_allow_html=True)\n",
        "        st.markdown(f'<p class=\"small-font\">({avg_high_savings_pct:.2f}%)</p>', unsafe_allow_html=True)\n",
        "        st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n",
        "    st.markdown(\"<br>\", unsafe_allow_html=True)\n",
        "\n",
        "    # Visualizations\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        st.subheader(\"Savings by Category\")\n",
        "        category_savings = df.groupby('ST Category')[['Est. Savings (Low)', 'Est. Savings (High)', 'Total']].sum().reset_index()\n",
        "        fig = px.bar(category_savings, x='ST Category', y=['Est. Savings (Low)', 'Est. Savings (High)'],\n",
        "                     title=\"Estimated Savings by Category\", barmode='group')\n",
        "        fig.update_layout(height=400)\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    with col2:\n",
        "        st.subheader(\"Top 10 Suppliers by Potential Savings\")\n",
        "        top_suppliers = df.groupby('Supplier Name')['Est. Savings (High)'].sum().nlargest(10).reset_index()\n",
        "        fig = px.bar(top_suppliers, x='Supplier Name', y='Est. Savings (High)',\n",
        "                     title=\"Top 10 Suppliers by Potential High Savings\")\n",
        "        fig.update_layout(height=400)\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    # Display monthly savings trend\n",
        "    display_monthly_savings(df)\n",
        "\n",
        "    display_savings_vs_spend(df)\n",
        "    # Heatmap\n",
        "    if 'Date' in df.columns and not df['Date'].empty:\n",
        "        st.subheader(\"Savings Heatmap by Category and Month\")\n",
        "        df['Month'] = df['Date'].dt.to_period('M').astype(str)\n",
        "        heatmap_data = df.pivot_table(values='Est. Savings (High)', index='ST Category', columns='Month', aggfunc='sum')\n",
        "        fig = px.imshow(heatmap_data, aspect=\"auto\", color_continuous_scale=\"YlOrRd\")\n",
        "        fig.update_layout(height=500)\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "    else:\n",
        "        st.warning(\"Date information is not available. Heatmap cannot be displayed.\")\n",
        "\n",
        "   # Treemap\n",
        "    st.subheader(\"Spend Distribution Treemap\")\n",
        "    # Filter out zero or negative values for the treemap\n",
        "    df_treemap = df[df['Total'] > 0]\n",
        "    if not df_treemap.empty:\n",
        "        fig = px.treemap(df_treemap, path=['ST Category', 'Supplier Name'], values='Total',\n",
        "                         color='Est. Savings (High)', color_continuous_scale='RdYlGn_r',\n",
        "                         title=\"Spend Distribution and Savings Potential\")\n",
        "        fig.update_layout(height=600)\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "    else:\n",
        "        st.warning(\"No positive 'Total' values available for the treemap visualization.\")\n",
        "\n",
        "\n",
        "    st.markdown(\"<br>\", unsafe_allow_html=True)\n",
        "    st.markdown(\"## Savings Estimates Tables\", unsafe_allow_html=True)\n",
        "\n",
        "    # Create and display savings tables\n",
        "    overall_savings, detailed_savings = create_savings_tables(df)\n",
        "    display_savings_table(overall_savings, \"Transcendia - Wave 1 Savings Estimates\", is_overall=True)\n",
        "    display_savings_table(detailed_savings, \"Detailed Savings Estimates\", is_overall=False)\n",
        "\n",
        "    st.markdown(\"<br>\", unsafe_allow_html=True)\n",
        "    st.markdown(\"## Raw Data\", unsafe_allow_html=True)\n",
        "\n",
        "    # Display raw data\n",
        "    st.subheader(\"Raw Data\")\n",
        "    # Display only the first 1000 rows to prevent performance issues\n",
        "    st.dataframe(df.head(1000))\n",
        "    if len(df) > 1000:\n",
        "        st.info(f\"Showing first 1000 rows out of {len(df)} total rows.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQw69xB09QTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d73a38-cb44-48b2-a685-f7b85eef9e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://03b8-34-125-62-150.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.62.150:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2nCrHcKsJCDT2SiVXRa56Z2LMkz_3uVedRt8Q5oT5BPb9R9Lx\")\n",
        "\n",
        "# Set up a tunnel to the full address with http protocol\n",
        "public_url = ngrok.connect(addr=\"http://localhost:8501\")  # Full address\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run app.py &\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UF0k3vmZ6Y3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing data\n"
      ],
      "metadata": {
        "id": "I64XM5TS6W8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_with_missing_data(existing_df, missing_data_file):\n",
        "    \"\"\"\n",
        "    Update the existing DataFrame with data from the missing data file.\n",
        "\n",
        "    Args:\n",
        "    existing_df (pd.DataFrame): The current DataFrame with potentially missing data.\n",
        "    missing_data_file (str): Path to the Excel file containing the missing data.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Updated DataFrame with filled-in missing data.\n",
        "    \"\"\"\n",
        "    # Load the missing data file\n",
        "    missing_df = pd.read_excel(missing_data_file)\n",
        "\n",
        "    # Ensure the 'Supplier Name' and 'SKU' columns exist in both DataFrames\n",
        "    key_columns = ['Supplier Name', 'SKU']\n",
        "    if not all(col in existing_df.columns for col in key_columns) or \\\n",
        "       not all(col in missing_df.columns for col in key_columns):\n",
        "        raise ValueError(\"Both DataFrames must contain 'Supplier Name' and 'SKU' columns\")\n",
        "\n",
        "    # Merge the DataFrames\n",
        "    updated_df = existing_df.merge(missing_df, on=key_columns, how='left', suffixes=('', '_new'))\n",
        "\n",
        "    # Update the existing DataFrame with new data where applicable\n",
        "    for col in missing_df.columns:\n",
        "        if col not in key_columns:\n",
        "            mask = updated_df[f'{col}_new'].notna()\n",
        "            updated_df.loc[mask, col] = updated_df.loc[mask, f'{col}_new']\n",
        "\n",
        "    # Drop the temporary columns created by the merge\n",
        "    columns_to_drop = [col for col in updated_df.columns if col.endswith('_new')]\n",
        "    updated_df = updated_df.drop(columns=columns_to_drop)\n",
        "\n",
        "    return updated_df"
      ],
      "metadata": {
        "id": "lm2bEn4o6aiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run again"
      ],
      "metadata": {
        "id": "MZjun8lW8DSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_example():\n",
        "    unified_lookup_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/unified_vendor_lookup.xlsx\"\n",
        "    client_file_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/Transcendia - Opportunity Assessment.xlsx\"\n",
        "    missing_raw_data_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/missing_raw_data.xlsx\"\n",
        "    missing_vendor_info_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/missing_vendor_info.xlsx\"\n",
        "    filled_missing_data_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/filled_missing_data.xlsx\"\n",
        "\n",
        "    print(\"Starting run_example function...\")\n",
        "\n",
        "    try:\n",
        "        # Load unified Vendor Look Up table\n",
        "        print(\"Loading unified vendor lookup table...\")\n",
        "        unified_lookup = load_unified_vendor_lookup(unified_lookup_path)\n",
        "\n",
        "        if unified_lookup is None:\n",
        "            raise ValueError(\"load_unified_vendor_lookup returned None\")\n",
        "\n",
        "        print(\"\\nUnified lookup table columns:\")\n",
        "        print(unified_lookup.columns.tolist())\n",
        "\n",
        "        print(\"\\nFirst few rows of unified lookup table:\")\n",
        "        print(unified_lookup.head())\n",
        "\n",
        "        # Process client input\n",
        "        print(\"\\nProcessing client input...\")\n",
        "        result = process_client_input(client_file_path)\n",
        "\n",
        "        if result is None:\n",
        "            raise ValueError(\"process_client_input returned None\")\n",
        "\n",
        "        if isinstance(result, tuple) and len(result) == 2:\n",
        "            df, updated_catalog = result\n",
        "        elif isinstance(result, pd.DataFrame):\n",
        "            df = result\n",
        "            updated_catalog = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected return type from process_client_input: {type(result)}\")\n",
        "\n",
        "        if df is None or not isinstance(df, pd.DataFrame):\n",
        "            raise ValueError(f\"Invalid DataFrame returned from process_client_input: {type(df)}\")\n",
        "\n",
        "        print(\"\\nClient input DataFrame columns:\")\n",
        "        print(df.columns.tolist())\n",
        "\n",
        "        print(\"\\nFirst few rows of client input DataFrame:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Create missing raw data report\n",
        "        print(\"\\nCreating missing raw data report...\")\n",
        "        missing_raw_data = create_missing_raw_data_report(df, missing_raw_data_path)\n",
        "        print(f\"Missing raw data report saved to {missing_raw_data_path}\")\n",
        "\n",
        "        # Calculate percentage of missing raw data\n",
        "        raw_data_missing_percentages = calculate_missing_data_percentage(df)\n",
        "        print(\"\\nPercentage of missing data in raw data:\")\n",
        "        print(raw_data_missing_percentages)\n",
        "\n",
        "        # Get unique suppliers from raw data\n",
        "        new_suppliers = df['Supplier Name'].unique().tolist()\n",
        "\n",
        "        # Update unified vendor lookup with new suppliers\n",
        "        print(\"\\nUpdating unified vendor lookup with new suppliers...\")\n",
        "        unified_lookup = update_vendor_lookup(unified_lookup, new_suppliers)\n",
        "\n",
        "        # Merge vendor data\n",
        "        print(\"\\nMerging vendor data...\")\n",
        "        final_df = merge_vendor_data(df, unified_lookup)\n",
        "\n",
        "        if final_df is None:\n",
        "            raise ValueError(\"merge_vendor_data returned None\")\n",
        "\n",
        "        # Calculate estimated savings\n",
        "        if 'Total' in final_df.columns and 'Est. Savings % (Low)' in final_df.columns and 'Est. Savings % (High)' in final_df.columns:\n",
        "            final_df['Est. Savings (Low)'] = final_df['Total'] * (final_df['Est. Savings % (Low)'])\n",
        "            final_df['Est. Savings (High)'] = final_df['Total'] * (final_df['Est. Savings % (High)'])\n",
        "            print(\"Estimated savings calculated.\")\n",
        "        else:\n",
        "            print(\"Warning: Unable to calculate estimated savings. Required columns missing.\")\n",
        "\n",
        "        print(\"\\nFinal DataFrame columns after merging:\")\n",
        "        print(final_df.columns.tolist())\n",
        "\n",
        "        print(\"\\nFirst few rows of final DataFrame after merging:\")\n",
        "        print(final_df.head())\n",
        "\n",
        "        # New section: Update with filled missing data\n",
        "        print(\"\\nChecking for filled missing data file...\")\n",
        "        if os.path.exists(filled_missing_data_path):\n",
        "            print(f\"Filled missing data file found: {filled_missing_data_path}\")\n",
        "\n",
        "            # Store the columns with null values before updating\n",
        "            null_columns = final_df.columns[final_df.isnull().any()].tolist()\n",
        "\n",
        "            print(\"Updating final DataFrame with filled missing data...\")\n",
        "            final_df_before = final_df.copy()\n",
        "            final_df = update_with_missing_data(final_df, filled_missing_data_path)\n",
        "            print(\"Final DataFrame updated with filled missing data.\")\n",
        "\n",
        "            # Verify the update process\n",
        "            print(\"\\nVerifying the update process...\")\n",
        "            try:\n",
        "                # Ensure both DataFrames have the same index and columns\n",
        "                common_index = final_df.index.intersection(final_df_before.index)\n",
        "                common_columns = list(set(null_columns) & set(final_df.columns) & set(final_df_before.columns))\n",
        "\n",
        "                if not common_columns:\n",
        "                    print(\"No common columns found for comparison.\")\n",
        "                else:\n",
        "                    final_df_subset = final_df.loc[common_index, common_columns]\n",
        "                    final_df_before_subset = final_df_before.loc[common_index, common_columns]\n",
        "\n",
        "                    # Compare the subsets\n",
        "                    updated_mask = (final_df_subset != final_df_before_subset) & ~final_df_subset.isnull()\n",
        "                    updated_rows = final_df_subset[updated_mask.any(axis=1)]\n",
        "\n",
        "                    if not updated_rows.empty:\n",
        "                        print(f\"\\nFirst 5 rows of previously null data that is now filled:\")\n",
        "                        print(updated_rows.head())\n",
        "                    else:\n",
        "                        print(\"No previously null values were updated.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during verification: {str(e)}\")\n",
        "                print(\"Proceeding with the updated data without verification.\")\n",
        "\n",
        "            # Recalculate metrics after updating\n",
        "            print(\"\\nRecalculating metrics after updating with filled missing data...\")\n",
        "            if 'Total' in final_df.columns and 'Est. Savings % (Low)' in final_df.columns and 'Est. Savings % (High)' in final_df.columns:\n",
        "                final_df['Est. Savings (Low)'] = final_df['Total'] * (final_df['Est. Savings % (Low)'])\n",
        "                final_df['Est. Savings (High)'] = final_df['Total'] * (final_df['Est. Savings % (High)'])\n",
        "                print(\"Estimated savings recalculated.\")\n",
        "            else:\n",
        "                print(\"Warning: Unable to recalculate estimated savings. Required columns missing.\")\n",
        "        else:\n",
        "            print(\"No filled missing data file found. Proceeding with original data.\")\n",
        "\n",
        "        # Create missing vendor info report\n",
        "        print(\"\\nCreating missing vendor info report...\")\n",
        "        missing_vendor_info = create_missing_vendor_info_report(final_df, unified_lookup, missing_vendor_info_path)\n",
        "        print(f\"Missing vendor information report saved to {missing_vendor_info_path}\")\n",
        "\n",
        "        # Calculate percentage of missing vendor info\n",
        "        vendor_info_missing_percentages = calculate_missing_data_percentage(missing_vendor_info)\n",
        "        print(\"\\nPercentage of missing data in vendor information:\")\n",
        "        print(vendor_info_missing_percentages)\n",
        "\n",
        "        # Calculate missing data percentages\n",
        "        print(\"\\nCalculating missing data percentages for final DataFrame...\")\n",
        "        final_df_missing_percentages = calculate_missing_data_percentage(final_df)\n",
        "        print(\"\\nPercentage of missing data in final DataFrame:\")\n",
        "        print(final_df_missing_percentages)\n",
        "\n",
        "        # Calculate and display total spend and estimated savings\n",
        "        if 'Total' in final_df.columns and 'Est. Savings (Low)' in final_df.columns and 'Est. Savings (High)' in final_df.columns:\n",
        "            total_spend = final_df['Total'].sum()\n",
        "            total_low_savings = final_df['Est. Savings (Low)'].sum()\n",
        "            total_high_savings = final_df['Est. Savings (High)'].sum()\n",
        "\n",
        "            print(f\"\\nTotal Spend: ${total_spend:,.2f}\")\n",
        "            print(f\"Estimated Low Savings: ${total_low_savings:,.2f} ({total_low_savings/total_spend*100:.2f}%)\")\n",
        "            print(f\"Estimated High Savings: ${total_high_savings:,.2f} ({total_high_savings/total_spend*100:.2f}%)\")\n",
        "\n",
        "        # Save updated unified lookup\n",
        "        print(\"\\nSaving updated unified lookup...\")\n",
        "        unified_lookup.to_excel(unified_lookup_path, index=False)\n",
        "        print(\"Updated unified Vendor Look Up table saved.\")\n",
        "\n",
        "        if updated_catalog is not None:\n",
        "            # Save updated catalog if it exists\n",
        "            print(\"\\nSaving updated catalog...\")\n",
        "            updated_catalog_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/updated_catalog.json\"\n",
        "            with open(updated_catalog_path, 'w') as f:\n",
        "                json.dump(updated_catalog, f, indent=4)\n",
        "            print(f\"Updated catalog saved to: {updated_catalog_path}\")\n",
        "\n",
        "        print(\"\\nMissing Data Summary:\")\n",
        "        print(\"Raw Data Missing Percentages:\")\n",
        "        print(raw_data_missing_percentages)\n",
        "        print(\"\\nVendor Info Missing Percentages:\")\n",
        "        print(vendor_info_missing_percentages)\n",
        "        print(\"\\nFinal DataFrame Missing Percentages:\")\n",
        "        print(final_df_missing_percentages)\n",
        "\n",
        "        return final_df, unified_lookup, missing_raw_data, missing_vendor_info, final_df_missing_percentages\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None, None, None, None\n",
        "\n",
        "# Run the example\n",
        "if __name__ == \"__main__\":\n",
        "    final_df, unified_lookup, missing_raw_data, missing_vendor_info, missing_percentages = run_example()\n",
        "\n",
        "    if all(v is not None for v in [final_df, unified_lookup, missing_raw_data, missing_vendor_info, missing_percentages]):\n",
        "        print(\"Run completed successfully\")\n",
        "    else:\n",
        "        print(\"Run failed\")"
      ],
      "metadata": {
        "id": "jaE_lY0n7Q7U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e50386c2-54bd-4d28-a10f-a391b8f13bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting run_example function...\n",
            "Loading unified vendor lookup table...\n",
            "\n",
            "Unified lookup table columns:\n",
            "['Supplier Name', 'Parent Vendor', 'Parent Vendor ID', 'Vendor ID', 'ST Category', 'ST Sub-Category', 'Opportunity Term', 'Est. Savings % (Low)', 'Est. Savings % (High)', 'Marketplace Designation', 'Notes']\n",
            "\n",
            "First few rows of unified lookup table:\n",
            "                          Supplier Name                   Parent Vendor  \\\n",
            "0  419 Neon LLC                                            419 Neon LLC   \n",
            "1  48Forty Solutions, LLC                        48Forty Solutions, LLC   \n",
            "2  4C2 Electrical Associates, LLC        4C2 Electrical Associates, LLC   \n",
            "3  A & A Machine Works, Inc.                  A & A Machine Works, Inc.   \n",
            "4  A & D Flexographic Repair LLC          A & D Flexographic Repair LLC   \n",
            "\n",
            "  Parent Vendor ID Vendor ID                      ST Category  \\\n",
            "0           4NL001  4NL001-1                              NaN   \n",
            "1           4SL001  4SL001-1               Packaging Supplies   \n",
            "2           4EA001  4EA001-1                              NaN   \n",
            "3           A&A001  A&A001-1                              NaN   \n",
            "4           A&D001  A&D001-1  Facility & Maintenance Services   \n",
            "\n",
            "       ST Sub-Category Opportunity Term  Est. Savings % (Low)  \\\n",
            "0                  NaN              NaN                   NaN   \n",
            "1              Pallets              NaN                   NaN   \n",
            "2                  NaN              NaN                   NaN   \n",
            "3                  NaN              NaN                   NaN   \n",
            "4  General Maintenance              NaN                   NaN   \n",
            "\n",
            "   Est. Savings % (High) Marketplace Designation Notes  \n",
            "0                    NaN                     NaN   NaN  \n",
            "1                    NaN                     TBD   NaN  \n",
            "2                    NaN                       N   NaN  \n",
            "3                    NaN                       N   NaN  \n",
            "4                    NaN                     TBD   NaN  \n",
            "\n",
            "Processing client input...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-bf2580b4544f>\u001b[0m in \u001b[0;36m<cell line: 192>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;31m# Run the example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mfinal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munified_lookup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_raw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_vendor_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_percentages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munified_lookup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_raw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_vendor_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_percentages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-bf2580b4544f>\u001b[0m in \u001b[0;36mrun_example\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Process client input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nProcessing client input...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_client_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-d228fc67ff64>\u001b[0m in \u001b[0;36mprocess_client_input\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Load the workbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mwb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Find the sheet with 'Data' in its name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openpyxl/reader/excel.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(filename, read_only, keep_vba, data_only, keep_links, rich_text)\u001b[0m\n\u001b[1;32m    346\u001b[0m     reader = ExcelReader(filename, read_only, keep_vba,\n\u001b[1;32m    347\u001b[0m                          data_only, keep_links, rich_text)\n\u001b[0;32m--> 348\u001b[0;31m     \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openpyxl/reader/excel.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mapply_stylesheet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"read worksheets\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_worksheets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"assign names\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openpyxl/reader/excel.py\u001b[0m in \u001b[0;36mread_worksheets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReadOnlyWorksheet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msheet_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sheets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/_read_only.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent_workbook, title, worksheet_path, shared_strings)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worksheet_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworksheet_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shared_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefined_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefinedNameDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/_read_only.py\u001b[0m in \u001b[0;36m_get_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWorkSheetParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mdimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdimensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/_reader.py\u001b[0m in \u001b[0;36mparse_dimensions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_event\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDIMENSION_TAG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSheetDimension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36miterator\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m   1253\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpullparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m                 \u001b[0;31m# load event buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m         \u001b[0;31m# Read up to n compressed bytes with at most one read() system call,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;31m# decrypt and decompress them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All Opp Assesment Master File"
      ],
      "metadata": {
        "id": "cf-fsuwmCmmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_example(client_file_path, unified_lookup_path, master_table_path):\n",
        "    missing_raw_data_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/missing_raw_data.xlsx\"\n",
        "    missing_vendor_info_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/missing_vendor_info.xlsx\"\n",
        "    filled_missing_data_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/filled_missing_data.xlsx\"\n",
        "\n",
        "    print(f\"Starting run_example function for file: {client_file_path}\")\n",
        "\n",
        "    try:\n",
        "        # Load unified Vendor Look Up table\n",
        "        print(\"Loading unified vendor lookup table...\")\n",
        "        unified_lookup = load_unified_vendor_lookup(unified_lookup_path)\n",
        "\n",
        "        if unified_lookup is None:\n",
        "            raise ValueError(\"load_unified_vendor_lookup returned None\")\n",
        "\n",
        "        print(\"\\nUnified lookup table columns:\")\n",
        "        print(unified_lookup.columns.tolist())\n",
        "\n",
        "        print(\"\\nFirst few rows of unified lookup table:\")\n",
        "        print(unified_lookup.head())\n",
        "\n",
        "        # Process client input\n",
        "        print(\"\\nProcessing client input...\")\n",
        "        result = process_client_input(client_file_path)\n",
        "\n",
        "        if isinstance(result, tuple) and len(result) == 2:\n",
        "            df, updated_catalog = result\n",
        "        elif isinstance(result, pd.DataFrame):\n",
        "            df = result\n",
        "            updated_catalog = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected return type from process_client_input: {type(result)}\")\n",
        "\n",
        "        if df is None or not isinstance(df, pd.DataFrame):\n",
        "            raise ValueError(f\"Invalid DataFrame returned from process_client_input: {type(df)}\")\n",
        "\n",
        "        print(\"\\nClient input DataFrame columns:\")\n",
        "        print(df.columns.tolist())\n",
        "\n",
        "        print(\"\\nFirst few rows of client input DataFrame:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Create missing raw data report\n",
        "        print(\"\\nCreating missing raw data report...\")\n",
        "        missing_raw_data = create_missing_raw_data_report(df, missing_raw_data_path)\n",
        "        print(f\"Missing raw data report saved to {missing_raw_data_path}\")\n",
        "\n",
        "        # Calculate percentage of missing raw data\n",
        "        raw_data_missing_percentages = calculate_missing_data_percentage(df)\n",
        "        print(\"\\nPercentage of missing data in raw data:\")\n",
        "        print(raw_data_missing_percentages)\n",
        "\n",
        "        # Get unique suppliers from raw data\n",
        "        new_suppliers = df['Supplier Name'].unique().tolist()\n",
        "\n",
        "        # Update unified vendor lookup with new suppliers\n",
        "        print(\"\\nUpdating unified vendor lookup with new suppliers...\")\n",
        "        unified_lookup = update_vendor_lookup(unified_lookup, new_suppliers)\n",
        "\n",
        "        # Merge vendor data\n",
        "        print(\"\\nMerging vendor data...\")\n",
        "        final_df = merge_vendor_data(df, unified_lookup)\n",
        "\n",
        "        if final_df is None:\n",
        "            raise ValueError(\"merge_vendor_data returned None\")\n",
        "\n",
        "        # Calculate estimated savings\n",
        "        if 'Total' in final_df.columns and 'Est. Savings % (Low)' in final_df.columns and 'Est. Savings % (High)' in final_df.columns:\n",
        "            final_df['Est. Savings (Low)'] = final_df['Total'] * (final_df['Est. Savings % (Low)'])\n",
        "            final_df['Est. Savings (High)'] = final_df['Total'] * (final_df['Est. Savings % (High)'])\n",
        "            print(\"Estimated savings calculated.\")\n",
        "        else:\n",
        "            print(\"Warning: Unable to calculate estimated savings. Required columns missing.\")\n",
        "\n",
        "        print(\"\\nFinal DataFrame columns after merging:\")\n",
        "        print(final_df.columns.tolist())\n",
        "\n",
        "        print(\"\\nFirst few rows of final DataFrame after merging:\")\n",
        "        print(final_df.head())\n",
        "\n",
        "        # Update with filled missing data\n",
        "        print(\"\\nChecking for filled missing data file...\")\n",
        "        if os.path.exists(filled_missing_data_path):\n",
        "            print(f\"Filled missing data file found: {filled_missing_data_path}\")\n",
        "\n",
        "            # Store the columns with null values before updating\n",
        "            null_columns = final_df.columns[final_df.isnull().any()].tolist()\n",
        "\n",
        "            print(\"Updating final DataFrame with filled missing data...\")\n",
        "            final_df_before = final_df.copy()\n",
        "            final_df = update_with_missing_data(final_df, filled_missing_data_path)\n",
        "            print(\"Final DataFrame updated with filled missing data.\")\n",
        "\n",
        "            # Verify the update process\n",
        "            print(\"\\nVerifying the update process...\")\n",
        "            try:\n",
        "                # Ensure both DataFrames have the same index and columns\n",
        "                common_index = final_df.index.intersection(final_df_before.index)\n",
        "                common_columns = list(set(null_columns) & set(final_df.columns) & set(final_df_before.columns))\n",
        "\n",
        "                if not common_columns:\n",
        "                    print(\"No common columns found for comparison.\")\n",
        "                else:\n",
        "                    final_df_subset = final_df.loc[common_index, common_columns]\n",
        "                    final_df_before_subset = final_df_before.loc[common_index, common_columns]\n",
        "\n",
        "                    # Compare the subsets\n",
        "                    updated_mask = (final_df_subset != final_df_before_subset) & ~final_df_subset.isnull()\n",
        "                    updated_rows = final_df_subset[updated_mask.any(axis=1)]\n",
        "\n",
        "                    if not updated_rows.empty:\n",
        "                        print(f\"\\nFirst 5 rows of previously null data that is now filled:\")\n",
        "                        print(updated_rows.head())\n",
        "                    else:\n",
        "                        print(\"No previously null values were updated.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during verification: {str(e)}\")\n",
        "                print(\"Proceeding with the updated data without verification.\")\n",
        "\n",
        "            # Recalculate metrics after updating\n",
        "            print(\"\\nRecalculating metrics after updating with filled missing data...\")\n",
        "            if 'Total' in final_df.columns and 'Est. Savings % (Low)' in final_df.columns and 'Est. Savings % (High)' in final_df.columns:\n",
        "                final_df['Est. Savings (Low)'] = final_df['Total'] * (final_df['Est. Savings % (Low)'])\n",
        "                final_df['Est. Savings (High)'] = final_df['Total'] * (final_df['Est. Savings % (High)'])\n",
        "                print(\"Estimated savings recalculated.\")\n",
        "            else:\n",
        "                print(\"Warning: Unable to recalculate estimated savings. Required columns missing.\")\n",
        "        else:\n",
        "            print(\"No filled missing data file found. Proceeding with original data.\")\n",
        "\n",
        "        # Create missing vendor info report\n",
        "        print(\"\\nCreating missing vendor info report...\")\n",
        "        missing_vendor_info = create_missing_vendor_info_report(final_df, unified_lookup, missing_vendor_info_path)\n",
        "        print(f\"Missing vendor information report saved to {missing_vendor_info_path}\")\n",
        "\n",
        "        # Calculate percentage of missing vendor info\n",
        "        vendor_info_missing_percentages = calculate_missing_data_percentage(missing_vendor_info)\n",
        "        print(\"\\nPercentage of missing data in vendor information:\")\n",
        "        print(vendor_info_missing_percentages)\n",
        "\n",
        "        # Calculate missing data percentages\n",
        "        print(\"\\nCalculating missing data percentages for final DataFrame...\")\n",
        "        final_df_missing_percentages = calculate_missing_data_percentage(final_df)\n",
        "        print(\"\\nPercentage of missing data in final DataFrame:\")\n",
        "        print(final_df_missing_percentages)\n",
        "\n",
        "        # Calculate and display total spend and estimated savings\n",
        "        if 'Total' in final_df.columns and 'Est. Savings (Low)' in final_df.columns and 'Est. Savings (High)' in final_df.columns:\n",
        "            total_spend = final_df['Total'].sum()\n",
        "            total_low_savings = final_df['Est. Savings (Low)'].sum()\n",
        "            total_high_savings = final_df['Est. Savings (High)'].sum()\n",
        "\n",
        "            print(f\"\\nTotal Spend: ${total_spend:,.2f}\")\n",
        "            print(f\"Estimated Low Savings: ${total_low_savings:,.2f} ({total_low_savings/total_spend*100:.2f}%)\")\n",
        "            print(f\"Estimated High Savings: ${total_high_savings:,.2f} ({total_high_savings/total_spend*100:.2f}%)\")\n",
        "\n",
        "        # Extract client name from the file name\n",
        "        client_name = os.path.basename(client_file_path).split(' - ')[0]\n",
        "\n",
        "        # Update master table\n",
        "        print(\"\\nUpdating master table...\")\n",
        "        master_table = update_master_table(final_df, client_name, master_table_path)\n",
        "        print(\"Master table updated successfully.\")\n",
        "\n",
        "        # Save updated unified lookup\n",
        "        print(\"\\nSaving updated unified lookup...\")\n",
        "        unified_lookup.to_excel(unified_lookup_path, index=False)\n",
        "        print(\"Updated unified Vendor Look Up table saved.\")\n",
        "\n",
        "        if updated_catalog is not None:\n",
        "            # Save updated catalog if it exists\n",
        "            print(\"\\nSaving updated catalog...\")\n",
        "            updated_catalog_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/updated_catalog.json\"\n",
        "            with open(updated_catalog_path, 'w') as f:\n",
        "                json.dump(updated_catalog, f, indent=4)\n",
        "            print(f\"Updated catalog saved to: {updated_catalog_path}\")\n",
        "\n",
        "        print(\"\\nMissing Data Summary:\")\n",
        "        print(\"Raw Data Missing Percentages:\")\n",
        "        print(raw_data_missing_percentages)\n",
        "        print(\"\\nVendor Info Missing Percentages:\")\n",
        "        print(vendor_info_missing_percentages)\n",
        "        print(\"\\nFinal DataFrame Missing Percentages:\")\n",
        "        print(final_df_missing_percentages)\n",
        "\n",
        "        return final_df, unified_lookup, missing_raw_data, missing_vendor_info, final_df_missing_percentages, master_table\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "# Function to process multiple client files\n",
        "def process_multiple_clients(client_files, unified_lookup_path, master_table_path):\n",
        "    for client_file in client_files:\n",
        "        print(f\"\\nProcessing file: {client_file}\")\n",
        "        final_df, unified_lookup, _, _, _, master_table = run_example(client_file, unified_lookup_path, master_table_path)\n",
        "        if final_df is None:\n",
        "            print(f\"Failed to process {client_file}\")\n",
        "        else:\n",
        "            print(f\"Successfully processed {client_file}\")\n",
        "\n",
        "    print(f\"\\nAll client files processed. Master table saved to {master_table_path}\")\n",
        "    return master_table\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    client_folder = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/Client Files\"\n",
        "    unified_lookup_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/unified_vendor_lookup.xlsx\"\n",
        "    master_table_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/opportunity_assessments_master_table.xlsx\"\n",
        "\n",
        "    client_files = [os.path.join(client_folder, f) for f in os.listdir(client_folder) if f.endswith('.xlsx')]\n",
        "    master_table = process_multiple_clients(client_files, unified_lookup_path, master_table_path)\n",
        "\n",
        "    print(f\"Master table shape: {master_table.shape}\")"
      ],
      "metadata": {
        "id": "XKQQ8-yw-x7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Ingredients"
      ],
      "metadata": {
        "id": "UKWh98Wg8475"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Catalog"
      ],
      "metadata": {
        "id": "7mE7sJ_a6kvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_or_create_catalog_ingredients(catalog_file='column_name_catalog.json'):\n",
        "    if os.path.exists(catalog_file):\n",
        "        with open(catalog_file, 'r') as f:\n",
        "            return json.load(f)\n",
        "    else:\n",
        "        return {\n",
        "            'Item ID': ['Item ID', 'Item Number', 'Item code','Itemcode','SKU'],\n",
        "            'Supplier Name': ['Supplier Name', 'Vendor Name', 'Supplier', 'Vendor','Vendor','Vendor Name (PO)'],\n",
        "            'Item Description': ['Item Description', 'Product Description', 'Description', 'Item Name', 'Product Name'],\n",
        "            'UOM': ['UOM', 'Unit of Measure', 'Purchase UOM', 'Order UOM',' Order UOM '],\n",
        "            'Quantity in UOM': ['Received Quantity','Qty Received In Purch UOM', 'Order Qty UOM', 'Quantity in Purchase UOM','Quatity UOM','Quantity'],\n",
        "            'Total': ['Actual Inventory Cost'],\n",
        "            'Date': ['Received Date','Receive Date', 'Purchase Date', 'Order Date', 'Transaction Date', 'Invoice Date']\n",
        "        }\n",
        "def save_catalog(catalog, catalog_file='column_name_catalog.json'):\n",
        "    \"\"\"\n",
        "    Save the catalog to a JSON file.\n",
        "\n",
        "    Args:\n",
        "    catalog (dict): The catalog dictionary to be saved.\n",
        "    catalog_file (str): The filename where the catalog will be saved.\n",
        "    \"\"\"\n",
        "    with open(catalog_file, 'w') as f:\n",
        "        json.dump(catalog, f, indent=4)\n",
        "\n",
        "def find_matching_column(df_columns, catalog):\n",
        "    column_mapping = {}\n",
        "    unmatched_columns = []\n",
        "    df_columns = pd.Index(df_columns)\n",
        "\n",
        "    for standard_name, variations in catalog.items():\n",
        "        match_found = False\n",
        "        for possible_name in variations:\n",
        "            exact_match = df_columns[df_columns.str.lower() == possible_name.lower()]\n",
        "            if len(exact_match) > 0:\n",
        "                column_mapping[standard_name] = exact_match[0]\n",
        "                match_found = True\n",
        "                break\n",
        "\n",
        "            partial_matches = [col for col in df_columns if possible_name.lower() in col.lower()]\n",
        "            if partial_matches:\n",
        "                column_mapping[standard_name] = partial_matches[0]\n",
        "                match_found = True\n",
        "                break\n",
        "\n",
        "        if not match_found:\n",
        "            unmatched_columns.append(standard_name)\n",
        "\n",
        "    return column_mapping, unmatched_columns\n",
        "def update_catalog(catalog, new_mappings):\n",
        "    \"\"\"\n",
        "    Update the existing catalog with new column name mappings.\n",
        "\n",
        "    Args:\n",
        "    catalog (dict): The existing catalog dictionary.\n",
        "    new_mappings (dict): A dictionary of new mappings to add or update.\n",
        "\n",
        "    Returns:\n",
        "    dict: The updated catalog dictionary.\n",
        "    \"\"\"\n",
        "    for standard_name, variations in new_mappings.items():\n",
        "        if standard_name in catalog:\n",
        "            catalog[standard_name].extend([v for v in variations if v not in catalog[standard_name]])\n",
        "        else:\n",
        "            catalog[standard_name] = variations\n",
        "    return catalog\n",
        "\n",
        "def interactive_column_mapping_ingredients(df_columns, catalog):\n",
        "    column_mapping, unmatched_columns = find_matching_column(df_columns, catalog)\n",
        "\n",
        "    print(\"Initial column mapping:\")\n",
        "    for standard_name, actual_name in column_mapping.items():\n",
        "        print(f\"{standard_name}: {actual_name}\")\n",
        "\n",
        "    print(\"\\nUnmatched columns:\")\n",
        "    for col in unmatched_columns:\n",
        "        print(f\"No match found for '{col}'\")\n",
        "\n",
        "    new_mappings = {}\n",
        "    for col in unmatched_columns:\n",
        "        user_input = input(f\"\\nEnter the correct column name for '{col}' (or press Enter to skip): \").strip()\n",
        "        if user_input:\n",
        "            if user_input in column_mapping.values():\n",
        "                print(f\"Error: '{user_input}' is already mapped to another column. Please choose a different name.\")\n",
        "            else:\n",
        "                column_mapping[col] = user_input\n",
        "                new_mappings[col] = [user_input]\n",
        "\n",
        "    if new_mappings:\n",
        "        catalog = update_catalog(catalog, new_mappings)\n",
        "        save_catalog(catalog)\n",
        "        print(\"\\nCatalog updated with new mappings.\")\n",
        "\n",
        "    return column_mapping, catalog\n",
        "\n",
        "\n",
        "def etl_process(df):\n",
        "    catalog = load_or_create_catalog()\n",
        "\n",
        "    print(\"Original DataFrame columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    column_mapping, updated_catalog = interactive_column_mapping(df.columns, catalog)\n",
        "\n",
        "    processed_df = process_file(df, column_mapping)\n",
        "\n",
        "    print(\"\\nProcessed DataFrame:\")\n",
        "    print(processed_df.head())\n",
        "    print(\"\\nProcessed DataFrame columns:\")\n",
        "    print(processed_df.columns.tolist())\n",
        "\n",
        "    return processed_df, updated_catalog\n"
      ],
      "metadata": {
        "id": "a-y0CaBV6kQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process file"
      ],
      "metadata": {
        "id": "SvmdqkzUsFJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_client_input(file_path):\n",
        "    \"\"\"\n",
        "    Process the client's input Excel file.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): The path to the client's Excel file.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - pd.DataFrame: The processed DataFrame with mapped columns and additional required columns.\n",
        "        - dict: The updated catalog.\n",
        "    \"\"\"\n",
        "    # Load the catalog with the function\n",
        "    catalog = load_or_create_catalog_ingredients()\n",
        "\n",
        "    # Extract client name from the file name to put on table\n",
        "    client_name = os.path.basename(file_path).split(' - ')[0]\n",
        "\n",
        "    # Load the workbook\n",
        "    wb = load_workbook(file_path, read_only=True, data_only=True)\n",
        "\n",
        "    # Find the sheet with 'Data' in its name\n",
        "    data_sheet = next((sheet for sheet in wb.sheetnames if 'data' in sheet.lower()), None)\n",
        "    if not data_sheet:\n",
        "        raise ValueError(\"No sheet with 'Data' in its name found in the workbook.\")\n",
        "\n",
        "    # Read the Excel file\n",
        "    df = pd.read_excel(file_path, sheet_name=data_sheet)\n",
        "\n",
        "    print(\"Original DataFrame columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    # Perform interactive column mapping\n",
        "    column_mapping, updated_catalog = interactive_column_mapping(df.columns, catalog)\n",
        "\n",
        "    # Process the file with the new mapping\n",
        "    df = process_file(df, column_mapping)\n",
        "\n",
        "    # Filter the DataFrame to include only the columns from our catalog\n",
        "    catalog_columns = list(column_mapping.keys())\n",
        "    df = df[catalog_columns]\n",
        "\n",
        "\n",
        "    df['Stage'] = 'Opportunity Assessment Ingredients'\n",
        "    df['Client Name'] = client_name\n",
        "\n",
        "    print(\"\\nProcessed DataFrame:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nProcessed DataFrame columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    return df, updated_catalog\n",
        "\n",
        "def run_example():\n",
        "    \"\"\"\n",
        "    Run an example of the client input processing.\n",
        "\n",
        "    This function processes a sample Excel file and displays the results.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: The processed DataFrame.\n",
        "    \"\"\"\n",
        "    # File path (replace with Azure Blob Storage path when moving to Azure)\n",
        "    file_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/Kevin's Natural Foods - Opportunity Assessment.xlsx\"\n",
        "\n",
        "    # Process the client input\n",
        "    processed_df, updated_catalog = process_client_input(file_path)\n",
        "\n",
        "    # Here you would typically save the processed DataFrame or perform further analysis\n",
        "    print(\"\\nProcessing completed. DataFrame shape:\", processed_df.shape)\n",
        "    print(\"\\nFirst few rows of the processed DataFrame:\")\n",
        "    print(processed_df.head())\n",
        "\n",
        "    # Display the updated catalog\n",
        "    print(\"\\nUpdated Catalog:\")\n",
        "    print(json.dumps(updated_catalog, indent=2))\n",
        "\n",
        "    return processed_df\n",
        "\n",
        "# Run the example and store the result in a global variable\n",
        "df = run_example()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBqSf6UMsGwc",
        "outputId": "23088535-20cf-4c05-e64d-4816a7fc7c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame columns:\n",
            "['Itemcode', 'Item Description', 'Brand', 'Packsize', 'Vendor ID (PO)', 'Vendor Name (PO)', 'Receive Date', 'PO Number', 'PO Line #', 'Ordered Quantity', 'Order UOM', 'Quatity UOM', 'Received Quantity', 'Rec. UOM', 'Weight (LB)', 'Variance (Quantity)', 'Estimate Inventory Cost', 'Actual Inventory Cost', 'Variance (Inventory Cost)', 'Estimate Add-on Cost', 'Actual Add-on Cost', 'Variance (Add-on Cost)']\n",
            "Initial column mapping:\n",
            "Item ID: Itemcode\n",
            "Supplier Name: Vendor Name (PO)\n",
            "Item Description: Item Description\n",
            "UOM: Order UOM\n",
            "Quantity in UOM: Received Quantity\n",
            "Total: Actual Inventory Cost\n",
            "Date: Receive Date\n",
            "\n",
            "Unmatched columns:\n",
            "\n",
            "Processed DataFrame:\n",
            "    Item ID                        Supplier Name  \\\n",
            "0  FKC00073  PACIFIC SPICE COMPANY INC             \n",
            "1  FKC00073  PACIFIC SPICE COMPANY INC             \n",
            "2  FKC00073  PACIFIC SPICE COMPANY INC             \n",
            "3  FKC00073  PACIFIC SPICE COMPANY INC             \n",
            "4  FKC00073  PACIFIC SPICE COMPANY INC             \n",
            "\n",
            "                      Item Description          UOM  Quantity in UOM  \\\n",
            "0  KNF CHICKEN SEASONING 4.25OZ/12PK    CS                     283.0   \n",
            "1  KNF CHICKEN SEASONING 4.25OZ/12PK    CS                     107.0   \n",
            "2  KNF CHICKEN SEASONING 4.25OZ/12PK    CS                     600.0   \n",
            "3  KNF CHICKEN SEASONING 4.25OZ/12PK    CS                     260.0   \n",
            "4  KNF CHICKEN SEASONING 4.25OZ/12PK    CS                     320.0   \n",
            "\n",
            "      Total       Date                               Stage  \\\n",
            "0   4992.12 2021-09-08  Opportunity Assessment Ingredients   \n",
            "1   2157.12 2021-09-27  Opportunity Assessment Ingredients   \n",
            "2  12096.00 2021-12-28  Opportunity Assessment Ingredients   \n",
            "3   5241.60 2021-09-27  Opportunity Assessment Ingredients   \n",
            "4   6028.80 2021-08-18  Opportunity Assessment Ingredients   \n",
            "\n",
            "             Client Name  \n",
            "0  Kevin's Natural Foods  \n",
            "1  Kevin's Natural Foods  \n",
            "2  Kevin's Natural Foods  \n",
            "3  Kevin's Natural Foods  \n",
            "4  Kevin's Natural Foods  \n",
            "\n",
            "Processed DataFrame columns:\n",
            "['Item ID', 'Supplier Name', 'Item Description', 'UOM', 'Quantity in UOM', 'Total', 'Date', 'Stage', 'Client Name']\n",
            "\n",
            "Processing completed. DataFrame shape: (10893, 9)\n",
            "\n",
            "First few rows of the processed DataFrame:\n",
            "    Item ID                        Supplier Name  \\\n",
            "0  FKC00073  PACIFIC SPICE COMPANY INC             \n",
            "1  FKC00073  PACIFIC SPICE COMPANY INC             \n",
            "2  FKC00073  PACIFIC SPICE COMPANY INC             \n",
            "3  FKC00073  PACIFIC SPICE COMPANY INC             \n",
            "4  FKC00073  PACIFIC SPICE COMPANY INC             \n",
            "\n",
            "                      Item Description          UOM  Quantity in UOM  \\\n",
            "0  KNF CHICKEN SEASONING 4.25OZ/12PK    CS                     283.0   \n",
            "1  KNF CHICKEN SEASONING 4.25OZ/12PK    CS                     107.0   \n",
            "2  KNF CHICKEN SEASONING 4.25OZ/12PK    CS                     600.0   \n",
            "3  KNF CHICKEN SEASONING 4.25OZ/12PK    CS                     260.0   \n",
            "4  KNF CHICKEN SEASONING 4.25OZ/12PK    CS                     320.0   \n",
            "\n",
            "      Total       Date                               Stage  \\\n",
            "0   4992.12 2021-09-08  Opportunity Assessment Ingredients   \n",
            "1   2157.12 2021-09-27  Opportunity Assessment Ingredients   \n",
            "2  12096.00 2021-12-28  Opportunity Assessment Ingredients   \n",
            "3   5241.60 2021-09-27  Opportunity Assessment Ingredients   \n",
            "4   6028.80 2021-08-18  Opportunity Assessment Ingredients   \n",
            "\n",
            "             Client Name  \n",
            "0  Kevin's Natural Foods  \n",
            "1  Kevin's Natural Foods  \n",
            "2  Kevin's Natural Foods  \n",
            "3  Kevin's Natural Foods  \n",
            "4  Kevin's Natural Foods  \n",
            "\n",
            "Updated Catalog:\n",
            "{\n",
            "  \"Item ID\": [\n",
            "    \"Item ID\",\n",
            "    \"Item Number\",\n",
            "    \"Item code\",\n",
            "    \"Itemcode\",\n",
            "    \"SKU\"\n",
            "  ],\n",
            "  \"Supplier Name\": [\n",
            "    \"Supplier Name\",\n",
            "    \"Vendor Name\",\n",
            "    \"Supplier\",\n",
            "    \"Vendor\",\n",
            "    \"Vendor\",\n",
            "    \"Vendor Name (PO)\"\n",
            "  ],\n",
            "  \"Item Description\": [\n",
            "    \"Item Description\",\n",
            "    \"Product Description\",\n",
            "    \"Description\",\n",
            "    \"Item Name\",\n",
            "    \"Product Name\"\n",
            "  ],\n",
            "  \"UOM\": [\n",
            "    \"UOM\",\n",
            "    \"Unit of Measure\",\n",
            "    \"Purchase UOM\",\n",
            "    \"Order UOM\",\n",
            "    \" Order UOM \"\n",
            "  ],\n",
            "  \"Quantity in UOM\": [\n",
            "    \"Received Quantity\",\n",
            "    \"Qty Received In Purch UOM\",\n",
            "    \"Order Qty UOM\",\n",
            "    \"Quantity in Purchase UOM\",\n",
            "    \"Quatity UOM\",\n",
            "    \"Quantity\"\n",
            "  ],\n",
            "  \"Total\": [\n",
            "    \"Actual Inventory Cost\"\n",
            "  ],\n",
            "  \"Date\": [\n",
            "    \"Received Date\",\n",
            "    \"Receive Date\",\n",
            "    \"Purchase Date\",\n",
            "    \"Order Date\",\n",
            "    \"Transaction Date\",\n",
            "    \"Invoice Date\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look Up table"
      ],
      "metadata": {
        "id": "FvvuyM0lsvZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_supplier_ID(name):\n",
        "    \"\"\"\n",
        "    Generate a supplier ID from a supplier name.\n",
        "    Example: 'AMICK FARMS INC' -> 'AFI'\n",
        "    \"\"\"\n",
        "    # Take first letter of each word, up to 3 words\n",
        "    return ''.join(word[0] for word in str(name).upper().split()[:3])\n",
        "\n",
        "def load_ingredients_lookup(path):\n",
        "    \"\"\"\n",
        "    Load the ingredients lookup table from Excel file.\n",
        "    Returns a pandas DataFrame with supplier information.\n",
        "    \"\"\"\n",
        "    return pd.read_excel(path)\n",
        "\n",
        "def update_ingredients_lookup(existing_lookup, new_suppliers):\n",
        "    \"\"\"\n",
        "    Update the ingredients lookup table with new suppliers.\n",
        "\n",
        "    Parameters:\n",
        "    existing_lookup (pd.DataFrame): Existing lookup table with 'Supplier Name' and 'Supplier ID'\n",
        "    new_suppliers (list): List of new supplier names to add\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Updated lookup table with new suppliers added\n",
        "    \"\"\"\n",
        "    # Create a copy of existing lookup to avoid modifying original\n",
        "    updated_lookup = existing_lookup.copy()\n",
        "\n",
        "    # Track existing supplier IDs\n",
        "    existing_ids = set(existing_lookup['Supplier ID'].dropna())\n",
        "\n",
        "    # Process new suppliers\n",
        "    new_entries = []\n",
        "    for supplier in new_suppliers:\n",
        "        if supplier not in existing_lookup['Supplier Name'].values:\n",
        "            base_id = generate_supplier_ID(supplier)\n",
        "\n",
        "            # If base ID already exists, append a number\n",
        "            if base_id in existing_ids:\n",
        "                counter = 1\n",
        "                while f\"{base_id}{counter:03d}\" in existing_ids:\n",
        "                    counter += 1\n",
        "                supplier_id = f\"{base_id}{counter:03d}\"\n",
        "            else:\n",
        "                supplier_id = base_id\n",
        "\n",
        "            existing_ids.add(supplier_id)\n",
        "\n",
        "            new_entry = {\n",
        "                'Supplier Name': supplier,\n",
        "                'Supplier ID': supplier_id,\n",
        "                'Item Description': '',\n",
        "                'Type': '',\n",
        "                'Category': '',\n",
        "                'Opportunity Term': '',\n",
        "                'Est. Savings % (Low)': 0.00,\n",
        "                'Est. Savings % (High)': 0.00\n",
        "            }\n",
        "            new_entries.append(new_entry)\n",
        "\n",
        "    # Add new entries to the lookup table\n",
        "    if new_entries:\n",
        "        new_entries_df = pd.DataFrame(new_entries)\n",
        "        updated_lookup = pd.concat([updated_lookup, new_entries_df], ignore_index=True)\n",
        "\n",
        "    return updated_lookup\n"
      ],
      "metadata": {
        "id": "cbZKDxbqsux9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge"
      ],
      "metadata": {
        "id": "nHXZwywhuUKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def merge_vendor_data_ingredients(client_df, ingredients_lookup):\n",
        "    \"\"\"\n",
        "    Merge client vendor data with ingredients lookup table based on Item Description.\n",
        "\n",
        "    Parameters:\n",
        "    client_df (pd.DataFrame): Client data with columns:\n",
        "        - Item ID\n",
        "        - Supplier Name\n",
        "        - Item Description\n",
        "        - UOM\n",
        "        - Quantity in UOM\n",
        "        - Total\n",
        "        - Date\n",
        "    ingredients_lookup (pd.DataFrame): Ingredients lookup table with columns:\n",
        "        - Supplier Name\n",
        "        - Supplier ID\n",
        "        - Item Description\n",
        "        - Type\n",
        "        - Category\n",
        "        - Opportunity Term\n",
        "        - Est. Savings % (Low)\n",
        "        - Est. Savings % (High)\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Merged dataframe with calculated savings\n",
        "    \"\"\"\n",
        "    # Create copies to avoid modifying original dataframes\n",
        "    df = client_df.copy()\n",
        "    lookup = ingredients_lookup.copy()\n",
        "\n",
        "    # Convert Item Description to uppercase in both dataframes for case-insensitive merge\n",
        "    df['Item Description_upper'] = df['Item Description'].str.upper()\n",
        "    lookup['Item Description_upper'] = lookup['Item Description'].str.upper()\n",
        "\n",
        "    # Perform the merge\n",
        "    final_df = pd.merge(\n",
        "        df,\n",
        "        lookup,\n",
        "        on='Item Description_upper',\n",
        "        how='left',\n",
        "        suffixes=('', '_lookup')\n",
        "    )\n",
        "\n",
        "    # Remove the uppercase columns used for merging\n",
        "    final_df = final_df.drop(['Item Description_upper'], axis=1)\n",
        "\n",
        "    # Calculate estimated savings\n",
        "    final_df['Est. Savings (Low)'] = final_df['Total'] * final_df['Est. Savings % (Low)']\n",
        "    final_df['Est. Savings (High)'] = final_df['Total'] * final_df['Est. Savings % (High)']\n",
        "\n",
        "    # Handle potential NaN values in the new columns\n",
        "    final_df['Est. Savings (Low)'] = final_df['Est. Savings (Low)'].fillna(0)\n",
        "    final_df['Est. Savings (High)'] = final_df['Est. Savings (High)'].fillna(0)\n",
        "\n",
        "    # Reorder columns to a logical sequence\n",
        "    columns_order = [\n",
        "        'Item ID',\n",
        "        'Supplier Name',\n",
        "        'Supplier ID',\n",
        "        'Item Description',\n",
        "        'Type',\n",
        "        'Category',\n",
        "        'UOM',\n",
        "        'Quantity in UOM',\n",
        "        'Total',\n",
        "        'Est. Savings % (Low)',\n",
        "        'Est. Savings % (High)',\n",
        "        'Est. Savings (Low)',\n",
        "        'Est. Savings (High)',\n",
        "        'Opportunity Term',\n",
        "        'Date'\n",
        "    ]\n",
        "\n",
        "    # Only include columns that exist in the final dataframe\n",
        "    final_columns = [col for col in columns_order if col in final_df.columns]\n",
        "    final_df = final_df[final_columns]\n",
        "\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "ArZXassQuT0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing Data"
      ],
      "metadata": {
        "id": "Za5cvFXwva0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_missing_raw_data_report_ingredients(df, output_path):\n",
        "    \"\"\"\n",
        "    Create a report of missing raw data from the client's ingredients upload.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame with expected columns:\n",
        "        - Item ID\n",
        "        - Supplier Name\n",
        "        - Item Description\n",
        "        - UOM\n",
        "        - Quantity in UOM\n",
        "        - Total\n",
        "        - Date\n",
        "    output_path (str): Base path to save the Excel report (will append '_ingredients')\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Records with missing data\n",
        "    \"\"\"\n",
        "    columns_to_check = [\n",
        "        'Item ID',\n",
        "        'Supplier Name',\n",
        "        'Item Description',\n",
        "        'UOM',\n",
        "        'Quantity in UOM',\n",
        "        'Total',\n",
        "        'Date'\n",
        "    ]\n",
        "\n",
        "    # Modify output path to include _ingredients\n",
        "    base_name, ext = os.path.splitext(output_path)\n",
        "    ingredients_output_path = f\"{base_name}_ingredients{ext}\"\n",
        "\n",
        "    missing_mask = df[columns_to_check].isnull().any(axis=1)\n",
        "    missing_data = df[missing_mask]\n",
        "    missing_data.to_excel(ingredients_output_path, index=False)\n",
        "    print(f\"Missing raw data report saved to {ingredients_output_path}\")\n",
        "    return missing_data\n",
        "\n",
        "def create_missing_vendor_info_report_ingredients(final_df, ingredients_lookup, output_path):\n",
        "    \"\"\"\n",
        "    Create a report of missing vendor information for ingredients in the client's data.\n",
        "\n",
        "    Parameters:\n",
        "    final_df (pd.DataFrame): Merged client data\n",
        "    ingredients_lookup (pd.DataFrame): Ingredients lookup table\n",
        "    output_path (str): Base path to save the Excel report (will append '_ingredients')\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Records with missing vendor information\n",
        "    \"\"\"\n",
        "    columns_to_check = [\n",
        "        'Supplier ID',\n",
        "        'Type',\n",
        "        'Category',\n",
        "        'Opportunity Term',\n",
        "        'Est. Savings % (Low)',\n",
        "        'Est. Savings % (High)'\n",
        "    ]\n",
        "\n",
        "    # Modify output path to include _ingredients\n",
        "    base_name, ext = os.path.splitext(output_path)\n",
        "    ingredients_output_path = f\"{base_name}_ingredients{ext}\"\n",
        "\n",
        "    client_items = final_df['Item Description'].str.upper().unique()\n",
        "    vendor_info = ingredients_lookup[\n",
        "        ingredients_lookup['Item Description'].str.upper().isin(client_items)\n",
        "    ]\n",
        "\n",
        "    missing_mask = vendor_info[columns_to_check].isnull().any(axis=1)\n",
        "    missing_vendor_info = vendor_info[missing_mask]\n",
        "    missing_vendor_info.to_excel(ingredients_output_path, index=False)\n",
        "    print(f\"Missing vendor information report saved to {ingredients_output_path}\")\n",
        "    return missing_vendor_info\n",
        "\n",
        "def calculate_missing_data_percentage_ingredients(df):\n",
        "    \"\"\"\n",
        "    Calculate the percentage of missing data for each column in ingredients data.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame to analyze\n",
        "\n",
        "    Returns:\n",
        "    pd.Series: Percentage of missing data for each column\n",
        "    \"\"\"\n",
        "    relevant_columns = [\n",
        "        'Item ID',\n",
        "        'Supplier Name',\n",
        "        'Item Description',\n",
        "        'UOM',\n",
        "        'Quantity in UOM',\n",
        "        'Total',\n",
        "        'Date'\n",
        "    ]\n",
        "\n",
        "    # Only calculate for columns that exist in the DataFrame\n",
        "    columns_to_check = [col for col in relevant_columns if col in df.columns]\n",
        "\n",
        "    total_rows = len(df)\n",
        "    missing_percentages = (df[columns_to_check].isnull().sum() / total_rows * 100).round(2)\n",
        "    return missing_percentages.sort_values(ascending=False)\n",
        "\n",
        "def update_df_with_missing_data_ingredients(original_df, missing_data):\n",
        "    \"\"\"\n",
        "    Update the original ingredients DataFrame with data from the missing data file.\n",
        "\n",
        "    Parameters:\n",
        "    original_df (pd.DataFrame): Original ingredients data\n",
        "    missing_data (pd.DataFrame): DataFrame containing missing data to be added\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Updated DataFrame with missing data filled in\n",
        "    \"\"\"\n",
        "    key_columns = ['Item Description', 'Supplier Name', 'Item ID']\n",
        "\n",
        "    # Verify key columns exist\n",
        "    for col in key_columns.copy():\n",
        "        if col not in original_df.columns or col not in missing_data.columns:\n",
        "            print(f\"Warning: Key column '{col}' not found in one of the DataFrames. Skipping this column for matching.\")\n",
        "            key_columns.remove(col)\n",
        "\n",
        "    if not key_columns:\n",
        "        raise ValueError(\"No valid key columns for matching. Cannot update DataFrame.\")\n",
        "\n",
        "    # Perform the merge\n",
        "    updated_df = pd.merge(\n",
        "        original_df,\n",
        "        missing_data,\n",
        "        on=key_columns,\n",
        "        how='left',\n",
        "        suffixes=('', '_new')\n",
        "    )\n",
        "\n",
        "    # Update existing values with new data where available\n",
        "    for col in missing_data.columns:\n",
        "        if col in original_df.columns and col not in key_columns:\n",
        "            new_col = f'{col}_new'\n",
        "            if new_col in updated_df.columns:\n",
        "                mask = updated_df[new_col].notna()\n",
        "                updated_df.loc[mask, col] = updated_df.loc[mask, new_col]\n",
        "\n",
        "    # Remove temporary columns"
      ],
      "metadata": {
        "id": "8xMEs6Eqsuvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process File"
      ],
      "metadata": {
        "id": "3UFPfYmQwaMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def process_file_ingredients(df, column_mapping):\n",
        "    \"\"\"\n",
        "    Process ingredients file by standardizing column names and data types.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): Raw ingredients data DataFrame\n",
        "    column_mapping (dict): Mapping of standard column names to input column names\n",
        "        Expected standard columns:\n",
        "        - Item ID\n",
        "        - Supplier Name\n",
        "        - Item Description\n",
        "        - UOM\n",
        "        - Quantity in UOM\n",
        "        - Total\n",
        "        - Date\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Processed DataFrame with standardized columns and data types\n",
        "    \"\"\"\n",
        "    # Rename columns based on the mapping\n",
        "    df_renamed = df.rename(columns={v: k for k, v in column_mapping.items()})\n",
        "\n",
        "    # Check for duplicate columns\n",
        "    duplicate_columns = df_renamed.columns[df_renamed.columns.duplicated()].tolist()\n",
        "    if duplicate_columns:\n",
        "        print(f\"Warning: Duplicate columns found after renaming: {duplicate_columns}\")\n",
        "        # Append a suffix to duplicate columns\n",
        "        for col in duplicate_columns:\n",
        "            mask = df_renamed.columns == col\n",
        "            df_renamed.columns = [f'{col}_ingredients_{i}' if x else col\n",
        "                                for i, x in enumerate(mask)]\n",
        "\n",
        "    # Process specific columns with appropriate data types\n",
        "    if 'Date' in df_renamed.columns:\n",
        "        df_renamed['Date'] = pd.to_datetime(df_renamed['Date'], errors='coerce')\n",
        "\n",
        "    if 'Quantity in UOM' in df_renamed.columns:\n",
        "        df_renamed['Quantity in UOM'] = pd.to_numeric(\n",
        "            df_renamed['Quantity in UOM'],\n",
        "            errors='coerce'\n",
        "        )\n",
        "\n",
        "    if 'Total' in df_renamed.columns:\n",
        "        df_renamed['Total'] = pd.to_numeric(\n",
        "            df_renamed['Total'],\n",
        "            errors='coerce'\n",
        "        )\n",
        "\n",
        "    # Ensure string columns are properly formatted\n",
        "    string_columns = ['Item ID', 'Supplier Name', 'Item Description', 'UOM']\n",
        "    for col in string_columns:\n",
        "        if col in df_renamed.columns:\n",
        "            df_renamed[col] = df_renamed[col].astype(str).replace('nan', '')\n",
        "\n",
        "    # Remove any leading/trailing whitespace from string columns\n",
        "    for col in string_columns:\n",
        "        if col in df_renamed.columns:\n",
        "            df_renamed[col] = df_renamed[col].str.strip()\n",
        "\n",
        "    # Add warning for missing required columns\n",
        "    required_columns = [\n",
        "        'Item ID',\n",
        "        'Supplier Name',\n",
        "        'Item Description',\n",
        "        'UOM',\n",
        "        'Quantity in UOM',\n",
        "        'Total',\n",
        "        'Date'\n",
        "    ]\n",
        "\n",
        "    missing_columns = [col for col in required_columns\n",
        "                      if col not in df_renamed.columns]\n",
        "    if missing_columns:\n",
        "        print(f\"Warning: Missing required columns: {missing_columns}\")\n",
        "\n",
        "    return df_renamed\n"
      ],
      "metadata": {
        "id": "2bE9Ii6ksutE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process Client input"
      ],
      "metadata": {
        "id": "N5CIomQ2wyM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "def process_client_input_ingredients(file_path):\n",
        "    \"\"\"\n",
        "    Process the client's ingredients input Excel file.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): The path to the client's Excel file.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - pd.DataFrame: The processed DataFrame\n",
        "        - dict: The updated catalog\n",
        "    \"\"\"\n",
        "    # Load the catalog\n",
        "    catalog = load_or_create_catalog_ingredients()\n",
        "\n",
        "    # Extract client name from the file name\n",
        "    client_name = os.path.basename(file_path).split(' - ')[0]\n",
        "\n",
        "    # Load the workbook\n",
        "    wb = load_workbook(file_path, read_only=True, data_only=True)\n",
        "\n",
        "    print(\"Available sheets in workbook:\")\n",
        "    print(wb.sheetnames)\n",
        "\n",
        "    # Priority order for sheet detection\n",
        "    priority_sheets = [\n",
        "        'Receipt Data',  # Exact match for Receipt Data\n",
        "        'Data',         # Any sheet with \"Data\" in the name\n",
        "        'Receipt',      # Any sheet with \"Receipt\" in the name\n",
        "        'Transaction',  # Common alternative names\n",
        "        'Orders'\n",
        "    ]\n",
        "\n",
        "    data_sheet = None\n",
        "\n",
        "    # First try exact match for Receipt Data\n",
        "    if 'Receipt Data' in wb.sheetnames:\n",
        "        data_sheet = 'Receipt Data'\n",
        "        print(\"\\nFound 'Receipt Data' sheet\")\n",
        "    else:\n",
        "        # Try finding sheets containing priority keywords\n",
        "        for priority in priority_sheets:\n",
        "            matching_sheets = [sheet for sheet in wb.sheetnames\n",
        "                             if priority.lower() in sheet.lower()]\n",
        "            if matching_sheets:\n",
        "                data_sheet = matching_sheets[0]\n",
        "                print(f\"\\nFound matching sheet: {data_sheet}\")\n",
        "                break\n",
        "\n",
        "    # If no automatic match, ask user to select\n",
        "    if not data_sheet:\n",
        "        print(\"\\nNo data sheet found automatically. Please select from available sheets:\")\n",
        "        print(\"\\nAvailable sheets:\")\n",
        "        for i, sheet in enumerate(wb.sheetnames, 1):\n",
        "            print(f\"{i}. {sheet}\")\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                sheet_index = int(input(\"\\nPlease enter the number of the sheet containing the data: \")) - 1\n",
        "                if 0 <= sheet_index < len(wb.sheetnames):\n",
        "                    data_sheet = wb.sheetnames[sheet_index]\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"Invalid sheet number. Please try again.\")\n",
        "            except ValueError:\n",
        "                print(\"Please enter a valid number.\")\n",
        "\n",
        "    print(f\"\\nUsing sheet: {data_sheet}\")\n",
        "\n",
        "    # Preview the selected sheet before processing\n",
        "    print(\"\\nPreviewing selected sheet contents:\")\n",
        "    preview_df = pd.read_excel(file_path, sheet_name=data_sheet, nrows=5)\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    print(preview_df.head())\n",
        "    print(\"\\nColumns found:\")\n",
        "    print(preview_df.columns.tolist())\n",
        "\n",
        "    # Confirm sheet selection with user\n",
        "    confirmation = input(\"\\nIs this the correct sheet? (y/n): \").lower().strip()\n",
        "    while confirmation not in ['y', 'n']:\n",
        "        confirmation = input(\"Please enter 'y' or 'n': \").lower().strip()\n",
        "\n",
        "    if confirmation == 'n':\n",
        "        print(\"\\nPlease run the process again and select a different sheet.\")\n",
        "        return None, None\n",
        "\n",
        "    # Read the full sheet\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, sheet_name=data_sheet)\n",
        "\n",
        "        # Check if we need to skip rows (look for actual headers)\n",
        "        if df.columns.str.contains('Unnamed:').all():\n",
        "            print(\"\\nSearching for actual data rows...\")\n",
        "            for skip_rows in range(10):\n",
        "                temp_df = pd.read_excel(file_path, sheet_name=data_sheet, skiprows=skip_rows)\n",
        "                if not temp_df.columns.str.contains('Unnamed:').all():\n",
        "                    df = temp_df\n",
        "                    print(f\"Found headers at row {skip_rows + 1}\")\n",
        "                    break\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading Excel file: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    print(\"\\nProcessed DataFrame columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    # Rest of the function remains the same...\n",
        "    column_mapping, updated_catalog = interactive_column_mapping_ingredients(df.columns, catalog)\n",
        "    df = process_file_ingredients(df, column_mapping)\n",
        "\n",
        "    return df, updated_catalog\n",
        "\n",
        "def preview_sheet_contents(file_path, sheet_name):\n",
        "    \"\"\"\n",
        "    Preview the contents of a specific sheet to help with debugging.\n",
        "    \"\"\"\n",
        "    print(f\"\\nPreviewing contents of sheet '{sheet_name}':\")\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, sheet_name=sheet_name, nrows=5)\n",
        "        print(\"\\nFirst 5 rows:\")\n",
        "        print(df.head())\n",
        "        print(\"\\nColumns:\")\n",
        "        print(df.columns.tolist())\n",
        "        print(\"\\nShape:\", df.shape)\n",
        "    except Exception as e:\n",
        "        print(f\"Error previewing sheet: {str(e)}\")"
      ],
      "metadata": {
        "id": "xVlwFi1ssuqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Kevins"
      ],
      "metadata": {
        "id": "wYNAGL_Dxdvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_example_ingredients():\n",
        "    \"\"\"\n",
        "    Run the complete ingredients processing workflow.\n",
        "    \"\"\"\n",
        "    # Define paths\n",
        "    unified_ingredients_lookup_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/unified_items_ingredients_look_up.xlsx\"\n",
        "    client_file_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/Kevin's Natural Foods - Opportunity Assessment.xlsx\"\n",
        "    missing_raw_data_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/missing_raw_data_ingredients.xlsx\"\n",
        "    missing_vendor_info_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/missing_vendor_info_ingredients.xlsx\"\n",
        "\n",
        "    print(\"Starting ingredients processing workflow...\")\n",
        "\n",
        "    try:\n",
        "        # Load ingredients lookup table\n",
        "        print(\"Loading unified ingredients lookup table...\")\n",
        "        ingredients_lookup = load_ingredients_lookup(unified_ingredients_lookup_path)\n",
        "\n",
        "        if ingredients_lookup is None:\n",
        "            raise ValueError(\"load_ingredients_lookup returned None\")\n",
        "\n",
        "        print(\"\\nIngredients lookup table columns:\")\n",
        "        print(ingredients_lookup.columns.tolist())\n",
        "\n",
        "        print(\"\\nFirst few rows of ingredients lookup table:\")\n",
        "        print(ingredients_lookup.head())\n",
        "\n",
        "        # Process client input\n",
        "        print(\"\\nProcessing client input...\")\n",
        "        result = process_client_input_ingredients(client_file_path)\n",
        "\n",
        "        if result is None:\n",
        "            raise ValueError(\"process_client_input_ingredients returned None\")\n",
        "\n",
        "        if isinstance(result, tuple) and len(result) == 2:\n",
        "            df, updated_catalog = result\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected return type from process_client_input_ingredients: {type(result)}\")\n",
        "\n",
        "        print(\"\\nClient input DataFrame columns:\")\n",
        "        print(df.columns.tolist())\n",
        "\n",
        "        print(\"\\nFirst few rows of client input DataFrame:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Create missing raw data report\n",
        "        print(\"\\nCreating missing raw data report...\")\n",
        "        missing_raw_data = create_missing_raw_data_report_ingredients(df, missing_raw_data_path)\n",
        "        print(f\"Missing raw data report saved to {missing_raw_data_path}\")\n",
        "\n",
        "        # Calculate percentage of missing raw data\n",
        "        raw_data_missing_percentages = calculate_missing_data_percentage_ingredients(df)\n",
        "        print(\"\\nPercentage of missing data in raw data:\")\n",
        "        print(raw_data_missing_percentages)\n",
        "\n",
        "        # Get unique items from raw data\n",
        "        new_items = df['Item Description'].unique().tolist()\n",
        "\n",
        "        # Update ingredients lookup with new items\n",
        "        print(\"\\nUpdating ingredients lookup with new items...\")\n",
        "        ingredients_lookup = update_ingredients_lookup(ingredients_lookup, new_items)\n",
        "\n",
        "        # Merge ingredient data\n",
        "        print(\"\\nMerging ingredient data...\")\n",
        "        final_df = merge_vendor_data_ingredients(df, ingredients_lookup)\n",
        "\n",
        "        if final_df is None:\n",
        "            raise ValueError(\"merge_vendor_data_ingredients returned None\")\n",
        "\n",
        "        # Calculate estimated savings\n",
        "        if all(col in final_df.columns for col in ['Total', 'Est. Savings % (Low)', 'Est. Savings % (High)']):\n",
        "            final_df['Est. Savings (Low)'] = final_df['Total'] * final_df['Est. Savings % (Low)']\n",
        "            final_df['Est. Savings (High)'] = final_df['Total'] * final_df['Est. Savings % (High)']\n",
        "            print(\"Estimated savings calculated.\")\n",
        "        else:\n",
        "            print(\"Warning: Unable to calculate estimated savings. Required columns missing.\")\n",
        "\n",
        "        print(\"\\nFinal DataFrame columns after merging:\")\n",
        "        print(final_df.columns.tolist())\n",
        "\n",
        "        # Create missing vendor info report\n",
        "        print(\"\\nCreating missing ingredient info report...\")\n",
        "        missing_vendor_info = create_missing_vendor_info_report_ingredients(\n",
        "            final_df,\n",
        "            ingredients_lookup,\n",
        "            missing_vendor_info_path\n",
        "        )\n",
        "        print(f\"Missing ingredient information report saved to {missing_vendor_info_path}\")\n",
        "\n",
        "        # Calculate missing data percentages for final DataFrame\n",
        "        print(\"\\nCalculating missing data percentages...\")\n",
        "        final_df_missing_percentages = calculate_missing_data_percentage_ingredients(final_df)\n",
        "        print(\"\\nPercentage of missing data in final DataFrame:\")\n",
        "        print(final_df_missing_percentages)\n",
        "\n",
        "        # Identify missing items\n",
        "        print(\"\\nIdentifying missing items...\")\n",
        "        missing_items = final_df[final_df['Supplier ID'].isnull()]['Item Description'].unique()\n",
        "        print(f\"Number of missing items: {len(missing_items)}\")\n",
        "\n",
        "        # Calculate and display total spend and estimated savings\n",
        "        if all(col in final_df.columns for col in ['Total', 'Est. Savings (Low)', 'Est. Savings (High)']):\n",
        "            total_spend = final_df['Total'].sum()\n",
        "            total_low_savings = final_df['Est. Savings (Low)'].sum()\n",
        "            total_high_savings = final_df['Est. Savings (High)'].sum()\n",
        "\n",
        "            print(f\"\\nTotal Spend: ${total_spend:,.2f}\")\n",
        "            print(f\"Estimated Low Savings: ${total_low_savings:,.2f} ({total_low_savings/total_spend*100:.2f}%)\")\n",
        "            print(f\"Estimated High Savings: ${total_high_savings:,.2f} ({total_high_savings/total_spend*100:.2f}%)\")\n",
        "\n",
        "        # Save updated ingredients lookup\n",
        "        print(\"\\nSaving updated ingredients lookup...\")\n",
        "        ingredients_lookup.to_excel(unified_ingredients_lookup_path, index=False)\n",
        "        print(\"Updated ingredients lookup table saved.\")\n",
        "\n",
        "        # Save updated catalog if it exists\n",
        "        if updated_catalog is not None:\n",
        "            print(\"\\nSaving updated catalog...\")\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            updated_catalog_path = f\"/content/drive/MyDrive/Seismic/Seismic Python Automation/ingredients_catalog_{timestamp}.json\"\n",
        "            with open(updated_catalog_path, 'w') as f:\n",
        "                json.dump(updated_catalog, f, indent=4)\n",
        "            print(f\"Updated catalog saved to: {updated_catalog_path}\")\n",
        "\n",
        "        # Print final summary\n",
        "        print(\"\\nIngredients Processing Summary:\")\n",
        "        print(\"Raw Data Missing Percentages:\")\n",
        "        print(raw_data_missing_percentages)\n",
        "        print(\"\\nIngredient Info Missing Percentages:\")\n",
        "        vendor_info_missing_percentages = calculate_missing_data_percentage_ingredients(missing_vendor_info)\n",
        "        print(vendor_info_missing_percentages)\n",
        "        print(\"\\nFinal DataFrame Missing Percentages:\")\n",
        "        print(final_df_missing_percentages)\n",
        "\n",
        "        return (\n",
        "            final_df,\n",
        "            ingredients_lookup,\n",
        "            missing_raw_data,\n",
        "            missing_vendor_info,\n",
        "            final_df_missing_percentages\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred in ingredients processing: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None, None, None, None\n",
        "\n",
        "# Run the example\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_example_ingredients()\n",
        "    final_df, ingredients_lookup, missing_raw_data, missing_vendor_info, missing_percentages = results\n",
        "\n",
        "    if all(v is not None for v in results):\n",
        "        print(\"Ingredients processing completed successfully\")\n",
        "    else:\n",
        "        print(\"Ingredients processing failed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUAUhbXWsuoA",
        "outputId": "b9bd9a9b-ee7c-4e83-b2a4-065750385ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ingredients processing workflow...\n",
            "Loading unified ingredients lookup table...\n",
            "\n",
            "Ingredients lookup table columns:\n",
            "['Supplier Name', 'Supplier ID', 'Item Description', 'Type', 'Category', 'Opportunity Term', 'Est. Savings % (Low)', 'Est. Savings % (High)', 'Item Description_lower']\n",
            "\n",
            "First few rows of ingredients lookup table:\n",
            "                         Supplier Name Supplier ID  \\\n",
            "0  AMICK FARMS INC                             NaN   \n",
            "1  AMICK FARMS INC                             NaN   \n",
            "2  PMI PARKER MIGLIORINI INTERNATIONAL         NaN   \n",
            "3  PMI PARKER MIGLIORINI INTERNATIONAL         NaN   \n",
            "4  PMI PARKER MIGLIORINI INTERNATIONAL         NaN   \n",
            "\n",
            "                      Item Description         Type  Category  \\\n",
            "0  NAE BNLS SKNLS CHICKEN BREAST,FRESH  Ingredients  Proteins   \n",
            "1  NAE CHICKEN BRST PORTIONS, BULK      Ingredients  Proteins   \n",
            "2  GAP NAE CHX FRSH BRST >.31 LBS       Ingredients  Proteins   \n",
            "3  GRASS FED, CC TOP SIRLOIN MEX        Ingredients  Proteins   \n",
            "4  GRASS FED, CC TOP SIRLOIN RUMP       Ingredients  Proteins   \n",
            "\n",
            "   Opportunity Term  Est. Savings % (Low)  Est. Savings % (High)  \\\n",
            "0               NaN                  0.02                   0.05   \n",
            "1               NaN                  0.02                   0.05   \n",
            "2               NaN                  0.02                   0.05   \n",
            "3               NaN                  0.02                   0.05   \n",
            "4               NaN                  0.02                   0.05   \n",
            "\n",
            "                Item Description_lower  \n",
            "0  nae bnls sknls chicken breast,fresh  \n",
            "1  nae chicken brst portions, bulk      \n",
            "2  gap nae chx frsh brst >.31 lbs       \n",
            "3  grass fed, cc top sirloin mex        \n",
            "4  grass fed, cc top sirloin rump       \n",
            "\n",
            "Processing client input...\n",
            "Available sheets in workbook:\n",
            "['Packaging Summary', 'Packaging Summary SKU', 'Pkg Recent Categories', 'Ingredient Summary', 'Ingredient Summary SKU', 'Ingredient Category Tree', 'Vendor LkUp', 'Vendor LkUp1', 'Receipt Data']\n",
            "\n",
            "Found 'Receipt Data' sheet\n",
            "\n",
            "Using sheet: Receipt Data\n",
            "\n",
            "Previewing selected sheet contents:\n",
            "\n",
            "First 5 rows:\n",
            "   Itemcode                     Item Description       Brand  Packsize  \\\n",
            "0  FKC00073  KNF CHICKEN SEASONING 4.25OZ/12PK    KNF               12   \n",
            "1  FKC00073  KNF CHICKEN SEASONING 4.25OZ/12PK    KNF               12   \n",
            "2  FKC00073  KNF CHICKEN SEASONING 4.25OZ/12PK    KNF               12   \n",
            "3  FKC00073  KNF CHICKEN SEASONING 4.25OZ/12PK    KNF               12   \n",
            "4  FKC00073  KNF CHICKEN SEASONING 4.25OZ/12PK    KNF               12   \n",
            "\n",
            "  Vendor ID (PO)                     Vendor Name (PO) Receive Date  PO Number  \\\n",
            "0       PACI02    PACIFIC SPICE COMPANY INC             09/08/2021       4169   \n",
            "1       PACI02    PACIFIC SPICE COMPANY INC             09/27/2021       4169   \n",
            "2       PACI02    PACIFIC SPICE COMPANY INC             12/28/2021       4383   \n",
            "3       PACI02    PACIFIC SPICE COMPANY INC             09/27/2021       4380   \n",
            "4       PACI02    PACIFIC SPICE COMPANY INC             08/18/2021       4100   \n",
            "\n",
            "   PO Line #  Ordered Quantity    Order UOM  Quatity UOM  Received Quantity  \\\n",
            "0          1               390  CS                   390                283   \n",
            "1          6               107  CS                   107                107   \n",
            "2          1               520  CS                   520                600   \n",
            "3          1               260  CS                   260                260   \n",
            "4          6               320  CS                   320                320   \n",
            "\n",
            "    Rec. UOM  Weight (LB)  Variance (Quantity)  Estimate Inventory Cost  \\\n",
            "0  CS              902.77               -27.44                  4992.12   \n",
            "1  CS              341.33                 0.00                  2015.88   \n",
            "2  CS             1914.00                15.38                 12096.00   \n",
            "3  CS              829.40                 0.00                  5241.60   \n",
            "4  CS             1020.80                 0.00                  6028.80   \n",
            "\n",
            "   Actual Inventory Cost  Variance (Inventory Cost)  Estimate Add-on Cost  \\\n",
            "0                4992.12                       0.00                257.69   \n",
            "1                2157.12                       7.01                  0.00   \n",
            "2               12096.00                       0.00                346.50   \n",
            "3                5241.60                       0.00                238.24   \n",
            "4                6028.80                       0.00                  0.00   \n",
            "\n",
            "   Actual Add-on Cost  Variance (Add-on Cost)  \n",
            "0              257.69                       0  \n",
            "1                0.00                       0  \n",
            "2              346.50                       0  \n",
            "3              238.24                       0  \n",
            "4                0.00                       0  \n",
            "\n",
            "Columns found:\n",
            "['Itemcode', 'Item Description', 'Brand', 'Packsize', 'Vendor ID (PO)', 'Vendor Name (PO)', 'Receive Date', 'PO Number', 'PO Line #', 'Ordered Quantity', 'Order UOM', 'Quatity UOM', 'Received Quantity', 'Rec. UOM', 'Weight (LB)', 'Variance (Quantity)', 'Estimate Inventory Cost', 'Actual Inventory Cost', 'Variance (Inventory Cost)', 'Estimate Add-on Cost', 'Actual Add-on Cost', 'Variance (Add-on Cost)']\n",
            "\n",
            "Is this the correct sheet? (y/n): y\n",
            "\n",
            "Processed DataFrame columns:\n",
            "['Itemcode', 'Item Description', 'Brand', 'Packsize', 'Vendor ID (PO)', 'Vendor Name (PO)', 'Receive Date', 'PO Number', 'PO Line #', 'Ordered Quantity', 'Order UOM', 'Quatity UOM', 'Received Quantity', 'Rec. UOM', 'Weight (LB)', 'Variance (Quantity)', 'Estimate Inventory Cost', 'Actual Inventory Cost', 'Variance (Inventory Cost)', 'Estimate Add-on Cost', 'Actual Add-on Cost', 'Variance (Add-on Cost)']\n",
            "Initial column mapping:\n",
            "Item ID: Itemcode\n",
            "Supplier Name: Vendor Name (PO)\n",
            "Item Description: Item Description\n",
            "UOM: Order UOM\n",
            "Quantity in UOM: Received Quantity\n",
            "Total: Actual Inventory Cost\n",
            "Date: Receive Date\n",
            "\n",
            "Unmatched columns:\n",
            "\n",
            "Client input DataFrame columns:\n",
            "['Item ID', 'Item Description', 'Brand', 'Packsize', 'Vendor ID (PO)', 'Supplier Name', 'Date', 'PO Number', 'PO Line #', 'Ordered Quantity', 'UOM', 'Quatity UOM', 'Quantity in UOM', 'Rec. UOM', 'Weight (LB)', 'Variance (Quantity)', 'Estimate Inventory Cost', 'Total', 'Variance (Inventory Cost)', 'Estimate Add-on Cost', 'Actual Add-on Cost', 'Variance (Add-on Cost)']\n",
            "\n",
            "First few rows of client input DataFrame:\n",
            "    Item ID                   Item Description       Brand    Packsize  \\\n",
            "0  FKC00073  KNF CHICKEN SEASONING 4.25OZ/12PK  KNF         12           \n",
            "1  FKC00073  KNF CHICKEN SEASONING 4.25OZ/12PK  KNF         12           \n",
            "2  FKC00073  KNF CHICKEN SEASONING 4.25OZ/12PK  KNF         12           \n",
            "3  FKC00073  KNF CHICKEN SEASONING 4.25OZ/12PK  KNF         12           \n",
            "4  FKC00073  KNF CHICKEN SEASONING 4.25OZ/12PK  KNF         12           \n",
            "\n",
            "  Vendor ID (PO)              Supplier Name       Date  PO Number  PO Line #  \\\n",
            "0       PACI02    PACIFIC SPICE COMPANY INC 2021-09-08       4169          1   \n",
            "1       PACI02    PACIFIC SPICE COMPANY INC 2021-09-27       4169          6   \n",
            "2       PACI02    PACIFIC SPICE COMPANY INC 2021-12-28       4383          1   \n",
            "3       PACI02    PACIFIC SPICE COMPANY INC 2021-09-27       4380          1   \n",
            "4       PACI02    PACIFIC SPICE COMPANY INC 2021-08-18       4100          6   \n",
            "\n",
            "   Ordered Quantity UOM  Quatity UOM  Quantity in UOM   Rec. UOM  Weight (LB)  \\\n",
            "0             390.0  CS        390.0            283.0  CS              902.77   \n",
            "1             107.0  CS        107.0            107.0  CS              341.33   \n",
            "2             520.0  CS        520.0            600.0  CS             1914.00   \n",
            "3             260.0  CS        260.0            260.0  CS              829.40   \n",
            "4             320.0  CS        320.0            320.0  CS             1020.80   \n",
            "\n",
            "   Variance (Quantity)  Estimate Inventory Cost     Total  \\\n",
            "0               -27.44                  4992.12   4992.12   \n",
            "1                 0.00                  2015.88   2157.12   \n",
            "2                15.38                 12096.00  12096.00   \n",
            "3                 0.00                  5241.60   5241.60   \n",
            "4                 0.00                  6028.80   6028.80   \n",
            "\n",
            "   Variance (Inventory Cost)  Estimate Add-on Cost  Actual Add-on Cost  \\\n",
            "0                       0.00                257.69              257.69   \n",
            "1                       7.01                  0.00                0.00   \n",
            "2                       0.00                346.50              346.50   \n",
            "3                       0.00                238.24              238.24   \n",
            "4                       0.00                  0.00                0.00   \n",
            "\n",
            "   Variance (Add-on Cost)  \n",
            "0                     0.0  \n",
            "1                     0.0  \n",
            "2                     0.0  \n",
            "3                     0.0  \n",
            "4                     0.0  \n",
            "\n",
            "Creating missing raw data report...\n",
            "Missing raw data report saved to /content/drive/MyDrive/Seismic/Seismic Python Automation/missing_raw_data_ingredients_ingredients.xlsx\n",
            "Missing raw data report saved to /content/drive/MyDrive/Seismic/Seismic Python Automation/missing_raw_data_ingredients.xlsx\n",
            "\n",
            "Percentage of missing data in raw data:\n",
            "Item ID             0.0\n",
            "Supplier Name       0.0\n",
            "Item Description    0.0\n",
            "UOM                 0.0\n",
            "Quantity in UOM     0.0\n",
            "Total               0.0\n",
            "Date                0.0\n",
            "dtype: float64\n",
            "\n",
            "Updating ingredients lookup with new items...\n",
            "\n",
            "Merging ingredient data...\n",
            "Estimated savings calculated.\n",
            "\n",
            "Final DataFrame columns after merging:\n",
            "['Item ID', 'Supplier Name', 'Supplier ID', 'Item Description', 'Type', 'Category', 'UOM', 'Quantity in UOM', 'Total', 'Est. Savings % (Low)', 'Est. Savings % (High)', 'Est. Savings (Low)', 'Est. Savings (High)', 'Opportunity Term', 'Date']\n",
            "\n",
            "Creating missing ingredient info report...\n",
            "Missing vendor information report saved to /content/drive/MyDrive/Seismic/Seismic Python Automation/missing_vendor_info_ingredients_ingredients.xlsx\n",
            "Missing ingredient information report saved to /content/drive/MyDrive/Seismic/Seismic Python Automation/missing_vendor_info_ingredients.xlsx\n",
            "\n",
            "Calculating missing data percentages...\n",
            "\n",
            "Percentage of missing data in final DataFrame:\n",
            "Item ID             0.0\n",
            "Supplier Name       0.0\n",
            "Item Description    0.0\n",
            "UOM                 0.0\n",
            "Quantity in UOM     0.0\n",
            "Total               0.0\n",
            "Date                0.0\n",
            "dtype: float64\n",
            "\n",
            "Identifying missing items...\n",
            "Number of missing items: 691\n",
            "\n",
            "Total Spend: $1,238,378,099.26\n",
            "Estimated Low Savings: $21,289,287.69 (1.72%)\n",
            "Estimated High Savings: $53,223,219.21 (4.30%)\n",
            "\n",
            "Saving updated ingredients lookup...\n",
            "Updated ingredients lookup table saved.\n",
            "\n",
            "Saving updated catalog...\n",
            "Updated catalog saved to: /content/drive/MyDrive/Seismic/Seismic Python Automation/ingredients_catalog_20241024_192805.json\n",
            "\n",
            "Ingredients Processing Summary:\n",
            "Raw Data Missing Percentages:\n",
            "Item ID             0.0\n",
            "Supplier Name       0.0\n",
            "Item Description    0.0\n",
            "UOM                 0.0\n",
            "Quantity in UOM     0.0\n",
            "Total               0.0\n",
            "Date                0.0\n",
            "dtype: float64\n",
            "\n",
            "Ingredient Info Missing Percentages:\n",
            "Supplier Name       0.0\n",
            "Item Description    0.0\n",
            "dtype: float64\n",
            "\n",
            "Final DataFrame Missing Percentages:\n",
            "Item ID             0.0\n",
            "Supplier Name       0.0\n",
            "Item Description    0.0\n",
            "UOM                 0.0\n",
            "Quantity in UOM     0.0\n",
            "Total               0.0\n",
            "Date                0.0\n",
            "dtype: float64\n",
            "Ingredients processing completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.sort_values(by='Est. Savings (Low)',ascending=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "BnDlf_lUsukc",
        "outputId": "6300e51e-053d-404d-cc2c-bb8a8a0b7fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Item ID                        Supplier Name Supplier ID  \\\n",
              "9854   RB000019                  MARCUS FOOD COMPANY         NaN   \n",
              "9853   RB000019                  MARCUS FOOD COMPANY         NaN   \n",
              "6111   PF000009                          CRYOVAC INC         NaN   \n",
              "6112   PF000009                          CRYOVAC INC         NaN   \n",
              "9820   RB000016  PMI PARKER MIGLIORINI INTERNATIONAL         NaN   \n",
              "...         ...                                  ...         ...   \n",
              "22825  WST00058                    TRIBE 9 FOODS LLC         NaN   \n",
              "22826  WST00058                    TRIBE 9 FOODS LLC         NaN   \n",
              "22827  ZOS00004                   APEX DISPLAY GROUP         NaN   \n",
              "22828  ZOS00004                   APEX DISPLAY GROUP         NaN   \n",
              "22829  ZOS00004                   APEX DISPLAY GROUP         NaN   \n",
              "\n",
              "                          Item Description         Type            Category  \\\n",
              "9854   GRASS-FED, FRSH HEART OF RUMP 100VL  Ingredients            Proteins   \n",
              "9853   GRASS-FED, FRSH HEART OF RUMP 100VL  Ingredients            Proteins   \n",
              "6111   FILM, 420MM SAUCE CRYOVAC FS5050 5M    Packaging  Flexible Packaging   \n",
              "6112   FILM, 420MM SAUCE CRYOVAC FS5050 5M    Packaging  Flexible Packaging   \n",
              "9820   GRASS-FED, TOP RND/SRLN BEEF-FOREST  Ingredients            Proteins   \n",
              "...                                    ...          ...                 ...   \n",
              "22825      WIP - CAULI PASTA, MACARONI 5OZ          NaN                 NaN   \n",
              "22826      WIP - CAULI PASTA, MACARONI 5OZ          NaN                 NaN   \n",
              "22827     POS, KNF SAUCE POUCH DISPLAY PKG          NaN                 NaN   \n",
              "22828     POS, KNF SAUCE POUCH DISPLAY PKG          NaN                 NaN   \n",
              "22829     POS, KNF SAUCE POUCH DISPLAY PKG          NaN                 NaN   \n",
              "\n",
              "      UOM  Quantity in UOM      Total  Est. Savings % (Low)  \\\n",
              "9854   LB        39093.280  175918.95                  0.02   \n",
              "9853   LB        39093.280  175918.95                  0.02   \n",
              "6111   RL          420.000  175527.44                  0.02   \n",
              "6112   RL          420.000  175527.44                  0.02   \n",
              "9820   LB        41844.450  173652.64                  0.02   \n",
              "...    ..              ...        ...                   ...   \n",
              "22825  LB         2953.125   10129.22                   NaN   \n",
              "22826  LB         3937.500   13505.63                   NaN   \n",
              "22827  EA         1000.000   20210.00                   NaN   \n",
              "22828  EA         1000.000   19000.00                   NaN   \n",
              "22829  EA         1507.000   27849.36                   NaN   \n",
              "\n",
              "       Est. Savings % (High)  Est. Savings (Low)  Est. Savings (High)  \\\n",
              "9854                    0.05           3518.3790            8795.9475   \n",
              "9853                    0.05           3518.3790            8795.9475   \n",
              "6111                    0.05           3510.5488            8776.3720   \n",
              "6112                    0.05           3510.5488            8776.3720   \n",
              "9820                    0.05           3473.0528            8682.6320   \n",
              "...                      ...                 ...                  ...   \n",
              "22825                    NaN                 NaN                  NaN   \n",
              "22826                    NaN                 NaN                  NaN   \n",
              "22827                    NaN                 NaN                  NaN   \n",
              "22828                    NaN                 NaN                  NaN   \n",
              "22829                    NaN                 NaN                  NaN   \n",
              "\n",
              "       Opportunity Term       Date  \n",
              "9854                NaN 2024-04-25  \n",
              "9853                NaN 2024-04-25  \n",
              "6111                NaN 2023-03-09  \n",
              "6112                NaN 2023-03-09  \n",
              "9820                NaN 2022-08-02  \n",
              "...                 ...        ...  \n",
              "22825               NaN 2023-03-29  \n",
              "22826               NaN 2023-04-03  \n",
              "22827               NaN 2022-08-22  \n",
              "22828               NaN 2023-06-22  \n",
              "22829               NaN 2024-05-06  \n",
              "\n",
              "[22830 rows x 15 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-758af083-5703-4ff1-977b-3e358fc004c9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Item ID</th>\n",
              "      <th>Supplier Name</th>\n",
              "      <th>Supplier ID</th>\n",
              "      <th>Item Description</th>\n",
              "      <th>Type</th>\n",
              "      <th>Category</th>\n",
              "      <th>UOM</th>\n",
              "      <th>Quantity in UOM</th>\n",
              "      <th>Total</th>\n",
              "      <th>Est. Savings % (Low)</th>\n",
              "      <th>Est. Savings % (High)</th>\n",
              "      <th>Est. Savings (Low)</th>\n",
              "      <th>Est. Savings (High)</th>\n",
              "      <th>Opportunity Term</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9854</th>\n",
              "      <td>RB000019</td>\n",
              "      <td>MARCUS FOOD COMPANY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GRASS-FED, FRSH HEART OF RUMP 100VL</td>\n",
              "      <td>Ingredients</td>\n",
              "      <td>Proteins</td>\n",
              "      <td>LB</td>\n",
              "      <td>39093.280</td>\n",
              "      <td>175918.95</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.05</td>\n",
              "      <td>3518.3790</td>\n",
              "      <td>8795.9475</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2024-04-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9853</th>\n",
              "      <td>RB000019</td>\n",
              "      <td>MARCUS FOOD COMPANY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GRASS-FED, FRSH HEART OF RUMP 100VL</td>\n",
              "      <td>Ingredients</td>\n",
              "      <td>Proteins</td>\n",
              "      <td>LB</td>\n",
              "      <td>39093.280</td>\n",
              "      <td>175918.95</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.05</td>\n",
              "      <td>3518.3790</td>\n",
              "      <td>8795.9475</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2024-04-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6111</th>\n",
              "      <td>PF000009</td>\n",
              "      <td>CRYOVAC INC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>FILM, 420MM SAUCE CRYOVAC FS5050 5M</td>\n",
              "      <td>Packaging</td>\n",
              "      <td>Flexible Packaging</td>\n",
              "      <td>RL</td>\n",
              "      <td>420.000</td>\n",
              "      <td>175527.44</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.05</td>\n",
              "      <td>3510.5488</td>\n",
              "      <td>8776.3720</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2023-03-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6112</th>\n",
              "      <td>PF000009</td>\n",
              "      <td>CRYOVAC INC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>FILM, 420MM SAUCE CRYOVAC FS5050 5M</td>\n",
              "      <td>Packaging</td>\n",
              "      <td>Flexible Packaging</td>\n",
              "      <td>RL</td>\n",
              "      <td>420.000</td>\n",
              "      <td>175527.44</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.05</td>\n",
              "      <td>3510.5488</td>\n",
              "      <td>8776.3720</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2023-03-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9820</th>\n",
              "      <td>RB000016</td>\n",
              "      <td>PMI PARKER MIGLIORINI INTERNATIONAL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GRASS-FED, TOP RND/SRLN BEEF-FOREST</td>\n",
              "      <td>Ingredients</td>\n",
              "      <td>Proteins</td>\n",
              "      <td>LB</td>\n",
              "      <td>41844.450</td>\n",
              "      <td>173652.64</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.05</td>\n",
              "      <td>3473.0528</td>\n",
              "      <td>8682.6320</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2022-08-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22825</th>\n",
              "      <td>WST00058</td>\n",
              "      <td>TRIBE 9 FOODS LLC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WIP - CAULI PASTA, MACARONI 5OZ</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>LB</td>\n",
              "      <td>2953.125</td>\n",
              "      <td>10129.22</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2023-03-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22826</th>\n",
              "      <td>WST00058</td>\n",
              "      <td>TRIBE 9 FOODS LLC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WIP - CAULI PASTA, MACARONI 5OZ</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>LB</td>\n",
              "      <td>3937.500</td>\n",
              "      <td>13505.63</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2023-04-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22827</th>\n",
              "      <td>ZOS00004</td>\n",
              "      <td>APEX DISPLAY GROUP</td>\n",
              "      <td>NaN</td>\n",
              "      <td>POS, KNF SAUCE POUCH DISPLAY PKG</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EA</td>\n",
              "      <td>1000.000</td>\n",
              "      <td>20210.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2022-08-22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22828</th>\n",
              "      <td>ZOS00004</td>\n",
              "      <td>APEX DISPLAY GROUP</td>\n",
              "      <td>NaN</td>\n",
              "      <td>POS, KNF SAUCE POUCH DISPLAY PKG</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EA</td>\n",
              "      <td>1000.000</td>\n",
              "      <td>19000.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2023-06-22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22829</th>\n",
              "      <td>ZOS00004</td>\n",
              "      <td>APEX DISPLAY GROUP</td>\n",
              "      <td>NaN</td>\n",
              "      <td>POS, KNF SAUCE POUCH DISPLAY PKG</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EA</td>\n",
              "      <td>1507.000</td>\n",
              "      <td>27849.36</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2024-05-06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22830 rows × 15 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-758af083-5703-4ff1-977b-3e358fc004c9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-758af083-5703-4ff1-977b-3e358fc004c9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-758af083-5703-4ff1-977b-3e358fc004c9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-450bb6d5-c668-4b79-9946-31a25e2a2dfc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-450bb6d5-c668-4b79-9946-31a25e2a2dfc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-450bb6d5-c668-4b79-9946-31a25e2a2dfc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iNRj0E0AAK16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modifications\n",
        "skip rows where mayority headers null\n",
        "go to consolidates"
      ],
      "metadata": {
        "id": "vdnmvkjxCbXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_client_input_ingredients(file_path):\n",
        "    \"\"\"\n",
        "    Process the client's ingredients input Excel file.\n",
        "    Handles various sheet formats, empty rows, and header structures.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): The path to the client's Excel file.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing:\n",
        "        - pd.DataFrame: The processed DataFrame\n",
        "        - dict: The updated catalog\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the catalog\n",
        "        catalog = load_or_create_catalog_ingredients()\n",
        "\n",
        "        # Extract client name from the file name\n",
        "        client_name = os.path.basename(file_path).split(' - ')[0]\n",
        "\n",
        "        # Load the workbook\n",
        "        wb = load_workbook(file_path, read_only=True, data_only=True)\n",
        "\n",
        "        print(\"Available sheets in workbook:\")\n",
        "        print(wb.sheetnames)\n",
        "\n",
        "        # Priority order for sheet detection with row count check\n",
        "        priority_sheets = [\n",
        "            'Consolidated Data',  # Consolidated data sheets\n",
        "            'Receipt Data',      # Receipt data sheets\n",
        "            'Raw data',          # Raw data sheets\n",
        "            'Data',             # Generic data sheets\n",
        "            'Transaction'        # Transaction sheets\n",
        "        ]\n",
        "\n",
        "        # Gather information about all sheets\n",
        "        sheet_info = []\n",
        "        for sheet_name in wb.sheetnames:\n",
        "            try:\n",
        "                df_temp = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "                row_count = len(df_temp)\n",
        "                sheet_info.append({\n",
        "                    'name': sheet_name,\n",
        "                    'rows': row_count\n",
        "                })\n",
        "                print(f\"Sheet '{sheet_name}' has {row_count} rows\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading sheet '{sheet_name}': {str(e)}\")\n",
        "\n",
        "        # First try to find priority sheets with the most rows\n",
        "        selected_sheet = None\n",
        "        max_rows = 0\n",
        "\n",
        "        for priority in priority_sheets:\n",
        "            matching_sheets = [info for info in sheet_info\n",
        "                            if priority.lower() in info['name'].lower()]\n",
        "            if matching_sheets:\n",
        "                best_match = max(matching_sheets, key=lambda x: x['rows'])\n",
        "                if best_match['rows'] > max_rows:\n",
        "                    max_rows = best_match['rows']\n",
        "                    selected_sheet = best_match['name']\n",
        "                    print(f\"\\nFound priority sheet: {selected_sheet} with {max_rows} rows\")\n",
        "                    break\n",
        "\n",
        "        # If no priority match found, take the sheet with the most rows\n",
        "        if not selected_sheet and sheet_info:\n",
        "            best_match = max(sheet_info, key=lambda x: x['rows'])\n",
        "            selected_sheet = best_match['name']\n",
        "            max_rows = best_match['rows']\n",
        "            print(f\"\\nSelected sheet with most rows: {selected_sheet} with {max_rows} rows\")\n",
        "\n",
        "        print(f\"\\nUsing sheet: {selected_sheet}\")\n",
        "\n",
        "        # Function to find the actual header row\n",
        "        def find_header_row(df):\n",
        "            \"\"\"Find the row where actual headers start.\"\"\"\n",
        "            header_keywords = ['location', 'sku', 'product', 'description', 'type', 'code',\n",
        "                           'receipt', 'transaction', 'date', 'price', 'quantity']\n",
        "\n",
        "            for idx in range(min(len(df), 30)):  # Check first 30 rows\n",
        "                row = df.iloc[idx]\n",
        "                row_values = [str(val).lower().strip() for val in row.values if pd.notna(val)]\n",
        "\n",
        "                # Skip empty rows\n",
        "                if not row_values:\n",
        "                    continue\n",
        "\n",
        "                # Check if row contains common header keywords\n",
        "                if any(keyword in ' '.join(row_values) for keyword in header_keywords):\n",
        "                    # Verify next row contains actual data\n",
        "                    if idx + 1 < len(df):\n",
        "                        next_row = df.iloc[idx + 1]\n",
        "                        if not next_row.isnull().all():  # Check if next row is not empty\n",
        "                            return idx\n",
        "\n",
        "                # Alternative check: Look for pattern of consistent data types\n",
        "                if idx + 2 < len(df):\n",
        "                    next_row = df.iloc[idx + 1]\n",
        "                    next_row_values = [str(val).lower().strip() for val in next_row.values if pd.notna(val)]\n",
        "                    if len(next_row_values) > 0 and any(val.isdigit() or 'receipt' in val.lower() for val in next_row_values):\n",
        "                        return idx\n",
        "\n",
        "            return 0\n",
        "\n",
        "        # Read and process the data\n",
        "        try:\n",
        "            # Read first several rows to analyze structure\n",
        "            preview_df = pd.read_excel(file_path, sheet_name=selected_sheet, nrows=30)\n",
        "            header_row = find_header_row(preview_df)\n",
        "\n",
        "            print(f\"\\nAnalyzing data structure...\")\n",
        "            print(f\"Detected header row at index: {header_row}\")\n",
        "\n",
        "            # Read the actual data with the correct header row\n",
        "            df = pd.read_excel(file_path, sheet_name=selected_sheet, skiprows=header_row)\n",
        "\n",
        "            # Clean up column names\n",
        "            df.columns = [str(col).strip() for col in df.columns]\n",
        "\n",
        "            # Remove any completely empty rows\n",
        "            df = df.dropna(how='all')\n",
        "\n",
        "            # Remove any remaining unnamed columns that are completely empty\n",
        "            unnamed_cols = [col for col in df.columns\n",
        "                          if 'unnamed' in str(col).lower() and df[col].isnull().all()]\n",
        "            if unnamed_cols:\n",
        "                df = df.drop(columns=unnamed_cols)\n",
        "\n",
        "            print(\"\\nProcessed columns:\")\n",
        "            print(df.columns.tolist())\n",
        "            print(\"\\nFirst 5 rows of processed data:\")\n",
        "            print(df.head())\n",
        "\n",
        "            # Ask for confirmation\n",
        "            while True:\n",
        "                confirmation = input(\"\\nDoes this data look correct? (y/n): \").lower().strip()\n",
        "                if confirmation in ['y', 'n']:\n",
        "                    break\n",
        "                print(\"Please enter 'y' or 'n'\")\n",
        "\n",
        "            if confirmation == 'n':\n",
        "                # If data doesn't look correct, try alternative skiprows values\n",
        "                for skip_rows in [1, 2, 3, header_row + 1]:\n",
        "                    print(f\"\\nTrying with skip_rows = {skip_rows}\")\n",
        "                    df = pd.read_excel(file_path, sheet_name=selected_sheet, skiprows=skip_rows)\n",
        "                    print(\"\\nColumns:\")\n",
        "                    print(df.columns.tolist())\n",
        "                    print(\"\\nFirst 5 rows:\")\n",
        "                    print(df.head())\n",
        "\n",
        "                    confirmation = input(\"\\nDoes this data look correct? (y/n): \").lower().strip()\n",
        "                    if confirmation == 'y':\n",
        "                        break\n",
        "\n",
        "                if confirmation == 'n':\n",
        "                    print(\"\\nUnable to find correct data structure. Please check the file format.\")\n",
        "                    return None, None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "        # Process the columns\n",
        "        column_mapping, updated_catalog = interactive_column_mapping_ingredients(df.columns, catalog)\n",
        "        df = process_file_ingredients(df, column_mapping)\n",
        "\n",
        "        # Add client name if not present\n",
        "        if 'Client Name' not in df.columns:\n",
        "            df['Client Name'] = client_name\n",
        "\n",
        "        return df, updated_catalog\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the file: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "MNt71kPiCi_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find matching"
      ],
      "metadata": {
        "id": "CQyWczqHC_4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_matching_column(df_columns, catalog):\n",
        "    \"\"\"\n",
        "    Find matching columns between DataFrame columns and catalog entries.\n",
        "    Handles non-string values by converting to string before comparison.\n",
        "\n",
        "    Args:\n",
        "    df_columns: List or Index of DataFrame columns\n",
        "    catalog: Dictionary of standard column names and their variations\n",
        "\n",
        "    Returns:\n",
        "    tuple: (column_mapping, unmatched_columns)\n",
        "    \"\"\"\n",
        "    column_mapping = {}\n",
        "    unmatched_columns = []\n",
        "    df_columns = pd.Index(df_columns)\n",
        "\n",
        "    for standard_name, variations in catalog.items():\n",
        "        match_found = False\n",
        "        for possible_name in variations:\n",
        "            # Convert both sides to string and lowercase for comparison\n",
        "            exact_match = df_columns[df_columns.astype(str).str.lower() == str(possible_name).lower()]\n",
        "            if len(exact_match) > 0:\n",
        "                column_mapping[standard_name] = exact_match[0]\n",
        "                match_found = True\n",
        "                break\n",
        "\n",
        "            # For partial matches, also use string conversion\n",
        "            partial_matches = [col for col in df_columns\n",
        "                             if str(possible_name).lower() in str(col).lower()]\n",
        "            if partial_matches:\n",
        "                column_mapping[standard_name] = partial_matches[0]\n",
        "                match_found = True\n",
        "                break\n",
        "\n",
        "        if not match_found:\n",
        "            unmatched_columns.append(standard_name)\n",
        "\n",
        "    return column_mapping, unmatched_columns"
      ],
      "metadata": {
        "id": "gmGIli7GDB1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IoyOJQ8HDC2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_str_compare(val1, val2):\n",
        "    \"\"\"\n",
        "    Safely compare two values as strings, handling non-string types.\n",
        "\n",
        "    Args:\n",
        "    val1: First value to compare\n",
        "    val2: Second value to compare\n",
        "\n",
        "    Returns:\n",
        "    bool: True if the lowercase string representations match\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return str(val1).lower() == str(val2).lower()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def find_partial_matches(column, possible_name):\n",
        "    \"\"\"\n",
        "    Find partial matches between column name and possible name, handling non-string types.\n",
        "\n",
        "    Args:\n",
        "    column: Column name to check\n",
        "    possible_name: Name to look for\n",
        "\n",
        "    Returns:\n",
        "    bool: True if possible_name is found within column\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return str(possible_name).lower() in str(column).lower()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def find_header_row(df):\n",
        "    \"\"\"\n",
        "    Find the row where actual data headers start.\n",
        "\n",
        "    Args:\n",
        "    df: DataFrame to analyze\n",
        "\n",
        "    Returns:\n",
        "    int: Index of the header row\n",
        "    \"\"\"\n",
        "    # Keywords that indicate actual data headers\n",
        "    header_keywords = ['location', 'sku', 'product', 'description', 'type', 'code',\n",
        "                      'receipt', 'transaction', 'date', 'price', 'quantity']\n",
        "\n",
        "    for idx in range(min(len(df), 30)):  # Check first 30 rows\n",
        "        row = df.iloc[idx]\n",
        "        row_values = [str(val).lower().strip() for val in row.values if pd.notna(val)]\n",
        "\n",
        "        # Skip empty rows\n",
        "        if not row_values:\n",
        "            continue\n",
        "\n",
        "        # Check if row contains common header keywords\n",
        "        if any(keyword in ' '.join(row_values) for keyword in header_keywords):\n",
        "            # Verify next row contains actual data (not more headers)\n",
        "            if idx + 1 < len(df):\n",
        "                next_row = df.iloc[idx + 1]\n",
        "                next_row_values = [str(val).lower().strip() for val in next_row.values if pd.notna(val)]\n",
        "                if any(val.isdigit() for val in next_row_values):  # Check if next row has numeric values\n",
        "                    return idx\n",
        "\n",
        "        # Alternative check: Look for pattern of consistent data types in subsequent rows\n",
        "        if idx + 2 < len(df):\n",
        "            curr_row_types = [type(val) for val in row.values if pd.notna(val)]\n",
        "            next_row_types = [type(val) for val in df.iloc[idx + 1].values if pd.notna(val)]\n",
        "            if len(curr_row_types) > 0 and curr_row_types == next_row_types:\n",
        "                return idx - 1  # Return previous row as header\n",
        "\n",
        "    return 0\n"
      ],
      "metadata": {
        "id": "b0yGeHAODDdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Sabrosura"
      ],
      "metadata": {
        "id": "N-iWrrYXAKMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_example_ingredients():\n",
        "    \"\"\"\n",
        "    Run the complete ingredients processing workflow.\n",
        "    \"\"\"\n",
        "    # Define paths\n",
        "    unified_ingredients_lookup_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/unified_items_ingredients_look_up.xlsx\"\n",
        "    client_file_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/Opp Assesments/Sabrosura - Opportunity Assessment.xlsx\"\n",
        "    missing_raw_data_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/missing_raw_data_ingredients.xlsx\"\n",
        "    missing_vendor_info_path = \"/content/drive/MyDrive/Seismic/Seismic Python Automation/missing_vendor_info_ingredients.xlsx\"\n",
        "\n",
        "    print(\"Starting ingredients processing workflow...\")\n",
        "\n",
        "    try:\n",
        "        # Load ingredients lookup table\n",
        "        print(\"Loading unified ingredients lookup table...\")\n",
        "        ingredients_lookup = load_ingredients_lookup(unified_ingredients_lookup_path)\n",
        "\n",
        "        if ingredients_lookup is None:\n",
        "            raise ValueError(\"load_ingredients_lookup returned None\")\n",
        "\n",
        "        print(\"\\nIngredients lookup table columns:\")\n",
        "        print(ingredients_lookup.columns.tolist())\n",
        "\n",
        "        print(\"\\nFirst few rows of ingredients lookup table:\")\n",
        "        print(ingredients_lookup.head())\n",
        "\n",
        "        # Process client input\n",
        "        print(\"\\nProcessing client input...\")\n",
        "        result = process_client_input_ingredients(client_file_path)\n",
        "\n",
        "        if result is None:\n",
        "            raise ValueError(\"process_client_input_ingredients returned None\")\n",
        "\n",
        "        if isinstance(result, tuple) and len(result) == 2:\n",
        "            df, updated_catalog = result\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected return type from process_client_input_ingredients: {type(result)}\")\n",
        "\n",
        "        print(\"\\nClient input DataFrame columns:\")\n",
        "        print(df.columns.tolist())\n",
        "\n",
        "        print(\"\\nFirst few rows of client input DataFrame:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Create missing raw data report\n",
        "        print(\"\\nCreating missing raw data report...\")\n",
        "        missing_raw_data = create_missing_raw_data_report_ingredients(df, missing_raw_data_path)\n",
        "        print(f\"Missing raw data report saved to {missing_raw_data_path}\")\n",
        "\n",
        "        # Calculate percentage of missing raw data\n",
        "        raw_data_missing_percentages = calculate_missing_data_percentage_ingredients(df)\n",
        "        print(\"\\nPercentage of missing data in raw data:\")\n",
        "        print(raw_data_missing_percentages)\n",
        "\n",
        "        # Get unique items from raw data\n",
        "        new_items = df['Item Description'].unique().tolist()\n",
        "\n",
        "        # Update ingredients lookup with new items\n",
        "        print(\"\\nUpdating ingredients lookup with new items...\")\n",
        "        ingredients_lookup = update_ingredients_lookup(ingredients_lookup, new_items)\n",
        "\n",
        "        # Merge ingredient data\n",
        "        print(\"\\nMerging ingredient data...\")\n",
        "        final_df = merge_vendor_data_ingredients(df, ingredients_lookup)\n",
        "\n",
        "        if final_df is None:\n",
        "            raise ValueError(\"merge_vendor_data_ingredients returned None\")\n",
        "\n",
        "        # Calculate estimated savings\n",
        "        if all(col in final_df.columns for col in ['Total', 'Est. Savings % (Low)', 'Est. Savings % (High)']):\n",
        "            final_df['Est. Savings (Low)'] = final_df['Total'] * final_df['Est. Savings % (Low)']\n",
        "            final_df['Est. Savings (High)'] = final_df['Total'] * final_df['Est. Savings % (High)']\n",
        "            print(\"Estimated savings calculated.\")\n",
        "        else:\n",
        "            print(\"Warning: Unable to calculate estimated savings. Required columns missing.\")\n",
        "\n",
        "        print(\"\\nFinal DataFrame columns after merging:\")\n",
        "        print(final_df.columns.tolist())\n",
        "\n",
        "        # Create missing vendor info report\n",
        "        print(\"\\nCreating missing ingredient info report...\")\n",
        "        missing_vendor_info = create_missing_vendor_info_report_ingredients(\n",
        "            final_df,\n",
        "            ingredients_lookup,\n",
        "            missing_vendor_info_path\n",
        "        )\n",
        "        print(f\"Missing ingredient information report saved to {missing_vendor_info_path}\")\n",
        "\n",
        "        # Calculate missing data percentages for final DataFrame\n",
        "        print(\"\\nCalculating missing data percentages...\")\n",
        "        final_df_missing_percentages = calculate_missing_data_percentage_ingredients(final_df)\n",
        "        print(\"\\nPercentage of missing data in final DataFrame:\")\n",
        "        print(final_df_missing_percentages)\n",
        "\n",
        "        # Identify missing items\n",
        "        print(\"\\nIdentifying missing items...\")\n",
        "        missing_items = final_df[final_df['Supplier ID'].isnull()]['Item Description'].unique()\n",
        "        print(f\"Number of missing items: {len(missing_items)}\")\n",
        "\n",
        "        # Calculate and display total spend and estimated savings\n",
        "        if all(col in final_df.columns for col in ['Total', 'Est. Savings (Low)', 'Est. Savings (High)']):\n",
        "            total_spend = final_df['Total'].sum()\n",
        "            total_low_savings = final_df['Est. Savings (Low)'].sum()\n",
        "            total_high_savings = final_df['Est. Savings (High)'].sum()\n",
        "\n",
        "            print(f\"\\nTotal Spend: ${total_spend:,.2f}\")\n",
        "            print(f\"Estimated Low Savings: ${total_low_savings:,.2f} ({total_low_savings/total_spend*100:.2f}%)\")\n",
        "            print(f\"Estimated High Savings: ${total_high_savings:,.2f} ({total_high_savings/total_spend*100:.2f}%)\")\n",
        "\n",
        "        # Save updated ingredients lookup\n",
        "        print(\"\\nSaving updated ingredients lookup...\")\n",
        "        ingredients_lookup.to_excel(unified_ingredients_lookup_path, index=False)\n",
        "        print(\"Updated ingredients lookup table saved.\")\n",
        "\n",
        "        # Save updated catalog if it exists\n",
        "        if updated_catalog is not None:\n",
        "            print(\"\\nSaving updated catalog...\")\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            updated_catalog_path = f\"/content/drive/MyDrive/Seismic/Seismic Python Automation/ingredients_catalog_{timestamp}.json\"\n",
        "            with open(updated_catalog_path, 'w') as f:\n",
        "                json.dump(updated_catalog, f, indent=4)\n",
        "            print(f\"Updated catalog saved to: {updated_catalog_path}\")\n",
        "\n",
        "        # Print final summary\n",
        "        print(\"\\nIngredients Processing Summary:\")\n",
        "        print(\"Raw Data Missing Percentages:\")\n",
        "        print(raw_data_missing_percentages)\n",
        "        print(\"\\nIngredient Info Missing Percentages:\")\n",
        "        vendor_info_missing_percentages = calculate_missing_data_percentage_ingredients(missing_vendor_info)\n",
        "        print(vendor_info_missing_percentages)\n",
        "        print(\"\\nFinal DataFrame Missing Percentages:\")\n",
        "        print(final_df_missing_percentages)\n",
        "\n",
        "        return (\n",
        "            final_df,\n",
        "            ingredients_lookup,\n",
        "            missing_raw_data,\n",
        "            missing_vendor_info,\n",
        "            final_df_missing_percentages\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred in ingredients processing: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None, None, None, None\n",
        "\n",
        "# Run the example\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_example_ingredients()\n",
        "    final_df, ingredients_lookup, missing_raw_data, missing_vendor_info, missing_percentages = results\n",
        "\n",
        "    if all(v is not None for v in results):\n",
        "        print(\"Ingredients processing completed successfully\")\n",
        "    else:\n",
        "        print(\"Ingredients processing failed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ayi3LDpALXZ",
        "outputId": "dfe77732-0a41-4bfd-d0f2-aed9b976550c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ingredients processing workflow...\n",
            "Loading unified ingredients lookup table...\n",
            "\n",
            "Ingredients lookup table columns:\n",
            "['Supplier Name', 'Supplier ID', 'Item Description', 'Type', 'Category', 'Opportunity Term', 'Est. Savings % (Low)', 'Est. Savings % (High)', 'Item Description_lower']\n",
            "\n",
            "First few rows of ingredients lookup table:\n",
            "                         Supplier Name Supplier ID  \\\n",
            "0  AMICK FARMS INC                             NaN   \n",
            "1  AMICK FARMS INC                             NaN   \n",
            "2  PMI PARKER MIGLIORINI INTERNATIONAL         NaN   \n",
            "3  PMI PARKER MIGLIORINI INTERNATIONAL         NaN   \n",
            "4  PMI PARKER MIGLIORINI INTERNATIONAL         NaN   \n",
            "\n",
            "                      Item Description         Type  Category  \\\n",
            "0  NAE BNLS SKNLS CHICKEN BREAST,FRESH  Ingredients  Proteins   \n",
            "1  NAE CHICKEN BRST PORTIONS, BULK      Ingredients  Proteins   \n",
            "2  GAP NAE CHX FRSH BRST >.31 LBS       Ingredients  Proteins   \n",
            "3  GRASS FED, CC TOP SIRLOIN MEX        Ingredients  Proteins   \n",
            "4  GRASS FED, CC TOP SIRLOIN RUMP       Ingredients  Proteins   \n",
            "\n",
            "   Opportunity Term  Est. Savings % (Low)  Est. Savings % (High)  \\\n",
            "0               NaN                  0.02                   0.05   \n",
            "1               NaN                  0.02                   0.05   \n",
            "2               NaN                  0.02                   0.05   \n",
            "3               NaN                  0.02                   0.05   \n",
            "4               NaN                  0.02                   0.05   \n",
            "\n",
            "                Item Description_lower  \n",
            "0  nae bnls sknls chicken breast,fresh  \n",
            "1  nae chicken brst portions, bulk      \n",
            "2  gap nae chx frsh brst >.31 lbs       \n",
            "3  grass fed, cc top sirloin mex        \n",
            "4  grass fed, cc top sirloin rump       \n",
            "\n",
            "Processing client input...\n",
            "Available sheets in workbook:\n",
            "[' Questionnaire', 'Summary', 'Consolidated Data', 'Raw data WI', 'Raw data CA', 'Seismic Categories']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sheet ' Questionnaire' has 31 rows\n",
            "Sheet 'Summary' has 51 rows\n",
            "Sheet 'Consolidated Data' has 4462 rows\n",
            "Sheet 'Raw data WI' has 3738 rows\n",
            "Sheet 'Raw data CA' has 1459 rows\n",
            "Sheet 'Seismic Categories' has 29 rows\n",
            "\n",
            "Found priority sheet: Consolidated Data with 4462 rows\n",
            "\n",
            "Using sheet: Consolidated Data\n",
            "\n",
            "Analyzing data structure...\n",
            "Detected header row at index: 0\n",
            "\n",
            "Processed columns:\n",
            "['Unnamed: 0', 'Unnamed: 1', '501', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', '13861301.217894038', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18']\n",
            "\n",
            "First 5 rows of processed data:\n",
            "   Unnamed: 0 Unnamed: 1    501         Unnamed: 3    Unnamed: 4  \\\n",
            "0    Location    Product  SKU's           Txn Type  Product Code   \n",
            "1  California   DP300-10      1  Receipts to Stock         DP300   \n",
            "2  California   DP300-10      0  Receipts to Stock         DP300   \n",
            "3  California   DP300-10      0  Receipts to Stock         DP300   \n",
            "4  California   DP300-10      0  Receipts to Stock         DP300   \n",
            "\n",
            "                Unnamed: 5                                Unnamed: 6  \\\n",
            "0                Warehouse                               Description   \n",
            "1  10 - Alcoa Cold Storage  LITHO FILM RICE PUDDING 8OZ LID 24686/rl   \n",
            "2  10 - Alcoa Cold Storage  LITHO FILM RICE PUDDING 8OZ LID 24686/rl   \n",
            "3  10 - Alcoa Cold Storage  LITHO FILM RICE PUDDING 8OZ LID 24686/rl   \n",
            "4  10 - Alcoa Cold Storage  LITHO FILM RICE PUDDING 8OZ LID 24686/rl   \n",
            "\n",
            "  Unnamed: 7           Unnamed: 8 Unnamed: 9   Unnamed: 10     Unnamed: 11  \\\n",
            "0       Type            Tran Date      Month  Product Cost  Inv Impact Stk   \n",
            "1        NaN  2022-11-29 00:00:00         11         0.009          516488   \n",
            "2        NaN  2023-01-26 00:00:00          1        0.0079         3212954   \n",
            "3        NaN  2023-03-22 00:00:00          3        0.0081         2017302   \n",
            "4        NaN  2023-05-15 00:00:00          5        0.0081         1879746   \n",
            "\n",
            "  13861301.217894038            Unnamed: 13         Unnamed: 14  \\\n",
            "0      Receipt Value       Product Category    Seismic Category   \n",
            "1           4648.392  711 - Containers/Lids  Plastic Containers   \n",
            "2         25382.3366  711 - Containers/Lids  Plastic Containers   \n",
            "3         16340.1462  711 - Containers/Lids  Plastic Containers   \n",
            "4         15225.9426  711 - Containers/Lids  Plastic Containers   \n",
            "\n",
            "            Unnamed: 15       Unnamed: 16       Unnamed: 17     Unnamed: 18  \n",
            "0  Seismic Sub-Category              Name  Current Supplier         Process  \n",
            "1       Containers/Lids  SUNSHINE FPC INC  SUNSHINE FPC INC  30 - Packaging  \n",
            "2       Containers/Lids  SUNSHINE FPC INC  SUNSHINE FPC INC  30 - Packaging  \n",
            "3       Containers/Lids  SUNSHINE FPC INC  SUNSHINE FPC INC  30 - Packaging  \n",
            "4       Containers/Lids  SUNSHINE FPC INC  SUNSHINE FPC INC  30 - Packaging  \n",
            "\n",
            "Does this data look correct? (y/n): n\n",
            "\n",
            "Trying with skip_rows = 1\n",
            "\n",
            "Columns:\n",
            "['Location', 'Product', \"SKU's\", 'Txn Type', 'Product Code', 'Warehouse', 'Description', 'Type', 'Tran Date', 'Month', 'Product Cost', 'Inv Impact Stk', 'Receipt Value', 'Product Category', 'Seismic Category', 'Seismic Sub-Category', 'Name', 'Current Supplier', 'Process']\n",
            "\n",
            "First 5 rows:\n",
            "     Location   Product  SKU's           Txn Type Product Code  \\\n",
            "0  California  DP300-10      1  Receipts to Stock        DP300   \n",
            "1  California  DP300-10      0  Receipts to Stock        DP300   \n",
            "2  California  DP300-10      0  Receipts to Stock        DP300   \n",
            "3  California  DP300-10      0  Receipts to Stock        DP300   \n",
            "4  California  DP300-10      0  Receipts to Stock        DP300   \n",
            "\n",
            "                 Warehouse                               Description Type  \\\n",
            "0  10 - Alcoa Cold Storage  LITHO FILM RICE PUDDING 8OZ LID 24686/rl  NaN   \n",
            "1  10 - Alcoa Cold Storage  LITHO FILM RICE PUDDING 8OZ LID 24686/rl  NaN   \n",
            "2  10 - Alcoa Cold Storage  LITHO FILM RICE PUDDING 8OZ LID 24686/rl  NaN   \n",
            "3  10 - Alcoa Cold Storage  LITHO FILM RICE PUDDING 8OZ LID 24686/rl  NaN   \n",
            "4  10 - Alcoa Cold Storage  LITHO FILM RICE PUDDING 8OZ LID 24686/rl  NaN   \n",
            "\n",
            "   Tran Date  Month  Product Cost  Inv Impact Stk  Receipt Value  \\\n",
            "0 2022-11-29     11        0.0090        516488.0      4648.3920   \n",
            "1 2023-01-26      1        0.0079       3212954.0     25382.3366   \n",
            "2 2023-03-22      3        0.0081       2017302.0     16340.1462   \n",
            "3 2023-05-15      5        0.0081       1879746.0     15225.9426   \n",
            "4 2023-08-30      8        0.0079       2972970.0     23486.4630   \n",
            "\n",
            "        Product Category    Seismic Category Seismic Sub-Category  \\\n",
            "0  711 - Containers/Lids  Plastic Containers      Containers/Lids   \n",
            "1  711 - Containers/Lids  Plastic Containers      Containers/Lids   \n",
            "2  711 - Containers/Lids  Plastic Containers      Containers/Lids   \n",
            "3  711 - Containers/Lids  Plastic Containers      Containers/Lids   \n",
            "4  711 - Containers/Lids  Plastic Containers      Containers/Lids   \n",
            "\n",
            "               Name  Current Supplier         Process  \n",
            "0  SUNSHINE FPC INC  SUNSHINE FPC INC  30 - Packaging  \n",
            "1  SUNSHINE FPC INC  SUNSHINE FPC INC  30 - Packaging  \n",
            "2  SUNSHINE FPC INC  SUNSHINE FPC INC  30 - Packaging  \n",
            "3  SUNSHINE FPC INC  SUNSHINE FPC INC  30 - Packaging  \n",
            "4  SUNSHINE FPC INC  SUNSHINE FPC INC  30 - Packaging  \n",
            "\n",
            "Does this data look correct? (y/n): y\n",
            "Initial column mapping:\n",
            "Item ID: SKU's\n",
            "Supplier Name: Current Supplier\n",
            "Item Description: Description\n",
            "\n",
            "Unmatched columns:\n",
            "No match found for 'UOM'\n",
            "No match found for 'Quantity in UOM'\n",
            "No match found for 'Total'\n",
            "No match found for 'Date'\n",
            "\n",
            "Enter the correct column name for 'UOM' (or press Enter to skip): Type\n",
            "\n",
            "Enter the correct column name for 'Quantity in UOM' (or press Enter to skip): Inv Impact Stk\n",
            "\n",
            "Enter the correct column name for 'Total' (or press Enter to skip): Receipt Value\n",
            "\n",
            "Enter the correct column name for 'Date' (or press Enter to skip): Tran Date\n",
            "\n",
            "Catalog updated with new mappings.\n",
            "\n",
            "Client input DataFrame columns:\n",
            "['Location', 'Product', 'Item ID', 'Txn Type', 'Product Code', 'Warehouse', 'Item Description', 'UOM', 'Date', 'Month', 'Product Cost', 'Quantity in UOM', 'Total', 'Product Category', 'Seismic Category', 'Seismic Sub-Category', 'Name', 'Supplier Name', 'Process', 'Client Name']\n",
            "\n",
            "First few rows of client input DataFrame:\n",
            "     Location   Product Item ID           Txn Type Product Code  \\\n",
            "0  California  DP300-10       1  Receipts to Stock        DP300   \n",
            "1  California  DP300-10       0  Receipts to Stock        DP300   \n",
            "2  California  DP300-10       0  Receipts to Stock        DP300   \n",
            "3  California  DP300-10       0  Receipts to Stock        DP300   \n",
            "4  California  DP300-10       0  Receipts to Stock        DP300   \n",
            "\n",
            "                 Warehouse                          Item Description UOM  \\\n",
            "0  10 - Alcoa Cold Storage  LITHO FILM RICE PUDDING 8OZ LID 24686/rl       \n",
            "1  10 - Alcoa Cold Storage  LITHO FILM RICE PUDDING 8OZ LID 24686/rl       \n",
            "2  10 - Alcoa Cold Storage  LITHO FILM RICE PUDDING 8OZ LID 24686/rl       \n",
            "3  10 - Alcoa Cold Storage  LITHO FILM RICE PUDDING 8OZ LID 24686/rl       \n",
            "4  10 - Alcoa Cold Storage  LITHO FILM RICE PUDDING 8OZ LID 24686/rl       \n",
            "\n",
            "        Date  Month  Product Cost  Quantity in UOM       Total  \\\n",
            "0 2022-11-29     11        0.0090         516488.0   4648.3920   \n",
            "1 2023-01-26      1        0.0079        3212954.0  25382.3366   \n",
            "2 2023-03-22      3        0.0081        2017302.0  16340.1462   \n",
            "3 2023-05-15      5        0.0081        1879746.0  15225.9426   \n",
            "4 2023-08-30      8        0.0079        2972970.0  23486.4630   \n",
            "\n",
            "        Product Category    Seismic Category Seismic Sub-Category  \\\n",
            "0  711 - Containers/Lids  Plastic Containers      Containers/Lids   \n",
            "1  711 - Containers/Lids  Plastic Containers      Containers/Lids   \n",
            "2  711 - Containers/Lids  Plastic Containers      Containers/Lids   \n",
            "3  711 - Containers/Lids  Plastic Containers      Containers/Lids   \n",
            "4  711 - Containers/Lids  Plastic Containers      Containers/Lids   \n",
            "\n",
            "               Name     Supplier Name         Process Client Name  \n",
            "0  SUNSHINE FPC INC  SUNSHINE FPC INC  30 - Packaging   Sabrosura  \n",
            "1  SUNSHINE FPC INC  SUNSHINE FPC INC  30 - Packaging   Sabrosura  \n",
            "2  SUNSHINE FPC INC  SUNSHINE FPC INC  30 - Packaging   Sabrosura  \n",
            "3  SUNSHINE FPC INC  SUNSHINE FPC INC  30 - Packaging   Sabrosura  \n",
            "4  SUNSHINE FPC INC  SUNSHINE FPC INC  30 - Packaging   Sabrosura  \n",
            "\n",
            "Creating missing raw data report...\n",
            "Missing raw data report saved to /content/drive/MyDrive/Seismic/Seismic Python Automation/missing_raw_data_ingredients_ingredients.xlsx\n",
            "Missing raw data report saved to /content/drive/MyDrive/Seismic/Seismic Python Automation/missing_raw_data_ingredients.xlsx\n",
            "\n",
            "Percentage of missing data in raw data:\n",
            "Item ID             0.0\n",
            "Supplier Name       0.0\n",
            "Item Description    0.0\n",
            "UOM                 0.0\n",
            "Quantity in UOM     0.0\n",
            "Total               0.0\n",
            "Date                0.0\n",
            "dtype: float64\n",
            "\n",
            "Updating ingredients lookup with new items...\n",
            "\n",
            "Merging ingredient data...\n",
            "Estimated savings calculated.\n",
            "\n",
            "Final DataFrame columns after merging:\n",
            "['Item ID', 'Supplier Name', 'Supplier ID', 'Item Description', 'Type', 'Category', 'UOM', 'Quantity in UOM', 'Total', 'Est. Savings % (Low)', 'Est. Savings % (High)', 'Est. Savings (Low)', 'Est. Savings (High)', 'Opportunity Term', 'Date']\n",
            "\n",
            "Creating missing ingredient info report...\n",
            "Missing vendor information report saved to /content/drive/MyDrive/Seismic/Seismic Python Automation/missing_vendor_info_ingredients_ingredients.xlsx\n",
            "Missing ingredient information report saved to /content/drive/MyDrive/Seismic/Seismic Python Automation/missing_vendor_info_ingredients.xlsx\n",
            "\n",
            "Calculating missing data percentages...\n",
            "\n",
            "Percentage of missing data in final DataFrame:\n",
            "Item ID             0.0\n",
            "Supplier Name       0.0\n",
            "Item Description    0.0\n",
            "UOM                 0.0\n",
            "Quantity in UOM     0.0\n",
            "Total               0.0\n",
            "Date                0.0\n",
            "dtype: float64\n",
            "\n",
            "Identifying missing items...\n",
            "Number of missing items: 472\n",
            "\n",
            "Total Spend: $13,861,301.22\n",
            "Estimated Low Savings: $0.00 (0.00%)\n",
            "Estimated High Savings: $0.00 (0.00%)\n",
            "\n",
            "Saving updated ingredients lookup...\n",
            "Updated ingredients lookup table saved.\n",
            "\n",
            "Saving updated catalog...\n",
            "Updated catalog saved to: /content/drive/MyDrive/Seismic/Seismic Python Automation/ingredients_catalog_20241024_194748.json\n",
            "\n",
            "Ingredients Processing Summary:\n",
            "Raw Data Missing Percentages:\n",
            "Item ID             0.0\n",
            "Supplier Name       0.0\n",
            "Item Description    0.0\n",
            "UOM                 0.0\n",
            "Quantity in UOM     0.0\n",
            "Total               0.0\n",
            "Date                0.0\n",
            "dtype: float64\n",
            "\n",
            "Ingredient Info Missing Percentages:\n",
            "Supplier Name      NaN\n",
            "Item Description   NaN\n",
            "dtype: float64\n",
            "\n",
            "Final DataFrame Missing Percentages:\n",
            "Item ID             0.0\n",
            "Supplier Name       0.0\n",
            "Item Description    0.0\n",
            "UOM                 0.0\n",
            "Quantity in UOM     0.0\n",
            "Total               0.0\n",
            "Date                0.0\n",
            "dtype: float64\n",
            "Ingredients processing completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "Ygay_vyWC9mH",
        "outputId": "b61f3e80-c7a2-40fa-dc97-7abd038ce987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Item ID     Supplier Name Supplier ID  \\\n",
              "0       1  SUNSHINE FPC INC         NaN   \n",
              "1       0  SUNSHINE FPC INC         NaN   \n",
              "2       0  SUNSHINE FPC INC         NaN   \n",
              "3       0  SUNSHINE FPC INC         NaN   \n",
              "4       0  SUNSHINE FPC INC         NaN   \n",
              "\n",
              "                           Item Description Type Category UOM  \\\n",
              "0  LITHO FILM RICE PUDDING 8OZ LID 24686/rl  NaN      NaN       \n",
              "1  LITHO FILM RICE PUDDING 8OZ LID 24686/rl  NaN      NaN       \n",
              "2  LITHO FILM RICE PUDDING 8OZ LID 24686/rl  NaN      NaN       \n",
              "3  LITHO FILM RICE PUDDING 8OZ LID 24686/rl  NaN      NaN       \n",
              "4  LITHO FILM RICE PUDDING 8OZ LID 24686/rl  NaN      NaN       \n",
              "\n",
              "   Quantity in UOM       Total  Est. Savings % (Low)  Est. Savings % (High)  \\\n",
              "0         516488.0   4648.3920                   NaN                    NaN   \n",
              "1        3212954.0  25382.3366                   NaN                    NaN   \n",
              "2        2017302.0  16340.1462                   NaN                    NaN   \n",
              "3        1879746.0  15225.9426                   NaN                    NaN   \n",
              "4        2972970.0  23486.4630                   NaN                    NaN   \n",
              "\n",
              "   Est. Savings (Low)  Est. Savings (High) Opportunity Term       Date  \n",
              "0                 NaN                  NaN              NaN 2022-11-29  \n",
              "1                 NaN                  NaN              NaN 2023-01-26  \n",
              "2                 NaN                  NaN              NaN 2023-03-22  \n",
              "3                 NaN                  NaN              NaN 2023-05-15  \n",
              "4                 NaN                  NaN              NaN 2023-08-30  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9eb3e055-b892-475c-950d-f76820af2f68\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Item ID</th>\n",
              "      <th>Supplier Name</th>\n",
              "      <th>Supplier ID</th>\n",
              "      <th>Item Description</th>\n",
              "      <th>Type</th>\n",
              "      <th>Category</th>\n",
              "      <th>UOM</th>\n",
              "      <th>Quantity in UOM</th>\n",
              "      <th>Total</th>\n",
              "      <th>Est. Savings % (Low)</th>\n",
              "      <th>Est. Savings % (High)</th>\n",
              "      <th>Est. Savings (Low)</th>\n",
              "      <th>Est. Savings (High)</th>\n",
              "      <th>Opportunity Term</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>SUNSHINE FPC INC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>LITHO FILM RICE PUDDING 8OZ LID 24686/rl</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>516488.0</td>\n",
              "      <td>4648.3920</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2022-11-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>SUNSHINE FPC INC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>LITHO FILM RICE PUDDING 8OZ LID 24686/rl</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>3212954.0</td>\n",
              "      <td>25382.3366</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2023-01-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>SUNSHINE FPC INC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>LITHO FILM RICE PUDDING 8OZ LID 24686/rl</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>2017302.0</td>\n",
              "      <td>16340.1462</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2023-03-22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>SUNSHINE FPC INC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>LITHO FILM RICE PUDDING 8OZ LID 24686/rl</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>1879746.0</td>\n",
              "      <td>15225.9426</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2023-05-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>SUNSHINE FPC INC</td>\n",
              "      <td>NaN</td>\n",
              "      <td>LITHO FILM RICE PUDDING 8OZ LID 24686/rl</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>2972970.0</td>\n",
              "      <td>23486.4630</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2023-08-30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9eb3e055-b892-475c-950d-f76820af2f68')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9eb3e055-b892-475c-950d-f76820af2f68 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9eb3e055-b892-475c-950d-f76820af2f68');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7f340565-f8a4-42fd-8dc0-3b740ae8dbb5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7f340565-f8a4-42fd-8dc0-3b740ae8dbb5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7f340565-f8a4-42fd-8dc0-3b740ae8dbb5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "final_df",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ingredients App"
      ],
      "metadata": {
        "id": "nfpkAHVZ0xS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ingredients_app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set page configuration\n",
        "st.set_page_config(page_title=\"Ingredients Savings Dashboard\", layout=\"wide\")\n",
        "\n",
        "# Custom CSS (same as before)\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .big-font {\n",
        "        font-size:30px !important;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "    .medium-font {\n",
        "        font-size:20px !important;\n",
        "    }\n",
        "    .small-font {\n",
        "        font-size:14px !important;\n",
        "    }\n",
        "    .metric-card {\n",
        "        background-color: #f0f2f6;\n",
        "        border-radius: 10px;\n",
        "        padding: 15px;\n",
        "        text-align: center;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    df = pd.read_csv('final_df.csv')\n",
        "    # Print column names for debugging\n",
        "    st.write(\"Available columns:\", df.columns.tolist())\n",
        "\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "        df = df.dropna(subset=['Date'])\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_metrics(df):\n",
        "    if 'Total' not in df.columns:\n",
        "        st.error(\"'Total' column not found in the data\")\n",
        "        return 0, 0, 0, 0, 0\n",
        "\n",
        "    total_spend = df['Total'].sum()\n",
        "    total_low_savings = df['Est. Savings (Low)'].sum() if 'Est. Savings (Low)' in df.columns else 0\n",
        "    total_high_savings = df['Est. Savings (High)'].sum() if 'Est. Savings (High)' in df.columns else 0\n",
        "\n",
        "    if total_spend > 0:\n",
        "        avg_low_savings_pct = (total_low_savings / total_spend) * 100\n",
        "        avg_high_savings_pct = (total_high_savings / total_spend) * 100\n",
        "    else:\n",
        "        avg_low_savings_pct = 0\n",
        "        avg_high_savings_pct = 0\n",
        "\n",
        "    return total_spend, total_low_savings, total_high_savings, avg_low_savings_pct, avg_high_savings_pct\n",
        "\n",
        "def display_savings_table(df, title):\n",
        "    st.subheader(title)\n",
        "\n",
        "    # Create summary table grouped by Supplier Name\n",
        "    summary = df.groupby('Supplier Name').agg({\n",
        "        'Total': 'sum',\n",
        "        'Est. Savings (Low)': 'sum',\n",
        "        'Est. Savings (High)': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Calculate percentages\n",
        "    summary['Est. Savings % (Low)'] = (summary['Est. Savings (Low)'] / summary['Total'] * 100).round(2)\n",
        "    summary['Est. Savings % (High)'] = (summary['Est. Savings (High)'] / summary['Total'] * 100).round(2)\n",
        "\n",
        "    # Add grand total\n",
        "    grand_total = pd.DataFrame({\n",
        "        'Supplier Name': ['Total'],\n",
        "        'Total': [df['Total'].sum()],\n",
        "        'Est. Savings (Low)': [df['Est. Savings (Low)'].sum()],\n",
        "        'Est. Savings (High)': [df['Est. Savings (High)'].sum()],\n",
        "        'Est. Savings % (Low)': [(df['Est. Savings (Low)'].sum() / df['Total'].sum() * 100).round(2)],\n",
        "        'Est. Savings % (High)': [(df['Est. Savings (High)'].sum() / df['Total'].sum() * 100).round(2)]\n",
        "    })\n",
        "\n",
        "    summary = pd.concat([summary, grand_total])\n",
        "    st.table(summary)\n",
        "\n",
        "    # Create pie chart for savings distribution\n",
        "    df_pie = df[df['Supplier Name'] != 'Total'].copy()\n",
        "    fig = px.pie(df_pie,\n",
        "                 values='Est. Savings (High)',\n",
        "                 names='Supplier Name',\n",
        "                 title=\"High Savings Distribution by Supplier\")\n",
        "    fig.update_traces(textposition='inside', textinfo='percent+label')\n",
        "    fig.update_layout(height=500)\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def display_monthly_savings(df):\n",
        "    if 'Date' not in df.columns:\n",
        "        st.warning(\"Date column not found. Cannot display monthly trends.\")\n",
        "        return\n",
        "\n",
        "    st.subheader(\"Monthly Savings Trend\")\n",
        "\n",
        "    monthly_savings = df.groupby(df['Date'].dt.to_period('M')).agg({\n",
        "        'Est. Savings (Low)': 'sum',\n",
        "        'Est. Savings (High)': 'sum'\n",
        "    }).reset_index()\n",
        "    monthly_savings['Date'] = monthly_savings['Date'].dt.to_timestamp()\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=monthly_savings['Date'],\n",
        "                            y=monthly_savings['Est. Savings (Low)'],\n",
        "                            mode='lines+markers',\n",
        "                            name='Low Savings'))\n",
        "    fig.add_trace(go.Scatter(x=monthly_savings['Date'],\n",
        "                            y=monthly_savings['Est. Savings (High)'],\n",
        "                            mode='lines+markers',\n",
        "                            name='High Savings'))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"Monthly Savings Trend\",\n",
        "        xaxis_title=\"Month\",\n",
        "        yaxis_title=\"Savings ($)\",\n",
        "        height=500\n",
        "    )\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def main():\n",
        "    st.markdown(\"<h1 style='text-align: center;'>Ingredients Savings Dashboard</h1>\",\n",
        "                unsafe_allow_html=True)\n",
        "\n",
        "    # Load data\n",
        "    df = load_data()\n",
        "\n",
        "    # Sidebar filters\n",
        "    st.sidebar.title(\"Filters\")\n",
        "\n",
        "    # Supplier filter\n",
        "    suppliers = ['All'] + sorted(df['Supplier Name'].unique().tolist())\n",
        "    selected_supplier = st.sidebar.selectbox(\"Select Supplier\", suppliers)\n",
        "    if selected_supplier != 'All':\n",
        "        df = df[df['Supplier Name'] == selected_supplier]\n",
        "\n",
        "    # Date filter\n",
        "    if 'Date' in df.columns and not df['Date'].empty:\n",
        "        min_date = df['Date'].min().date()\n",
        "        max_date = df['Date'].max().date()\n",
        "        date_range = st.sidebar.date_input(\"Select Date Range\",\n",
        "                                         value=[min_date, max_date],\n",
        "                                         min_value=min_date,\n",
        "                                         max_value=max_date)\n",
        "\n",
        "        if len(date_range) == 2:\n",
        "            df = df[(df['Date'].dt.date >= date_range[0]) &\n",
        "                   (df['Date'].dt.date <= date_range[1])]\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = calculate_metrics(df)\n",
        "    total_spend, total_low_savings, total_high_savings, avg_low_pct, avg_high_pct = metrics\n",
        "\n",
        "    # Display metrics\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        st.markdown('<div class=\"metric-card\">', unsafe_allow_html=True)\n",
        "        st.markdown('<p class=\"medium-font\">Total Ingredient Spend</p>',\n",
        "                   unsafe_allow_html=True)\n",
        "        st.markdown(f'<p class=\"big-font\">${total_spend:,.2f}</p>',\n",
        "                   unsafe_allow_html=True)\n",
        "        st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n",
        "    with col2:\n",
        "        st.markdown('<div class=\"metric-card\">', unsafe_allow_html=True)\n",
        "        st.markdown('<p class=\"medium-font\">Est. Low Savings</p>',\n",
        "                   unsafe_allow_html=True)\n",
        "        st.markdown(f'<p class=\"big-font\">${total_low_savings:,.2f}</p>',\n",
        "                   unsafe_allow_html=True)\n",
        "        st.markdown(f'<p class=\"small-font\">({avg_low_pct:.1f}%)</p>',\n",
        "                   unsafe_allow_html=True)\n",
        "        st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n",
        "    with col3:\n",
        "        st.markdown('<div class=\"metric-card\">', unsafe_allow_html=True)\n",
        "        st.markdown('<p class=\"medium-font\">Est. High Savings</p>',\n",
        "                   unsafe_allow_html=True)\n",
        "        st.markdown(f'<p class=\"big-font\">${total_high_savings:,.2f}</p>',\n",
        "                   unsafe_allow_html=True)\n",
        "        st.markdown(f'<p class=\"small-font\">({avg_high_pct:.1f}%)</p>',\n",
        "                   unsafe_allow_html=True)\n",
        "        st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n",
        "    # Visualizations\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        st.subheader(\"Top 10 Ingredients by Spend\")\n",
        "        top_spend = df.groupby('Item Description')['Total'].sum().nlargest(10).reset_index()\n",
        "        fig = px.bar(top_spend,\n",
        "                    x='Total',\n",
        "                    y='Item Description',\n",
        "                    orientation='h',\n",
        "                    title=\"Top 10 Ingredients by Spend\")\n",
        "        fig.update_layout(height=400)\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    with col2:\n",
        "        st.subheader(\"Top 10 Suppliers by Spend\")\n",
        "        top_suppliers = df.groupby('Supplier Name')['Total'].sum().nlargest(10).reset_index()\n",
        "        fig = px.bar(top_suppliers,\n",
        "                    x='Total',\n",
        "                    y='Supplier Name',\n",
        "                    orientation='h',\n",
        "                    title=\"Top 10 Suppliers by Spend\")\n",
        "        fig.update_layout(height=400)\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    # Monthly trend\n",
        "    display_monthly_savings(df)\n",
        "\n",
        "    # Ingredients savings summary\n",
        "    display_savings_table(df, \"Ingredients Savings Summary\")\n",
        "\n",
        "    # Raw data view\n",
        "    st.subheader(\"Raw Data\")\n",
        "    st.dataframe(df.head(1000))\n",
        "    if len(df) > 1000:\n",
        "        st.info(f\"Showing first 1000 rows out of {len(df)} total rows.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPi1179msuVQ",
        "outputId": "ad140b55-147c-463d-fab0-1c52a8bcfdc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ingredients_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final DataFrame\n",
        "final_df.to_csv('final_df.csv', index=False)\n",
        "\n",
        "# Import and setup ngrok\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2nCrHcKsJCDT2SiVXRa56Z2LMkz_3uVedRt8Q5oT5BPb9R9Lx\")\n",
        "\n",
        "# Set up a tunnel to the full address with http protocol\n",
        "public_url = ngrok.connect(addr=\"http://localhost:8501\")  # Full address\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run ingredients_app.py &"
      ],
      "metadata": {
        "id": "T1TXfKiv_ZBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HiIIP4eJ2LLa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}